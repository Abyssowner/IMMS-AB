{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90423658-42cd-4a8d-8eba-8801f060cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import torch\n",
    "from protein_bert_pytorch import ProteinBERT, PretrainingWrapper\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tape import TAPETokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "def get_feature(_list):\n",
    "    # load model\n",
    "    model = ProteinBertModel.from_pretrained('bert-base')\n",
    "    torch.save(model, 'pretrain_bert.models')\n",
    "    device = torch.device('cuda')\n",
    "    # model = torch.load('../cmap_final/src/models/pretrain_bert.models')\n",
    "    # model = ProteinBertModel.from_pretrained('./bert-base-chinese')\n",
    "    model = model.to(device)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.eval()\n",
    "    tokenizer = TAPETokenizer(vocab='iupac')  # iupac是TAPE模型的词汇表，UniRep模型使用unirep。\n",
    "    feature = []\n",
    "    for seq in tqdm(_list):      # 进度条\n",
    "        token_ids = torch.tensor([tokenizer.encode(seq)])\n",
    "        output = model(token_ids.to(device))\n",
    "        pooled_output = output[1]\n",
    "        feature.append(pooled_output[0].tolist())\n",
    "    _df = pd.DataFrame(np.array(feature))\n",
    "    return _df\n",
    "\n",
    "\n",
    "\n",
    "def get_feature2():\n",
    "    model = ProteinBERT(\n",
    "        num_tokens=21,\n",
    "        num_annotation=8943,\n",
    "        dim=512,\n",
    "        dim_global=256,\n",
    "        depth=6,\n",
    "        narrow_conv_kernel=9,\n",
    "        wide_conv_kernel=9,\n",
    "        wide_conv_dilation=5,\n",
    "        attn_heads=8,\n",
    "        attn_dim_head=64\n",
    "    )\n",
    "\n",
    "    seq = torch.randint(0, 21, (2, 2048))\n",
    "    mask = torch.ones(2, 2048).bool()\n",
    "    annotation = torch.randint(0, 1, (2, 8943)).float()\n",
    "\n",
    "    seq_logits, annotation_logits = model(seq, annotation, mask=mask)\n",
    "\n",
    "def parse(f, comment=\"#\"):\n",
    "    names = []\n",
    "    sequences = []\n",
    "\n",
    "    for record in SeqIO.parse(f, \"fasta\"):\n",
    "        names.append(record.name)\n",
    "        sequences.append(str(record.seq))\n",
    "\n",
    "    return names, sequences\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    fastaPath = './datasets/seq_natural.fasta'\n",
    "    outputPath = './datasets/seq_natural_embedding.csv'\n",
    "    names, sequence = parse(fastaPath)\n",
    "    new_sequence = []\n",
    "    for seq in sequence:\n",
    "\n",
    "        seq = seq.replace('_', '')\n",
    "        seq = seq.replace('J', '')\n",
    "        new_sequence.append(seq)\n",
    "    rows = zip(names, new_sequence)\n",
    "    with open(outputPath, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for row in rows:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    df = get_feature(new_sequence)\n",
    "\n",
    "    df.to_csv(outputPath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ecf043-efd5-4ccd-bea3-5ce450913eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a new model.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import IterableDataset, dataloader\n",
    "from multiprocessing.reduction import ForkingPickler\n",
    "from sklearn.metrics import average_precision_score as average_precision\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, NamedTuple, Optional\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from src.models.mvsf import ModelAffinity\n",
    "from src.utils import *\n",
    "from multiprocessing.reduction import ForkingPickler\n",
    "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torcheval.metrics.functional import r2_score\n",
    "\n",
    "# 解决使用multiprocessing模块时由于Tensor对象内部实现机制导致的序列化错误\n",
    "default_collate_func = dataloader.default_collate\n",
    "def default_collate_override(batch):\n",
    "    dataloader._use_shared_memory = False\n",
    "    return default_collate_func(batch)\n",
    "setattr(dataloader, 'default_collate', default_collate_override)\n",
    "for t in torch._storage_classes:\n",
    "    if sys.version_info[0] == 2:\n",
    "        if t in ForkingPickler.dispatch:\n",
    "            del ForkingPickler.dispatch[t]\n",
    "    else:\n",
    "        if t in ForkingPickler._extra_reducers:\n",
    "            del ForkingPickler._extra_reducers[t]\n",
    "\n",
    "class TrainArguments(NamedTuple):\n",
    "    cmd: str\n",
    "    device: int\n",
    "    train: str\n",
    "    test: str\n",
    "    no_augment: bool\n",
    "    augment_weight: float\n",
    "    weight_module1: float\n",
    "    weight_module2: float\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    weight_decay: float\n",
    "    lr: float\n",
    "    kfolds: int\n",
    "    outfile: Optional[str]\n",
    "    save_prefix: Optional[str]\n",
    "    checkpoint: Optional[str]\n",
    "    seed: Optional[int]\n",
    "    func: Callable[[TrainArguments], None]\n",
    "\n",
    "def add_args(parser):\n",
    "    data_grp = parser.add_argument_group(\"Data\")\n",
    "    contact_grp = parser.add_argument_group(\"Contact Module\")\n",
    "    train_grp = parser.add_argument_group(\"Training\")\n",
    "    misc_grp = parser.add_argument_group(\"Output and Device\")\n",
    "\n",
    "    # Data\n",
    "    data_grp.add_argument(\"--train\", default=\"datasets/pairs_sabdab.csv\", help=\"list of training pairs\")\n",
    "    data_grp.add_argument(\"--test\", default=\"datasets/pairs_benchmark.csv\", help=\"list of validation/testing pairs\")\n",
    "    data_grp.add_argument(\"--seq-path\", default=\"datasets/seq_natural.fasta\")\n",
    "    data_grp.add_argument(\"--feature-path\", default=\"datasets/seq_natural_embedding.csv\")\n",
    "    data_grp.add_argument(\"--no-augment\", default=True, help=\"data is automatically augmented by adding (B A) for all pairs (A B). Set this flag to not augment data\",)\n",
    "    data_grp.add_argument(\"--augment-weight\", type=float, default=0.5, help=\"weight of augment data\",)\n",
    "\n",
    "    # Model\n",
    "    contact_grp.add_argument(\"--weight-module1\", type=float, default=1, help=\"weight of module1\",)\n",
    "    contact_grp.add_argument(\"--weight-module2\", type=float, default=1, help=\"weight of module1\",)\n",
    "\n",
    "    # Training\n",
    "    train_grp.add_argument(\"--num-epochs\", type=int, default=30, help=\"number of epochs\",)\n",
    "    train_grp.add_argument(\"--batch-size\", type=int, default=16, help=\"minibatch size (default: 16)\",)\n",
    "    train_grp.add_argument(\"--weight-decay\", type=float, default=0.00001, help=\"L2 regularization /0.0001\",)  # 正则化项的设置\n",
    "    train_grp.add_argument(\"--lr\", type=float, default=0.00001, help=\"learning rate\",)\n",
    "    train_grp.add_argument(\"--kfolds\", type=int, default=10)\n",
    "    train_grp.add_argument(\"--cross-validate\", default=True, help=\"cross validate\",)\n",
    "\n",
    "    # Output and Device\n",
    "    misc_grp.add_argument(\"-o\", \"--outfile\", help=\"output file path (default: stdout)\")\n",
    "    misc_grp.add_argument(\"--save-prefix\", help=\"path prefix for saving models\")\n",
    "    misc_grp.add_argument(\"-d\", \"--device\", type=int, required=True, help=\"compute device to use\")\n",
    "    misc_grp.add_argument(\"--checkpoint\", help=\"checkpoint model to start training from\")\n",
    "    misc_grp.add_argument(\"--seed\", help=\"Set random seed\", type=int)\n",
    "    return parser\n",
    "\n",
    "def predict_affinity(model, Lchain, Hchain, antigen, embedding_tensor, aaindex_feature, use_cuda):\n",
    "    b = len(Hchain)\n",
    "    lchain_embeddings = []\n",
    "    hchain_embeddings = []\n",
    "    ag_embeddings = []\n",
    "\n",
    "    lchain_aaindex = []\n",
    "    hchain_aaindex = []\n",
    "    ag_aaindex = []\n",
    "\n",
    "    for i in range(b):\n",
    "        lchain_embedding = embedding_tensor[Lchain[i]]\n",
    "        hchain_embedding = embedding_tensor[Hchain[i]]\n",
    "        ag_embedding = embedding_tensor[antigen[i]]\n",
    "\n",
    "        lchain_aaindex.append(aaindex_feature[Lchain[i]])\n",
    "        hchain_aaindex.append(aaindex_feature[Hchain[i]])\n",
    "        ag_aaindex.append(aaindex_feature[antigen[i]])\n",
    "\n",
    "        lchain_embeddings.append(lchain_embedding)\n",
    "        hchain_embeddings.append(hchain_embedding)\n",
    "        ag_embeddings.append(ag_embedding)\n",
    "\n",
    "    if use_cuda:\n",
    "        lchain_embeddings = torch.stack(lchain_embeddings, 0).cuda()\n",
    "        hchain_embeddings = torch.stack(hchain_embeddings, 0).cuda()\n",
    "        ag_embeddings = torch.stack(ag_embeddings, 0).cuda()\n",
    "\n",
    "        lchain_aaindex = torch.stack(lchain_aaindex, 0).cuda()\n",
    "        hchain_aaindex = torch.stack(hchain_aaindex, 0).cuda()\n",
    "        ag_aaindex = torch.stack(ag_aaindex, 0).cuda()\n",
    "\n",
    "\n",
    "\n",
    "    ph = model.predict(lchain_aaindex, hchain_aaindex, ag_aaindex, lchain_embeddings, hchain_embeddings, ag_embeddings)\n",
    "    return ph\n",
    "\n",
    "def model_eval(model, test_iterator, embedding_tensors, aaindex_feature, write, weight1, weight2, use_cuda):\n",
    "\n",
    "    p_hat = []\n",
    "    true_y = []\n",
    "\n",
    "    for lchain, hchain, antigen, y in test_iterator:\n",
    "\n",
    "        ph = predict_affinity(model, lchain, hchain, antigen, embedding_tensors, aaindex_feature, use_cuda)\n",
    "        p_hat.append(ph)\n",
    "        true_y.append(y)\n",
    "\n",
    "    y = torch.cat(true_y, 0)\n",
    "\n",
    "    p_hat = torch.cat(p_hat, 0)\n",
    "    if use_cuda:\n",
    "        y.cuda()\n",
    "        p_hat = torch.Tensor([x.cuda() for x in p_hat])\n",
    "        p_hat.cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(p_hat.float(), y.float())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p_hat = p_hat.float()\n",
    "        y = y.float()\n",
    "        max_val = 16.9138\n",
    "        min_val = 5.0400\n",
    "        p_hat = (p_hat * (max_val - min_val)) + min_val\n",
    "\n",
    "        # if write:\n",
    "        #     with open('pred_skempi.csv', 'a') as f:\n",
    "        #         for i in range(len(y)):\n",
    "        #             f.write(str(y[i].item()) + ',' + str(p_hat[i].item()) + '\\n')\n",
    "\n",
    "        rmse = torch.sqrt(torch.mean((y - p_hat) ** 2)).item()\n",
    "        mae = torch.mean(torch.abs(y - p_hat)).item()\n",
    "        r_2 = r2_score(y, p_hat).item()\n",
    "        p = pearsonr(y, p_hat).item()\n",
    "\n",
    "    return loss, rmse, mae, r_2, p\n",
    "\n",
    "def train_model(args, output):\n",
    "    # Create data sets\n",
    "    batch_size = args.batch_size\n",
    "    use_cuda = (args.device > -1) and torch.cuda.is_available()  # True\n",
    "    train_fi = args.train\n",
    "    train_df = pd.read_csv(train_fi)\n",
    "    test_fi = args.test\n",
    "    test_df = pd.read_csv(test_fi)\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    lr = args.lr\n",
    "    # wd = args.weight_decay  # 0.0001\n",
    "    num_epochs = args.num_epochs\n",
    "    batch_size = args.batch_size\n",
    "    digits = int(np.floor(np.log10(num_epochs))) + 1\n",
    "    save_prefix = args.save_prefix\n",
    "    weight1 = args.weight_module1\n",
    "    weight2 = args.weight_module2\n",
    "\n",
    "\n",
    "    log(f'Using save prefix \"{save_prefix}\"', file=output)\n",
    "    log(f\"Training with SAM: lr={lr}\", file=output)\n",
    "    log(f\"\\tnum_epochs: {num_epochs}\", file=output)\n",
    "    log(f\"\\tbatch_size: {batch_size}\", file=output)\n",
    "    log(f\"\\tmodule 1 weight: {weight1}\", file=output)\n",
    "    log(f\"\\tmodule 2 weight: {weight2}\", file=output)\n",
    "    output.flush()\n",
    "\n",
    "\n",
    "    if(args.cross_validate):\n",
    "    # ===============================================cross validation=================================================\n",
    "        k_folds = args.kfolds\n",
    "        kfold = KFold(n_splits=k_folds, shuffle=False)\n",
    "        for fold, (train_ids, test_ids) in enumerate(kfold.split(train_df)):\n",
    "            print(f'******************************** FOLD {fold} ******************************')\n",
    "            log(f'******************************** FOLD {fold} ******************************', file=output)\n",
    "            train_df_fold = train_df.iloc[train_ids]\n",
    "            test_df_fold = train_df.iloc[test_ids]\n",
    "            train_df_fold = train_df_fold.reset_index(drop=True)\n",
    "            test_df_fold = test_df_fold.reset_index(drop=True)\n",
    "\n",
    "            train_df_fold.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "            train_l_fold = train_df_fold[\"light\"]\n",
    "            train_h_fold = train_df_fold[\"heavy\"]\n",
    "            train_ag_fold = train_df_fold[\"antigen\"]\n",
    "            train_y_fold = torch.from_numpy(train_df_fold[\"delta_g\"].values)\n",
    "            train_y_fold = -train_y_fold\n",
    "\n",
    "            max_val = 16.05654\n",
    "            min_val = 5.0400\n",
    "            train_y_fold = (train_y_fold - min_val) / (max_val - min_val)\n",
    "\n",
    "            test_df_fold.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "            test_l_fold = test_df_fold[\"light\"]\n",
    "            test_h_fold = test_df_fold[\"heavy\"]\n",
    "            test_ag_fold = test_df_fold[\"antigen\"]\n",
    "            test_y_fold = torch.from_numpy(test_df_fold[\"delta_g\"].values)\n",
    "            test_y_fold = -test_y_fold\n",
    "\n",
    "            train_dataset_fold = PairedDataset(train_l_fold, train_h_fold, train_ag_fold, train_y_fold)\n",
    "            train_iterator_fold = torch.utils.data.DataLoader(\n",
    "                train_dataset_fold,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_paired_sequences,\n",
    "                shuffle=True,\n",
    "                pin_memory=False,\n",
    "                drop_last=False,\n",
    "                # num_workers=2,\n",
    "            )\n",
    "            log(f\"Loaded {len(train_l_fold)} training pairs\", file=output)\n",
    "            output.flush()\n",
    "\n",
    "            test_dataset_fold = PairedDataset(test_l_fold, test_h_fold, test_ag_fold, test_y_fold)\n",
    "            test_iterator_fold = torch.utils.data.DataLoader(\n",
    "                test_dataset_fold,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_paired_sequences,\n",
    "                shuffle=False,\n",
    "                pin_memory=False,\n",
    "                drop_last=False,\n",
    "                # num_workers=2,\n",
    "            )\n",
    "\n",
    "            all_proteins = set(train_l_fold).union(train_h_fold).union(train_ag_fold) \\\n",
    "                .union(test_l_fold).union(test_h_fold).union(test_ag_fold)\n",
    "            fastaPath = args.seq_path\n",
    "            embeddingPath = args.feature_path\n",
    "            embeddings = embed_dict(fastaPath, embeddingPath)\n",
    "            log(\"Embedded successfully...\", file=output)\n",
    "            aaindex_feature = seq_aaindex_dict(all_proteins, fastaPath)\n",
    "\n",
    "            model = ModelAffinity(batch_size, use_cuda)\n",
    "            model.use_cuda = use_cuda  # default is False\n",
    "            if use_cuda:\n",
    "                model.cuda()\n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            base_optimizer = optim.SGD\n",
    "            optimizer = SAM(params, base_optimizer, lr=lr, weight_decay=args.weight_decay)\n",
    "\n",
    "            batch_report_fmt = (\"[{}/{}] training {:.1%}: Loss={:.6}, MSE={:.6}, MAE={:.6}\")\n",
    "            epoch_report_fmt = (\n",
    "                \"-----------------------------------Finished Epoch {}/{}: Loss={:.6}, RMSE={:.6}, MAE={:.6}, r_2={:.6}, p={:.6}\")\n",
    "\n",
    "            N = len(train_iterator_fold) * batch_size\n",
    "\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                if epoch == 10:\n",
    "                    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / 10\n",
    "                print(\"lr:\", optimizer.param_groups[0]['lr'])\n",
    "                model.train()\n",
    "                n = 0\n",
    "                loss_accum = 0\n",
    "                # acc_accum = 0\n",
    "                mse_accum = 0\n",
    "                mae_accum = 0\n",
    "                optimizer.zero_grad()\n",
    "                all_y = []\n",
    "                all_p_hat = []\n",
    "                for (lchain, hchain, antigen, y) in train_iterator_fold:\n",
    "\n",
    "                    phat = predict_affinity(\n",
    "                        model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda=use_cuda)\n",
    "                    phat = phat.float().view(-1)\n",
    "\n",
    "                    if use_cuda:\n",
    "                        y = y.cuda()\n",
    "                    # y = Variable(y)\n",
    "                    y = y.float()\n",
    "\n",
    "                    criterion = nn.MSELoss()\n",
    "                    b = len(y)\n",
    "                    loss = criterion(phat, y)\n",
    "                    loss.requires_grad_(True)\n",
    "                    loss.backward()\n",
    "                    # scaler.scale(loss).backward()\n",
    "                    if use_cuda:\n",
    "                        y = y.cpu()\n",
    "                        phat = phat.cpu()\n",
    "                    all_y.append(y)\n",
    "                    all_p_hat.append(phat)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        phat = phat.float()\n",
    "                        y = y.float()\n",
    "                        mse = torch.mean((y - phat) ** 2).item()\n",
    "                        mae = torch.mean(torch.abs(y - phat)).item()\n",
    "                    n += b\n",
    "                    delta = b * (loss.item() - loss_accum)\n",
    "                    loss_accum += delta / n\n",
    "                    delta = b * (mse - mse_accum)\n",
    "                    mse_accum += delta / n\n",
    "                    delta = b * (mae - mae_accum)\n",
    "                    mae_accum += delta / n\n",
    "                    report = (n - b) // 100 < n // 100\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if report:\n",
    "                        tokens = [epoch + 1, num_epochs, n / N, loss_accum, mse_accum, mae_accum, ]\n",
    "                        log(batch_report_fmt.format(*tokens), file=output)\n",
    "                        output.flush()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    if epoch+1 == 30:\n",
    "                        write = True\n",
    "                    else:\n",
    "                        write = False\n",
    "                    (inter_loss, inter_rmse, inter_mae, inter_r_2, inter_p,) = model_eval(\n",
    "                        model, test_iterator_fold, embeddings, aaindex_feature, write, weight1, weight2, use_cuda=use_cuda)\n",
    "\n",
    "                    tokens = [epoch + 1, num_epochs, inter_loss, inter_mae, inter_rmse, inter_r_2, inter_p, ]\n",
    "\n",
    "                    # scheduler.step(inter_mse)\n",
    "                    log(epoch_report_fmt.format(*tokens), file=output)\n",
    "                    output.flush()\n",
    "\n",
    "                    # Save the model (every epoch)\n",
    "                    # if save_prefix is not None:\n",
    "                    #     save_path = (save_prefix + \"_epoch\" + str(epoch + 1).zfill(digits) + \".pth\")\n",
    "                    #     log(f\"Saving model to {save_path}\", file=output)\n",
    "                    #     model.cpu()\n",
    "                    #     torch.save(model, save_path)\n",
    "                    #     if use_cuda:\n",
    "                    #         model.cuda()\n",
    "\n",
    "                    # update learning rate\n",
    "                    # scheduler.step()\n",
    "\n",
    "                # output.flush()\n",
    "            # break\n",
    "    else:\n",
    "        num_samples = len(train_df)\n",
    "        train_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "        train_l = train_df[\"light\"]\n",
    "        train_h = train_df[\"heavy\"]\n",
    "        train_ag = train_df[\"antigen\"]\n",
    "        train_y = torch.from_numpy(train_df[\"delta_g\"].values)\n",
    "        train_y = -train_y\n",
    "        train_y = NormalizeData(train_y)\n",
    "\n",
    "        test_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "        test_l = test_df[\"light\"]\n",
    "        test_h = test_df[\"heavy\"]\n",
    "        test_ag = test_df[\"antigen\"]\n",
    "        test_y = torch.from_numpy(test_df[\"delta_g\"].values)\n",
    "        test_y = -test_y\n",
    "\n",
    "        train_dataset = PairedDataset(train_l, train_h, train_ag, train_y)\n",
    "        train_iterator = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_paired_sequences,\n",
    "            shuffle=True,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "            # num_workers=4,\n",
    "        )\n",
    "        log(f\"Loaded {len(train_l)} training pairs\", file=output)\n",
    "        output.flush()\n",
    "\n",
    "        test_dataset = PairedDataset(test_l, test_h, test_ag, test_y)\n",
    "        test_iterator = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_paired_sequences,\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "            # num_workers=4,\n",
    "        )\n",
    "\n",
    "        log(f\"Loaded {len(test_l)} test pairs\", file=output)\n",
    "        log(\"Loading embeddings...\", file=output)\n",
    "        output.flush()\n",
    "\n",
    "        all_proteins = set(train_l).union(train_h).union(train_ag).union(test_l).union(test_h).union(test_ag)\n",
    "\n",
    "        fastaPath = args.seq_path\n",
    "        embeddingPath = args.feature_path\n",
    "        embeddings = embed_dict(fastaPath, embeddingPath)\n",
    "        log(\"embeded successfully...\", file=output)\n",
    "        aaindex_feature = seq_aaindex_dict(all_proteins, fastaPath)\n",
    "\n",
    "        model = ModelAffinity(batch_size, use_cuda)\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        base_optimizer = optim.Adam\n",
    "        optimizer = SAM(params, base_optimizer, lr=lr)\n",
    "        log(f'Using save prefix \"{save_prefix}\"', file=output)\n",
    "        log(f\"Training with SAM: lr={lr}\", file=output)\n",
    "        log(f\"\\tnum_epochs: {num_epochs}\", file=output)\n",
    "        log(f\"\\tbatch_size: {batch_size}\", file=output)\n",
    "        log(f\"\\tmodule 1 weight: {weight1}\", file=output)\n",
    "        log(f\"\\tmodule 2 weight: {weight2}\", file=output)\n",
    "        output.flush()\n",
    "\n",
    "        batch_report_fmt = (\"[{}/{}] training {:.1%}: Loss={:.6}, MSE={:.6}, MAE={:.6}\")\n",
    "        epoch_report_fmt = (\n",
    "            \"-----------------------------------Finished Epoch {}/{}: Loss={:.6}, RMSE={:.6}, MAE={:.6}, r_2={:.6}, p={:.6}\")\n",
    "\n",
    "        N = len(train_iterator) * batch_size\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            model.train()\n",
    "            n = 0\n",
    "            loss_accum = 0\n",
    "            mse_accum = 0\n",
    "            mae_accum = 0\n",
    "            all_y = []\n",
    "            all_p_hat = []\n",
    "            optimizer.zero_grad()\n",
    "            for (lchain, hchain, antigen, y) in train_iterator:\n",
    "                phat = predict_affinity(\n",
    "                    model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda=use_cuda)\n",
    "                phat = phat.float().view(-1)\n",
    "\n",
    "                if use_cuda:\n",
    "                    y = y.cuda()\n",
    "                # y = Variable(y)\n",
    "                y = y.float()\n",
    "\n",
    "                criterion = nn.MSELoss()\n",
    "                b = len(y)\n",
    "                loss = criterion(phat, y)\n",
    "                loss.requires_grad_(True)\n",
    "                loss.backward()\n",
    "                # scaler.scale(loss).backward()\n",
    "                if use_cuda:\n",
    "                    y = y.cpu()\n",
    "                    phat = phat.cpu()\n",
    "\n",
    "                all_y.append(y)\n",
    "                all_p_hat.append(phat)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    phat = phat.float()\n",
    "                    y = y.float()\n",
    "                    mse = torch.mean((y - phat) ** 2).item()\n",
    "                    mae = torch.mean(torch.abs(y - phat)).item()\n",
    "                n += b\n",
    "                delta = b * (loss.item() - loss_accum)\n",
    "                loss_accum += delta / n\n",
    "                delta = b * (mse - mse_accum)\n",
    "                mse_accum += delta / n\n",
    "                delta = b * (mae - mae_accum)\n",
    "                mae_accum += delta / n\n",
    "                report = (n - b) // 100 < n // 100\n",
    "\n",
    "                optimizer.step()\n",
    "                if report:\n",
    "                    tokens = [epoch + 1, num_epochs, n / N, loss_accum, mse_accum, mae_accum, ]\n",
    "                    log(batch_report_fmt.format(*tokens), file=output)\n",
    "                    output.flush()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                (inter_loss, inter_rmse, inter_mae, inter_r_2, inter_p,) = model_eval(\n",
    "                    model, test_iterator, embeddings, aaindex_feature, weight1, weight2, use_cuda=use_cuda)\n",
    "                tokens = [epoch + 1, num_epochs, inter_loss, inter_mae, inter_rmse, inter_r_2, inter_p, ]\n",
    "                # scheduler.step(inter_mse)\n",
    "                log(epoch_report_fmt.format(*tokens), file=output)\n",
    "                output.flush()\n",
    "\n",
    "                # Save the model (every epoch)\n",
    "                # if save_prefix is not None:\n",
    "                #     save_path = (save_prefix + \"_epoch\" + str(epoch + 1).zfill(digits) + \".pth\")\n",
    "                #     log(f\"Saving model to {save_path}\", file=output)\n",
    "                #     model.cpu()\n",
    "                #     torch.save(model, save_path)\n",
    "                #     if use_cuda:\n",
    "                #         model.cuda()\n",
    "\n",
    "                # update learning rate\n",
    "                # scheduler.step()\n",
    "\n",
    "    # Save the model (final)\n",
    "    # if save_prefix is not None:\n",
    "    #     save_path = save_prefix + \"_final.pth\"\n",
    "    #     log(f\"Saving final model to {save_path}\", file=output)\n",
    "    #     model.cpu()\n",
    "    #     torch.save(model, save_path)\n",
    "    #     if use_cuda:\n",
    "    #         model.cuda()\n",
    "\n",
    "def main(args):\n",
    "    output = args.outfile\n",
    "    if output is None:\n",
    "        output = sys.stdout\n",
    "    else:\n",
    "        output = open(output, \"w\")\n",
    "\n",
    "    log(f'Called as: {\" \".join(sys.argv)}', file=output, print_also=True)\n",
    "\n",
    "    # Set the device\n",
    "    device = args.device\n",
    "    use_cuda = (device > -1) and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        torch.cuda.set_device(device)\n",
    "        log(\n",
    "            f\"Using CUDA device {device} - {torch.cuda.get_device_name(device)}\",\n",
    "            file=output,\n",
    "            print_also=True,\n",
    "        )\n",
    "    else:\n",
    "        log(\"Using CPU\", file=output, print_also=True)\n",
    "        device = \"cpu\"\n",
    "\n",
    "    if args.seed is not None:\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "    train_model(args, output)\n",
    "\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35410761-20ce-4dc0-b204-5a30258a9794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with parameters:\n",
      "Using CUDA device 0 - NVIDIA L20\n",
      "Using save prefix \"saved_models/model\"\n",
      "Training with SAM: lr=1e-05\n",
      "\tnum_epochs: 30\n",
      "\tbatch_size: 16\n",
      "\tmodule 1 weight: 1.0\n",
      "\tmodule 2 weight: 1.0\n",
      "******************************** FOLD 0 ******************************\n",
      "******************************** FOLD 0 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0409009, MSE=0.0409009, MAE=0.158833\n",
      "[1/30] training 39.4%: Loss=0.0407173, MSE=0.0407173, MAE=0.160486\n",
      "[1/30] training 57.6%: Loss=0.0411997, MSE=0.0411997, MAE=0.162105\n",
      "[1/30] training 75.8%: Loss=0.039164, MSE=0.039164, MAE=0.156853\n",
      "[1/30] training 97.0%: Loss=0.0383314, MSE=0.0383314, MAE=0.154979\n",
      "-----------------------------------Finished Epoch 1/30: Loss=111.843, RMSE=1.74707, MAE=2.1198, r_2=-42.1287, p=0.216402\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0239794, MSE=0.0239794, MAE=0.121914\n",
      "[2/30] training 39.4%: Loss=0.0307642, MSE=0.0307642, MAE=0.1371\n",
      "[2/30] training 57.6%: Loss=0.0337429, MSE=0.0337429, MAE=0.144387\n",
      "[2/30] training 75.8%: Loss=0.0346746, MSE=0.0346746, MAE=0.146852\n",
      "[2/30] training 97.0%: Loss=0.0358996, MSE=0.0358996, MAE=0.149512\n",
      "-----------------------------------Finished Epoch 2/30: Loss=111.894, RMSE=1.65114, MAE=2.03068, r_2=-10.4963, p=0.35588\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0409927, MSE=0.0409927, MAE=0.157166\n",
      "[3/30] training 39.4%: Loss=0.0353613, MSE=0.0353613, MAE=0.145654\n",
      "[3/30] training 57.6%: Loss=0.0338526, MSE=0.0338526, MAE=0.144128\n",
      "[3/30] training 75.8%: Loss=0.0349053, MSE=0.0349053, MAE=0.147645\n",
      "[3/30] training 97.0%: Loss=0.0348021, MSE=0.0348021, MAE=0.146752\n",
      "-----------------------------------Finished Epoch 3/30: Loss=111.941, RMSE=1.59641, MAE=1.97284, r_2=-4.70285, p=0.413889\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0321852, MSE=0.0321852, MAE=0.138632\n",
      "[4/30] training 39.4%: Loss=0.0336804, MSE=0.0336804, MAE=0.142896\n",
      "[4/30] training 57.6%: Loss=0.033507, MSE=0.033507, MAE=0.143569\n",
      "[4/30] training 75.8%: Loss=0.0322031, MSE=0.0322031, MAE=0.14116\n",
      "[4/30] training 97.0%: Loss=0.0332115, MSE=0.0332115, MAE=0.143835\n",
      "-----------------------------------Finished Epoch 4/30: Loss=111.997, RMSE=1.5796, MAE=1.9497, r_2=-3.64703, p=0.437339\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0267622, MSE=0.0267622, MAE=0.130319\n",
      "[5/30] training 39.4%: Loss=0.0310436, MSE=0.0310436, MAE=0.140315\n",
      "[5/30] training 57.6%: Loss=0.0339973, MSE=0.0339973, MAE=0.144569\n",
      "[5/30] training 75.8%: Loss=0.0337535, MSE=0.0337535, MAE=0.143785\n",
      "[5/30] training 97.0%: Loss=0.033702, MSE=0.033702, MAE=0.143725\n",
      "-----------------------------------Finished Epoch 5/30: Loss=111.657, RMSE=1.55992, MAE=1.94402, r_2=-3.67144, p=0.443731\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0330716, MSE=0.0330716, MAE=0.14112\n",
      "[6/30] training 39.4%: Loss=0.0329999, MSE=0.0329999, MAE=0.144136\n",
      "[6/30] training 57.6%: Loss=0.033587, MSE=0.033587, MAE=0.147433\n",
      "[6/30] training 75.8%: Loss=0.0335833, MSE=0.0335833, MAE=0.14546\n",
      "[6/30] training 97.0%: Loss=0.0324374, MSE=0.0324374, MAE=0.142872\n",
      "-----------------------------------Finished Epoch 6/30: Loss=111.681, RMSE=1.54997, MAE=1.92561, r_2=-2.91385, p=0.458784\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.025313, MSE=0.025313, MAE=0.124656\n",
      "[7/30] training 39.4%: Loss=0.0285567, MSE=0.0285567, MAE=0.13085\n",
      "[7/30] training 57.6%: Loss=0.0306282, MSE=0.0306282, MAE=0.136296\n",
      "[7/30] training 75.8%: Loss=0.0305777, MSE=0.0305777, MAE=0.137847\n",
      "[7/30] training 97.0%: Loss=0.0307847, MSE=0.0307847, MAE=0.138605\n",
      "-----------------------------------Finished Epoch 7/30: Loss=111.779, RMSE=1.55916, MAE=1.92229, r_2=-3.16694, p=0.461077\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.032787, MSE=0.032787, MAE=0.142907\n",
      "[8/30] training 39.4%: Loss=0.0371841, MSE=0.0371841, MAE=0.155086\n",
      "[8/30] training 57.6%: Loss=0.0344299, MSE=0.0344299, MAE=0.147762\n",
      "[8/30] training 75.8%: Loss=0.0317269, MSE=0.0317269, MAE=0.14073\n",
      "[8/30] training 97.0%: Loss=0.0299106, MSE=0.0299106, MAE=0.136478\n",
      "-----------------------------------Finished Epoch 8/30: Loss=111.636, RMSE=1.55218, MAE=1.91682, r_2=-2.5191, p=0.467317\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0250892, MSE=0.0250892, MAE=0.120899\n",
      "[9/30] training 39.4%: Loss=0.0293888, MSE=0.0293888, MAE=0.131426\n",
      "[9/30] training 57.6%: Loss=0.029366, MSE=0.029366, MAE=0.132926\n",
      "[9/30] training 75.8%: Loss=0.029552, MSE=0.029552, MAE=0.134453\n",
      "[9/30] training 97.0%: Loss=0.0303618, MSE=0.0303618, MAE=0.136917\n",
      "-----------------------------------Finished Epoch 9/30: Loss=111.598, RMSE=1.55982, MAE=1.91925, r_2=-2.68306, p=0.466404\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0317872, MSE=0.0317872, MAE=0.132419\n",
      "[10/30] training 39.4%: Loss=0.0302883, MSE=0.0302883, MAE=0.131712\n",
      "[10/30] training 57.6%: Loss=0.0296793, MSE=0.0296793, MAE=0.133824\n",
      "[10/30] training 75.8%: Loss=0.0284575, MSE=0.0284575, MAE=0.131576\n",
      "[10/30] training 97.0%: Loss=0.0292297, MSE=0.0292297, MAE=0.134451\n",
      "-----------------------------------Finished Epoch 10/30: Loss=111.627, RMSE=1.56425, MAE=1.91372, r_2=-2.02308, p=0.471323\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0274088, MSE=0.0274088, MAE=0.128849\n",
      "[11/30] training 39.4%: Loss=0.0250143, MSE=0.0250143, MAE=0.124618\n",
      "[11/30] training 57.6%: Loss=0.0259161, MSE=0.0259161, MAE=0.127022\n",
      "[11/30] training 75.8%: Loss=0.028128, MSE=0.028128, MAE=0.131263\n",
      "[11/30] training 97.0%: Loss=0.027793, MSE=0.027793, MAE=0.13062\n",
      "-----------------------------------Finished Epoch 11/30: Loss=111.764, RMSE=1.55874, MAE=1.90895, r_2=-2.04114, p=0.473068\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0247383, MSE=0.0247383, MAE=0.116554\n",
      "[12/30] training 39.4%: Loss=0.0286095, MSE=0.0286095, MAE=0.130609\n",
      "[12/30] training 57.6%: Loss=0.0288504, MSE=0.0288504, MAE=0.131896\n",
      "[12/30] training 75.8%: Loss=0.0276891, MSE=0.0276891, MAE=0.128841\n",
      "[12/30] training 97.0%: Loss=0.0275932, MSE=0.0275932, MAE=0.129446\n",
      "-----------------------------------Finished Epoch 12/30: Loss=111.531, RMSE=1.56754, MAE=1.92167, r_2=-1.97902, p=0.468263\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0304861, MSE=0.0304861, MAE=0.136636\n",
      "[13/30] training 39.4%: Loss=0.0281259, MSE=0.0281259, MAE=0.132172\n",
      "[13/30] training 57.6%: Loss=0.0271187, MSE=0.0271187, MAE=0.127322\n",
      "[13/30] training 75.8%: Loss=0.0270165, MSE=0.0270165, MAE=0.127064\n",
      "[13/30] training 97.0%: Loss=0.027729, MSE=0.027729, MAE=0.131075\n",
      "-----------------------------------Finished Epoch 13/30: Loss=111.226, RMSE=1.56096, MAE=1.94427, r_2=-2.07363, p=0.46744\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0271963, MSE=0.0271963, MAE=0.130694\n",
      "[14/30] training 39.4%: Loss=0.0266756, MSE=0.0266756, MAE=0.130452\n",
      "[14/30] training 57.6%: Loss=0.0276953, MSE=0.0276953, MAE=0.133873\n",
      "[14/30] training 75.8%: Loss=0.0281464, MSE=0.0281464, MAE=0.134408\n",
      "[14/30] training 97.0%: Loss=0.0287995, MSE=0.0287995, MAE=0.134096\n",
      "-----------------------------------Finished Epoch 14/30: Loss=111.479, RMSE=1.55559, MAE=1.92042, r_2=-2.09153, p=0.470706\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0237451, MSE=0.0237451, MAE=0.11514\n",
      "[15/30] training 39.4%: Loss=0.0268545, MSE=0.0268545, MAE=0.126833\n",
      "[15/30] training 57.6%: Loss=0.026689, MSE=0.026689, MAE=0.126341\n",
      "[15/30] training 75.8%: Loss=0.0271579, MSE=0.0271579, MAE=0.128517\n",
      "[15/30] training 97.0%: Loss=0.0263938, MSE=0.0263938, MAE=0.127068\n",
      "-----------------------------------Finished Epoch 15/30: Loss=111.597, RMSE=1.55939, MAE=1.92095, r_2=-2.37277, p=0.465045\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0269031, MSE=0.0269031, MAE=0.124143\n",
      "[16/30] training 39.4%: Loss=0.0278674, MSE=0.0278674, MAE=0.129312\n",
      "[16/30] training 57.6%: Loss=0.0266883, MSE=0.0266883, MAE=0.125378\n",
      "[16/30] training 75.8%: Loss=0.0268479, MSE=0.0268479, MAE=0.127013\n",
      "[16/30] training 97.0%: Loss=0.0264482, MSE=0.0264482, MAE=0.126613\n",
      "-----------------------------------Finished Epoch 16/30: Loss=111.423, RMSE=1.55739, MAE=1.91779, r_2=-1.84503, p=0.476608\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0329264, MSE=0.0329264, MAE=0.146893\n",
      "[17/30] training 39.4%: Loss=0.0315185, MSE=0.0315185, MAE=0.140594\n",
      "[17/30] training 57.6%: Loss=0.0290758, MSE=0.0290758, MAE=0.136535\n",
      "[17/30] training 75.8%: Loss=0.026653, MSE=0.026653, MAE=0.12728\n",
      "[17/30] training 97.0%: Loss=0.0268545, MSE=0.0268545, MAE=0.128244\n",
      "-----------------------------------Finished Epoch 17/30: Loss=111.263, RMSE=1.53168, MAE=1.90584, r_2=-1.83949, p=0.494815\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.025531, MSE=0.025531, MAE=0.126072\n",
      "[18/30] training 39.4%: Loss=0.0242915, MSE=0.0242915, MAE=0.123871\n",
      "[18/30] training 57.6%: Loss=0.025481, MSE=0.025481, MAE=0.125715\n",
      "[18/30] training 75.8%: Loss=0.0245632, MSE=0.0245632, MAE=0.123022\n",
      "[18/30] training 97.0%: Loss=0.0246787, MSE=0.0246787, MAE=0.123811\n",
      "-----------------------------------Finished Epoch 18/30: Loss=111.475, RMSE=1.5353, MAE=1.88193, r_2=-1.53843, p=0.502469\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0207688, MSE=0.0207688, MAE=0.11238\n",
      "[19/30] training 39.4%: Loss=0.0205862, MSE=0.0205862, MAE=0.111923\n",
      "[19/30] training 57.6%: Loss=0.0240417, MSE=0.0240417, MAE=0.119536\n",
      "[19/30] training 75.8%: Loss=0.0254885, MSE=0.0254885, MAE=0.123929\n",
      "[19/30] training 97.0%: Loss=0.0254882, MSE=0.0254882, MAE=0.124288\n",
      "-----------------------------------Finished Epoch 19/30: Loss=111.013, RMSE=1.53693, MAE=1.93276, r_2=-1.84718, p=0.495552\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0245613, MSE=0.0245613, MAE=0.127381\n",
      "[20/30] training 39.4%: Loss=0.0233784, MSE=0.0233784, MAE=0.123796\n",
      "[20/30] training 57.6%: Loss=0.0234097, MSE=0.0234097, MAE=0.121538\n",
      "[20/30] training 75.8%: Loss=0.0254027, MSE=0.0254027, MAE=0.125826\n",
      "[20/30] training 97.0%: Loss=0.0240966, MSE=0.0240966, MAE=0.121418\n",
      "-----------------------------------Finished Epoch 20/30: Loss=111.159, RMSE=1.51724, MAE=1.9035, r_2=-1.66103, p=0.504608\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0260652, MSE=0.0260652, MAE=0.129326\n",
      "[21/30] training 39.4%: Loss=0.0244787, MSE=0.0244787, MAE=0.122465\n",
      "[21/30] training 57.6%: Loss=0.0233786, MSE=0.0233786, MAE=0.117821\n",
      "[21/30] training 75.8%: Loss=0.0238107, MSE=0.0238107, MAE=0.11918\n",
      "[21/30] training 97.0%: Loss=0.0238529, MSE=0.0238529, MAE=0.119442\n",
      "-----------------------------------Finished Epoch 21/30: Loss=111.102, RMSE=1.53109, MAE=1.9194, r_2=-1.80527, p=0.497294\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0250828, MSE=0.0250828, MAE=0.125539\n",
      "[22/30] training 39.4%: Loss=0.0241384, MSE=0.0241384, MAE=0.123312\n",
      "[22/30] training 57.6%: Loss=0.0256037, MSE=0.0256037, MAE=0.12654\n",
      "[22/30] training 75.8%: Loss=0.025117, MSE=0.025117, MAE=0.124894\n",
      "[22/30] training 97.0%: Loss=0.0250209, MSE=0.0250209, MAE=0.123607\n",
      "-----------------------------------Finished Epoch 22/30: Loss=111.33, RMSE=1.53025, MAE=1.892, r_2=-1.50103, p=0.502354\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0254207, MSE=0.0254207, MAE=0.12395\n",
      "[23/30] training 39.4%: Loss=0.0240423, MSE=0.0240423, MAE=0.122078\n",
      "[23/30] training 57.6%: Loss=0.0254485, MSE=0.0254485, MAE=0.125364\n",
      "[23/30] training 75.8%: Loss=0.0244245, MSE=0.0244245, MAE=0.122581\n",
      "[23/30] training 97.0%: Loss=0.023704, MSE=0.023704, MAE=0.120524\n",
      "-----------------------------------Finished Epoch 23/30: Loss=111.136, RMSE=1.52685, MAE=1.91072, r_2=-1.46005, p=0.502533\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.0226769, MSE=0.0226769, MAE=0.113267\n",
      "[24/30] training 39.4%: Loss=0.0232048, MSE=0.0232048, MAE=0.117493\n",
      "[24/30] training 57.6%: Loss=0.022339, MSE=0.022339, MAE=0.11492\n",
      "[24/30] training 75.8%: Loss=0.0219419, MSE=0.0219419, MAE=0.113602\n",
      "[24/30] training 97.0%: Loss=0.0223397, MSE=0.0223397, MAE=0.114819\n",
      "-----------------------------------Finished Epoch 24/30: Loss=111.145, RMSE=1.52988, MAE=1.90866, r_2=-1.56319, p=0.502534\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0226143, MSE=0.0226143, MAE=0.116159\n",
      "[25/30] training 39.4%: Loss=0.0245887, MSE=0.0245887, MAE=0.120559\n",
      "[25/30] training 57.6%: Loss=0.0251798, MSE=0.0251798, MAE=0.123414\n",
      "[25/30] training 75.8%: Loss=0.0239199, MSE=0.0239199, MAE=0.120308\n",
      "[25/30] training 97.0%: Loss=0.0226966, MSE=0.0226966, MAE=0.117236\n",
      "-----------------------------------Finished Epoch 25/30: Loss=110.926, RMSE=1.51734, MAE=1.92852, r_2=-1.58928, p=0.508145\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0225053, MSE=0.0225053, MAE=0.112138\n",
      "[26/30] training 39.4%: Loss=0.0239832, MSE=0.0239832, MAE=0.11763\n",
      "[26/30] training 57.6%: Loss=0.0238954, MSE=0.0238954, MAE=0.118686\n",
      "[26/30] training 75.8%: Loss=0.0241664, MSE=0.0241664, MAE=0.119354\n",
      "[26/30] training 97.0%: Loss=0.0227504, MSE=0.0227504, MAE=0.115562\n",
      "-----------------------------------Finished Epoch 26/30: Loss=111.169, RMSE=1.51665, MAE=1.89262, r_2=-1.25561, p=0.514181\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.01762, MSE=0.01762, MAE=0.0998481\n",
      "[27/30] training 39.4%: Loss=0.0190104, MSE=0.0190104, MAE=0.104888\n",
      "[27/30] training 57.6%: Loss=0.0206652, MSE=0.0206652, MAE=0.10826\n",
      "[27/30] training 75.8%: Loss=0.0215084, MSE=0.0215084, MAE=0.113019\n",
      "[27/30] training 97.0%: Loss=0.0215505, MSE=0.0215505, MAE=0.114561\n",
      "-----------------------------------Finished Epoch 27/30: Loss=111.141, RMSE=1.51715, MAE=1.89813, r_2=-1.30802, p=0.511997\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0233158, MSE=0.0233158, MAE=0.118239\n",
      "[28/30] training 39.4%: Loss=0.0240702, MSE=0.0240702, MAE=0.121894\n",
      "[28/30] training 57.6%: Loss=0.0227762, MSE=0.0227762, MAE=0.119687\n",
      "[28/30] training 75.8%: Loss=0.0230774, MSE=0.0230774, MAE=0.118901\n",
      "[28/30] training 97.0%: Loss=0.0223315, MSE=0.0223315, MAE=0.117278\n",
      "-----------------------------------Finished Epoch 28/30: Loss=110.859, RMSE=1.50873, MAE=1.92483, r_2=-1.32014, p=0.519124\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0194132, MSE=0.0194132, MAE=0.111573\n",
      "[29/30] training 39.4%: Loss=0.0199247, MSE=0.0199247, MAE=0.110344\n",
      "[29/30] training 57.6%: Loss=0.0223557, MSE=0.0223557, MAE=0.11512\n",
      "[29/30] training 75.8%: Loss=0.0228536, MSE=0.0228536, MAE=0.117226\n",
      "[29/30] training 97.0%: Loss=0.0219835, MSE=0.0219835, MAE=0.115998\n",
      "-----------------------------------Finished Epoch 29/30: Loss=110.944, RMSE=1.51891, MAE=1.92257, r_2=-1.29218, p=0.512581\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0254603, MSE=0.0254603, MAE=0.128299\n",
      "[30/30] training 39.4%: Loss=0.0234542, MSE=0.0234542, MAE=0.12224\n",
      "[30/30] training 57.6%: Loss=0.0213835, MSE=0.0213835, MAE=0.114796\n",
      "[30/30] training 75.8%: Loss=0.0220827, MSE=0.0220827, MAE=0.116841\n",
      "[30/30] training 97.0%: Loss=0.0207095, MSE=0.0207095, MAE=0.113215\n",
      "-----------------------------------Finished Epoch 30/30: Loss=111.027, RMSE=1.51855, MAE=1.91202, r_2=-1.10897, p=0.514592\n",
      "Saving final model to saved_models/model_fold0_final.pth\n",
      "******************************** FOLD 1 ******************************\n",
      "******************************** FOLD 1 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0455261, MSE=0.0455261, MAE=0.174592\n",
      "[1/30] training 39.4%: Loss=0.0440654, MSE=0.0440654, MAE=0.169398\n",
      "[1/30] training 57.6%: Loss=0.044256, MSE=0.044256, MAE=0.170683\n",
      "[1/30] training 75.8%: Loss=0.0405546, MSE=0.0405546, MAE=0.161874\n",
      "[1/30] training 97.0%: Loss=0.0406459, MSE=0.0406459, MAE=0.162156\n",
      "-----------------------------------Finished Epoch 1/30: Loss=115.728, RMSE=1.95182, MAE=2.38124, r_2=-115.148, p=0.0806349\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0362467, MSE=0.0362467, MAE=0.146873\n",
      "[2/30] training 39.4%: Loss=0.0405895, MSE=0.0405895, MAE=0.159517\n",
      "[2/30] training 57.6%: Loss=0.038528, MSE=0.038528, MAE=0.156805\n",
      "[2/30] training 75.8%: Loss=0.0374514, MSE=0.0374514, MAE=0.154949\n",
      "[2/30] training 97.0%: Loss=0.0364332, MSE=0.0364332, MAE=0.152707\n",
      "-----------------------------------Finished Epoch 2/30: Loss=115.612, RMSE=1.86028, MAE=2.24031, r_2=-19.2176, p=0.39542\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0345064, MSE=0.0345064, MAE=0.148685\n",
      "[3/30] training 39.4%: Loss=0.0342928, MSE=0.0342928, MAE=0.147647\n",
      "[3/30] training 57.6%: Loss=0.0330859, MSE=0.0330859, MAE=0.145097\n",
      "[3/30] training 75.8%: Loss=0.0353254, MSE=0.0353254, MAE=0.149264\n",
      "[3/30] training 97.0%: Loss=0.0340838, MSE=0.0340838, MAE=0.146869\n",
      "-----------------------------------Finished Epoch 3/30: Loss=115.798, RMSE=1.77601, MAE=2.14186, r_2=-7.86424, p=0.474684\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0310414, MSE=0.0310414, MAE=0.136185\n",
      "[4/30] training 39.4%: Loss=0.0296844, MSE=0.0296844, MAE=0.133633\n",
      "[4/30] training 57.6%: Loss=0.0321528, MSE=0.0321528, MAE=0.140338\n",
      "[4/30] training 75.8%: Loss=0.03341, MSE=0.03341, MAE=0.144782\n",
      "[4/30] training 97.0%: Loss=0.0316879, MSE=0.0316879, MAE=0.140689\n",
      "-----------------------------------Finished Epoch 4/30: Loss=115.852, RMSE=1.71849, MAE=2.08861, r_2=-4.85794, p=0.506888\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0247664, MSE=0.0247664, MAE=0.125385\n",
      "[5/30] training 39.4%: Loss=0.029625, MSE=0.029625, MAE=0.13531\n",
      "[5/30] training 57.6%: Loss=0.0295836, MSE=0.0295836, MAE=0.133677\n",
      "[5/30] training 75.8%: Loss=0.029131, MSE=0.029131, MAE=0.133372\n",
      "[5/30] training 97.0%: Loss=0.0298938, MSE=0.0298938, MAE=0.135687\n",
      "-----------------------------------Finished Epoch 5/30: Loss=115.675, RMSE=1.67479, MAE=2.0474, r_2=-4.24319, p=0.540471\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0315518, MSE=0.0315518, MAE=0.144175\n",
      "[6/30] training 39.4%: Loss=0.0285711, MSE=0.0285711, MAE=0.136405\n",
      "[6/30] training 57.6%: Loss=0.0295958, MSE=0.0295958, MAE=0.137019\n",
      "[6/30] training 75.8%: Loss=0.0294232, MSE=0.0294232, MAE=0.136146\n",
      "[6/30] training 97.0%: Loss=0.0295769, MSE=0.0295769, MAE=0.13625\n",
      "-----------------------------------Finished Epoch 6/30: Loss=115.75, RMSE=1.64516, MAE=2.02279, r_2=-3.29624, p=0.54991\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0362894, MSE=0.0362894, MAE=0.15919\n",
      "[7/30] training 39.4%: Loss=0.0335134, MSE=0.0335134, MAE=0.146473\n",
      "[7/30] training 57.6%: Loss=0.0297525, MSE=0.0297525, MAE=0.136609\n",
      "[7/30] training 75.8%: Loss=0.0298143, MSE=0.0298143, MAE=0.135488\n",
      "[7/30] training 97.0%: Loss=0.0298647, MSE=0.0298647, MAE=0.136194\n",
      "-----------------------------------Finished Epoch 7/30: Loss=115.947, RMSE=1.62558, MAE=2.01232, r_2=-3.15264, p=0.562219\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.038909, MSE=0.038909, MAE=0.156513\n",
      "[8/30] training 39.4%: Loss=0.0330734, MSE=0.0330734, MAE=0.143691\n",
      "[8/30] training 57.6%: Loss=0.0293195, MSE=0.0293195, MAE=0.135794\n",
      "[8/30] training 75.8%: Loss=0.0291679, MSE=0.0291679, MAE=0.134841\n",
      "[8/30] training 97.0%: Loss=0.0294193, MSE=0.0294193, MAE=0.13494\n",
      "-----------------------------------Finished Epoch 8/30: Loss=115.416, RMSE=1.61552, MAE=1.97553, r_2=-3.02164, p=0.59217\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0284905, MSE=0.0284905, MAE=0.133178\n",
      "[9/30] training 39.4%: Loss=0.0275218, MSE=0.0275218, MAE=0.130278\n",
      "[9/30] training 57.6%: Loss=0.0275024, MSE=0.0275024, MAE=0.131489\n",
      "[9/30] training 75.8%: Loss=0.0284775, MSE=0.0284775, MAE=0.134101\n",
      "[9/30] training 97.0%: Loss=0.0274677, MSE=0.0274677, MAE=0.131782\n",
      "-----------------------------------Finished Epoch 9/30: Loss=115.909, RMSE=1.56515, MAE=1.95528, r_2=-2.41694, p=0.59786\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0332762, MSE=0.0332762, MAE=0.14386\n",
      "[10/30] training 39.4%: Loss=0.0289805, MSE=0.0289805, MAE=0.133254\n",
      "[10/30] training 57.6%: Loss=0.0286754, MSE=0.0286754, MAE=0.134166\n",
      "[10/30] training 75.8%: Loss=0.0285781, MSE=0.0285781, MAE=0.134474\n",
      "[10/30] training 97.0%: Loss=0.0276976, MSE=0.0276976, MAE=0.132008\n",
      "-----------------------------------Finished Epoch 10/30: Loss=115.547, RMSE=1.5547, MAE=1.93101, r_2=-2.17642, p=0.608255\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0248428, MSE=0.0248428, MAE=0.126529\n",
      "[11/30] training 39.4%: Loss=0.0253111, MSE=0.0253111, MAE=0.126537\n",
      "[11/30] training 57.6%: Loss=0.0254266, MSE=0.0254266, MAE=0.123899\n",
      "[11/30] training 75.8%: Loss=0.0282546, MSE=0.0282546, MAE=0.131201\n",
      "[11/30] training 97.0%: Loss=0.0275129, MSE=0.0275129, MAE=0.130535\n",
      "-----------------------------------Finished Epoch 11/30: Loss=115.585, RMSE=1.55596, MAE=1.93344, r_2=-2.21802, p=0.606982\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0286036, MSE=0.0286036, MAE=0.12976\n",
      "[12/30] training 39.4%: Loss=0.0267986, MSE=0.0267986, MAE=0.126654\n",
      "[12/30] training 57.6%: Loss=0.026423, MSE=0.026423, MAE=0.125329\n",
      "[12/30] training 75.8%: Loss=0.0256762, MSE=0.0256762, MAE=0.124011\n",
      "[12/30] training 97.0%: Loss=0.0263242, MSE=0.0263242, MAE=0.125162\n",
      "-----------------------------------Finished Epoch 12/30: Loss=115.584, RMSE=1.54875, MAE=1.92298, r_2=-2.12373, p=0.613289\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0248856, MSE=0.0248856, MAE=0.124513\n",
      "[13/30] training 39.4%: Loss=0.0258435, MSE=0.0258435, MAE=0.127332\n",
      "[13/30] training 57.6%: Loss=0.0251495, MSE=0.0251495, MAE=0.124671\n",
      "[13/30] training 75.8%: Loss=0.0276985, MSE=0.0276985, MAE=0.131543\n",
      "[13/30] training 97.0%: Loss=0.0265969, MSE=0.0265969, MAE=0.128141\n",
      "-----------------------------------Finished Epoch 13/30: Loss=115.677, RMSE=1.52455, MAE=1.90309, r_2=-1.87093, p=0.622985\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0226329, MSE=0.0226329, MAE=0.116597\n",
      "[14/30] training 39.4%: Loss=0.0241249, MSE=0.0241249, MAE=0.120974\n",
      "[14/30] training 57.6%: Loss=0.0263819, MSE=0.0263819, MAE=0.126136\n",
      "[14/30] training 75.8%: Loss=0.0262112, MSE=0.0262112, MAE=0.126685\n",
      "[14/30] training 97.0%: Loss=0.025633, MSE=0.025633, MAE=0.125869\n",
      "-----------------------------------Finished Epoch 14/30: Loss=115.383, RMSE=1.53254, MAE=1.90329, r_2=-1.80582, p=0.623736\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0289996, MSE=0.0289996, MAE=0.133162\n",
      "[15/30] training 39.4%: Loss=0.0269363, MSE=0.0269363, MAE=0.12609\n",
      "[15/30] training 57.6%: Loss=0.0264192, MSE=0.0264192, MAE=0.124058\n",
      "[15/30] training 75.8%: Loss=0.0273748, MSE=0.0273748, MAE=0.128007\n",
      "[15/30] training 97.0%: Loss=0.0263746, MSE=0.0263746, MAE=0.126246\n",
      "-----------------------------------Finished Epoch 15/30: Loss=115.535, RMSE=1.51662, MAE=1.89132, r_2=-1.61959, p=0.625375\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0206103, MSE=0.0206103, MAE=0.115126\n",
      "[16/30] training 39.4%: Loss=0.0237158, MSE=0.0237158, MAE=0.12033\n",
      "[16/30] training 57.6%: Loss=0.0230429, MSE=0.0230429, MAE=0.120504\n",
      "[16/30] training 75.8%: Loss=0.0238463, MSE=0.0238463, MAE=0.121145\n",
      "[16/30] training 97.0%: Loss=0.0241904, MSE=0.0241904, MAE=0.121418\n",
      "-----------------------------------Finished Epoch 16/30: Loss=115.817, RMSE=1.50946, MAE=1.89634, r_2=-1.49353, p=0.621228\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0236318, MSE=0.0236318, MAE=0.125062\n",
      "[17/30] training 39.4%: Loss=0.0229226, MSE=0.0229226, MAE=0.121492\n",
      "[17/30] training 57.6%: Loss=0.0233472, MSE=0.0233472, MAE=0.120793\n",
      "[17/30] training 75.8%: Loss=0.0235858, MSE=0.0235858, MAE=0.120992\n",
      "[17/30] training 97.0%: Loss=0.0256147, MSE=0.0256147, MAE=0.126623\n",
      "-----------------------------------Finished Epoch 17/30: Loss=115.797, RMSE=1.50861, MAE=1.88872, r_2=-1.38845, p=0.623871\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0230835, MSE=0.0230835, MAE=0.121243\n",
      "[18/30] training 39.4%: Loss=0.0238653, MSE=0.0238653, MAE=0.121361\n",
      "[18/30] training 57.6%: Loss=0.0235142, MSE=0.0235142, MAE=0.119751\n",
      "[18/30] training 75.8%: Loss=0.023899, MSE=0.023899, MAE=0.12095\n",
      "[18/30] training 97.0%: Loss=0.0238304, MSE=0.0238304, MAE=0.120737\n",
      "-----------------------------------Finished Epoch 18/30: Loss=115.574, RMSE=1.48972, MAE=1.86866, r_2=-1.23058, p=0.631603\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0297009, MSE=0.0297009, MAE=0.134566\n",
      "[19/30] training 39.4%: Loss=0.0252457, MSE=0.0252457, MAE=0.122176\n",
      "[19/30] training 57.6%: Loss=0.0239421, MSE=0.0239421, MAE=0.11986\n",
      "[19/30] training 75.8%: Loss=0.0234991, MSE=0.0234991, MAE=0.119386\n",
      "[19/30] training 97.0%: Loss=0.0236241, MSE=0.0236241, MAE=0.120244\n",
      "-----------------------------------Finished Epoch 19/30: Loss=115.526, RMSE=1.49254, MAE=1.86602, r_2=-1.13469, p=0.631236\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0248587, MSE=0.0248587, MAE=0.121701\n",
      "[20/30] training 39.4%: Loss=0.0244761, MSE=0.0244761, MAE=0.122896\n",
      "[20/30] training 57.6%: Loss=0.024608, MSE=0.024608, MAE=0.122909\n",
      "[20/30] training 75.8%: Loss=0.0242311, MSE=0.0242311, MAE=0.121808\n",
      "[20/30] training 97.0%: Loss=0.0238547, MSE=0.0238547, MAE=0.1219\n",
      "-----------------------------------Finished Epoch 20/30: Loss=115.793, RMSE=1.50949, MAE=1.88334, r_2=-1.2086, p=0.623711\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0176234, MSE=0.0176234, MAE=0.105959\n",
      "[21/30] training 39.4%: Loss=0.0194953, MSE=0.0194953, MAE=0.112039\n",
      "[21/30] training 57.6%: Loss=0.0218154, MSE=0.0218154, MAE=0.117844\n",
      "[21/30] training 75.8%: Loss=0.0224781, MSE=0.0224781, MAE=0.119605\n",
      "[21/30] training 97.0%: Loss=0.022526, MSE=0.022526, MAE=0.118221\n",
      "-----------------------------------Finished Epoch 21/30: Loss=115.751, RMSE=1.46127, MAE=1.85932, r_2=-1.2783, p=0.640992\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.020636, MSE=0.020636, MAE=0.11158\n",
      "[22/30] training 39.4%: Loss=0.0213703, MSE=0.0213703, MAE=0.117531\n",
      "[22/30] training 57.6%: Loss=0.0219638, MSE=0.0219638, MAE=0.11726\n",
      "[22/30] training 75.8%: Loss=0.0214293, MSE=0.0214293, MAE=0.11595\n",
      "[22/30] training 97.0%: Loss=0.022176, MSE=0.022176, MAE=0.118689\n",
      "-----------------------------------Finished Epoch 22/30: Loss=115.475, RMSE=1.47955, MAE=1.85921, r_2=-1.17514, p=0.637134\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0213277, MSE=0.0213277, MAE=0.110797\n",
      "[23/30] training 39.4%: Loss=0.0212468, MSE=0.0212468, MAE=0.112406\n",
      "[23/30] training 57.6%: Loss=0.0202882, MSE=0.0202882, MAE=0.111178\n",
      "[23/30] training 75.8%: Loss=0.0210932, MSE=0.0210932, MAE=0.114639\n",
      "[23/30] training 97.0%: Loss=0.0221912, MSE=0.0221912, MAE=0.11706\n",
      "-----------------------------------Finished Epoch 23/30: Loss=115.588, RMSE=1.47205, MAE=1.85324, r_2=-1.08585, p=0.638485\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.023026, MSE=0.023026, MAE=0.113514\n",
      "[24/30] training 39.4%: Loss=0.0237018, MSE=0.0237018, MAE=0.12139\n",
      "[24/30] training 57.6%: Loss=0.0244645, MSE=0.0244645, MAE=0.12374\n",
      "[24/30] training 75.8%: Loss=0.0242709, MSE=0.0242709, MAE=0.122976\n",
      "[24/30] training 97.0%: Loss=0.0226256, MSE=0.0226256, MAE=0.117519\n",
      "-----------------------------------Finished Epoch 24/30: Loss=115.236, RMSE=1.48718, MAE=1.86047, r_2=-1.08159, p=0.639105\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.022576, MSE=0.022576, MAE=0.119547\n",
      "[25/30] training 39.4%: Loss=0.0211778, MSE=0.0211778, MAE=0.116241\n",
      "[25/30] training 57.6%: Loss=0.0213232, MSE=0.0213232, MAE=0.114559\n",
      "[25/30] training 75.8%: Loss=0.0232143, MSE=0.0232143, MAE=0.118644\n",
      "[25/30] training 97.0%: Loss=0.0220176, MSE=0.0220176, MAE=0.11652\n",
      "-----------------------------------Finished Epoch 25/30: Loss=115.188, RMSE=1.48437, MAE=1.85357, r_2=-1.01733, p=0.643578\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.019983, MSE=0.019983, MAE=0.106326\n",
      "[26/30] training 39.4%: Loss=0.0198281, MSE=0.0198281, MAE=0.107603\n",
      "[26/30] training 57.6%: Loss=0.021087, MSE=0.021087, MAE=0.11292\n",
      "[26/30] training 75.8%: Loss=0.0209766, MSE=0.0209766, MAE=0.111889\n",
      "[26/30] training 97.0%: Loss=0.0220324, MSE=0.0220324, MAE=0.115387\n",
      "-----------------------------------Finished Epoch 26/30: Loss=115.237, RMSE=1.48199, MAE=1.85132, r_2=-0.933988, p=0.641213\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0218866, MSE=0.0218866, MAE=0.119751\n",
      "[27/30] training 39.4%: Loss=0.0217648, MSE=0.0217648, MAE=0.118933\n",
      "[27/30] training 57.6%: Loss=0.0217852, MSE=0.0217852, MAE=0.118681\n",
      "[27/30] training 75.8%: Loss=0.0215421, MSE=0.0215421, MAE=0.116108\n",
      "[27/30] training 97.0%: Loss=0.0206732, MSE=0.0206732, MAE=0.113276\n",
      "-----------------------------------Finished Epoch 27/30: Loss=115.372, RMSE=1.48925, MAE=1.85426, r_2=-0.911444, p=0.635614\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0162378, MSE=0.0162378, MAE=0.10162\n",
      "[28/30] training 39.4%: Loss=0.0193163, MSE=0.0193163, MAE=0.107615\n",
      "[28/30] training 57.6%: Loss=0.0195211, MSE=0.0195211, MAE=0.110369\n",
      "[28/30] training 75.8%: Loss=0.0200777, MSE=0.0200777, MAE=0.111156\n",
      "[28/30] training 97.0%: Loss=0.0203855, MSE=0.0203855, MAE=0.11211\n",
      "-----------------------------------Finished Epoch 28/30: Loss=115.437, RMSE=1.45762, MAE=1.83542, r_2=-0.876041, p=0.645535\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0202715, MSE=0.0202715, MAE=0.108772\n",
      "[29/30] training 39.4%: Loss=0.0202126, MSE=0.0202126, MAE=0.109816\n",
      "[29/30] training 57.6%: Loss=0.0206456, MSE=0.0206456, MAE=0.111129\n",
      "[29/30] training 75.8%: Loss=0.0199537, MSE=0.0199537, MAE=0.109897\n",
      "[29/30] training 97.0%: Loss=0.0207179, MSE=0.0207179, MAE=0.11154\n",
      "-----------------------------------Finished Epoch 29/30: Loss=115.148, RMSE=1.46729, MAE=1.83618, r_2=-0.767373, p=0.649503\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0262794, MSE=0.0262794, MAE=0.127865\n",
      "[30/30] training 39.4%: Loss=0.0236491, MSE=0.0236491, MAE=0.119797\n",
      "[30/30] training 57.6%: Loss=0.0209715, MSE=0.0209715, MAE=0.110614\n",
      "[30/30] training 75.8%: Loss=0.0201921, MSE=0.0201921, MAE=0.108983\n",
      "[30/30] training 97.0%: Loss=0.0204385, MSE=0.0204385, MAE=0.110184\n",
      "-----------------------------------Finished Epoch 30/30: Loss=115.418, RMSE=1.44926, MAE=1.81938, r_2=-0.630697, p=0.650154\n",
      "Saving final model to saved_models/model_fold1_final.pth\n",
      "******************************** FOLD 2 ******************************\n",
      "******************************** FOLD 2 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0392074, MSE=0.0392074, MAE=0.150894\n",
      "[1/30] training 39.4%: Loss=0.0417889, MSE=0.0417889, MAE=0.154831\n",
      "[1/30] training 57.6%: Loss=0.04194, MSE=0.04194, MAE=0.155936\n",
      "[1/30] training 75.8%: Loss=0.0450915, MSE=0.0450915, MAE=0.165701\n",
      "[1/30] training 97.0%: Loss=0.0427456, MSE=0.0427456, MAE=0.161953\n",
      "-----------------------------------Finished Epoch 1/30: Loss=124.223, RMSE=1.54963, MAE=1.95671, r_2=-45.3313, p=0.181971\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0475624, MSE=0.0475624, MAE=0.168856\n",
      "[2/30] training 39.4%: Loss=0.0425047, MSE=0.0425047, MAE=0.161134\n",
      "[2/30] training 57.6%: Loss=0.0437572, MSE=0.0437572, MAE=0.165833\n",
      "[2/30] training 75.8%: Loss=0.0417336, MSE=0.0417336, MAE=0.160626\n",
      "[2/30] training 97.0%: Loss=0.0393471, MSE=0.0393471, MAE=0.155857\n",
      "-----------------------------------Finished Epoch 2/30: Loss=123.97, RMSE=1.4471, MAE=1.81804, r_2=-8.97643, p=0.396001\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0424711, MSE=0.0424711, MAE=0.161755\n",
      "[3/30] training 39.4%: Loss=0.038827, MSE=0.038827, MAE=0.153632\n",
      "[3/30] training 57.6%: Loss=0.0337547, MSE=0.0337547, MAE=0.14122\n",
      "[3/30] training 75.8%: Loss=0.0354194, MSE=0.0354194, MAE=0.146173\n",
      "[3/30] training 97.0%: Loss=0.0362827, MSE=0.0362827, MAE=0.149873\n",
      "-----------------------------------Finished Epoch 3/30: Loss=123.886, RMSE=1.39747, MAE=1.75657, r_2=-4.5881, p=0.453842\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0434976, MSE=0.0434976, MAE=0.165622\n",
      "[4/30] training 39.4%: Loss=0.039433, MSE=0.039433, MAE=0.158461\n",
      "[4/30] training 57.6%: Loss=0.0388265, MSE=0.0388265, MAE=0.156181\n",
      "[4/30] training 75.8%: Loss=0.0360735, MSE=0.0360735, MAE=0.149198\n",
      "[4/30] training 97.0%: Loss=0.0353598, MSE=0.0353598, MAE=0.149564\n",
      "-----------------------------------Finished Epoch 4/30: Loss=123.794, RMSE=1.36336, MAE=1.72658, r_2=-3.33397, p=0.472979\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0262634, MSE=0.0262634, MAE=0.13097\n",
      "[5/30] training 39.4%: Loss=0.0306054, MSE=0.0306054, MAE=0.137492\n",
      "[5/30] training 57.6%: Loss=0.0320827, MSE=0.0320827, MAE=0.139674\n",
      "[5/30] training 75.8%: Loss=0.0327599, MSE=0.0327599, MAE=0.140738\n",
      "[5/30] training 97.0%: Loss=0.0331759, MSE=0.0331759, MAE=0.1435\n",
      "-----------------------------------Finished Epoch 5/30: Loss=123.768, RMSE=1.33816, MAE=1.70837, r_2=-2.60265, p=0.488056\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0361715, MSE=0.0361715, MAE=0.149523\n",
      "[6/30] training 39.4%: Loss=0.0366365, MSE=0.0366365, MAE=0.150333\n",
      "[6/30] training 57.6%: Loss=0.0345015, MSE=0.0345015, MAE=0.146005\n",
      "[6/30] training 75.8%: Loss=0.0347402, MSE=0.0347402, MAE=0.146815\n",
      "[6/30] training 97.0%: Loss=0.0338981, MSE=0.0338981, MAE=0.144377\n",
      "-----------------------------------Finished Epoch 6/30: Loss=123.829, RMSE=1.32832, MAE=1.6992, r_2=-2.25776, p=0.50367\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0325693, MSE=0.0325693, MAE=0.141561\n",
      "[7/30] training 39.4%: Loss=0.0298217, MSE=0.0298217, MAE=0.137158\n",
      "[7/30] training 57.6%: Loss=0.0296995, MSE=0.0296995, MAE=0.135902\n",
      "[7/30] training 75.8%: Loss=0.0319834, MSE=0.0319834, MAE=0.140624\n",
      "[7/30] training 97.0%: Loss=0.0337502, MSE=0.0337502, MAE=0.144234\n",
      "-----------------------------------Finished Epoch 7/30: Loss=123.883, RMSE=1.31664, MAE=1.69265, r_2=-1.87154, p=0.516779\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.0307267, MSE=0.0307267, MAE=0.144804\n",
      "[8/30] training 39.4%: Loss=0.0310861, MSE=0.0310861, MAE=0.141067\n",
      "[8/30] training 57.6%: Loss=0.0317696, MSE=0.0317696, MAE=0.141737\n",
      "[8/30] training 75.8%: Loss=0.0306541, MSE=0.0306541, MAE=0.139288\n",
      "[8/30] training 97.0%: Loss=0.0306152, MSE=0.0306152, MAE=0.139926\n",
      "-----------------------------------Finished Epoch 8/30: Loss=123.824, RMSE=1.29955, MAE=1.6822, r_2=-1.90644, p=0.519486\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0284558, MSE=0.0284558, MAE=0.132968\n",
      "[9/30] training 39.4%: Loss=0.0312256, MSE=0.0312256, MAE=0.138158\n",
      "[9/30] training 57.6%: Loss=0.030084, MSE=0.030084, MAE=0.137195\n",
      "[9/30] training 75.8%: Loss=0.0314767, MSE=0.0314767, MAE=0.140516\n",
      "[9/30] training 97.0%: Loss=0.0311703, MSE=0.0311703, MAE=0.139805\n",
      "-----------------------------------Finished Epoch 9/30: Loss=123.665, RMSE=1.26432, MAE=1.65203, r_2=-1.57334, p=0.531828\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0246555, MSE=0.0246555, MAE=0.12654\n",
      "[10/30] training 39.4%: Loss=0.0264819, MSE=0.0264819, MAE=0.128566\n",
      "[10/30] training 57.6%: Loss=0.0286843, MSE=0.0286843, MAE=0.132887\n",
      "[10/30] training 75.8%: Loss=0.0282649, MSE=0.0282649, MAE=0.132432\n",
      "[10/30] training 97.0%: Loss=0.0287226, MSE=0.0287226, MAE=0.133117\n",
      "-----------------------------------Finished Epoch 10/30: Loss=123.952, RMSE=1.27339, MAE=1.67854, r_2=-1.51255, p=0.539222\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0314106, MSE=0.0314106, MAE=0.140018\n",
      "[11/30] training 39.4%: Loss=0.030544, MSE=0.030544, MAE=0.140179\n",
      "[11/30] training 57.6%: Loss=0.0277099, MSE=0.0277099, MAE=0.133643\n",
      "[11/30] training 75.8%: Loss=0.0277091, MSE=0.0277091, MAE=0.132333\n",
      "[11/30] training 97.0%: Loss=0.0290244, MSE=0.0290244, MAE=0.135653\n",
      "-----------------------------------Finished Epoch 11/30: Loss=123.892, RMSE=1.27558, MAE=1.67743, r_2=-1.46876, p=0.533403\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0322235, MSE=0.0322235, MAE=0.146225\n",
      "[12/30] training 39.4%: Loss=0.0321367, MSE=0.0321367, MAE=0.143253\n",
      "[12/30] training 57.6%: Loss=0.0294862, MSE=0.0294862, MAE=0.13708\n",
      "[12/30] training 75.8%: Loss=0.0287145, MSE=0.0287145, MAE=0.135285\n",
      "[12/30] training 97.0%: Loss=0.0282462, MSE=0.0282462, MAE=0.133509\n",
      "-----------------------------------Finished Epoch 12/30: Loss=123.721, RMSE=1.24609, MAE=1.65203, r_2=-1.16806, p=0.539831\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0300544, MSE=0.0300544, MAE=0.132705\n",
      "[13/30] training 39.4%: Loss=0.0272476, MSE=0.0272476, MAE=0.12738\n",
      "[13/30] training 57.6%: Loss=0.0286111, MSE=0.0286111, MAE=0.132901\n",
      "[13/30] training 75.8%: Loss=0.0269571, MSE=0.0269571, MAE=0.12989\n",
      "[13/30] training 97.0%: Loss=0.0279877, MSE=0.0279877, MAE=0.131817\n",
      "-----------------------------------Finished Epoch 13/30: Loss=123.695, RMSE=1.24098, MAE=1.65302, r_2=-1.04482, p=0.538991\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.029272, MSE=0.029272, MAE=0.138398\n",
      "[14/30] training 39.4%: Loss=0.0270502, MSE=0.0270502, MAE=0.131145\n",
      "[14/30] training 57.6%: Loss=0.0270047, MSE=0.0270047, MAE=0.130132\n",
      "[14/30] training 75.8%: Loss=0.0275698, MSE=0.0275698, MAE=0.13013\n",
      "[14/30] training 97.0%: Loss=0.0271252, MSE=0.0271252, MAE=0.129406\n",
      "-----------------------------------Finished Epoch 14/30: Loss=123.741, RMSE=1.22865, MAE=1.63519, r_2=-1.06551, p=0.555536\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0273849, MSE=0.0273849, MAE=0.131741\n",
      "[15/30] training 39.4%: Loss=0.0253917, MSE=0.0253917, MAE=0.123272\n",
      "[15/30] training 57.6%: Loss=0.0259322, MSE=0.0259322, MAE=0.125897\n",
      "[15/30] training 75.8%: Loss=0.0258332, MSE=0.0258332, MAE=0.125406\n",
      "[15/30] training 97.0%: Loss=0.027893, MSE=0.027893, MAE=0.130666\n",
      "-----------------------------------Finished Epoch 15/30: Loss=123.778, RMSE=1.2154, MAE=1.62409, r_2=-0.976542, p=0.568121\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.027856, MSE=0.027856, MAE=0.129607\n",
      "[16/30] training 39.4%: Loss=0.0292873, MSE=0.0292873, MAE=0.135924\n",
      "[16/30] training 57.6%: Loss=0.0290571, MSE=0.0290571, MAE=0.134792\n",
      "[16/30] training 75.8%: Loss=0.0282679, MSE=0.0282679, MAE=0.133144\n",
      "[16/30] training 97.0%: Loss=0.0270489, MSE=0.0270489, MAE=0.130263\n",
      "-----------------------------------Finished Epoch 16/30: Loss=123.72, RMSE=1.20758, MAE=1.619, r_2=-0.937094, p=0.567229\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0251649, MSE=0.0251649, MAE=0.12468\n",
      "[17/30] training 39.4%: Loss=0.0313539, MSE=0.0313539, MAE=0.140545\n",
      "[17/30] training 57.6%: Loss=0.0312315, MSE=0.0312315, MAE=0.141067\n",
      "[17/30] training 75.8%: Loss=0.0283812, MSE=0.0283812, MAE=0.134204\n",
      "[17/30] training 97.0%: Loss=0.0269191, MSE=0.0269191, MAE=0.131021\n",
      "-----------------------------------Finished Epoch 17/30: Loss=123.462, RMSE=1.17067, MAE=1.59139, r_2=-0.844891, p=0.570795\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0232389, MSE=0.0232389, MAE=0.113364\n",
      "[18/30] training 39.4%: Loss=0.0237069, MSE=0.0237069, MAE=0.119124\n",
      "[18/30] training 57.6%: Loss=0.0245197, MSE=0.0245197, MAE=0.123363\n",
      "[18/30] training 75.8%: Loss=0.0242334, MSE=0.0242334, MAE=0.123166\n",
      "[18/30] training 97.0%: Loss=0.0253081, MSE=0.0253081, MAE=0.12524\n",
      "-----------------------------------Finished Epoch 18/30: Loss=123.504, RMSE=1.1707, MAE=1.60725, r_2=-0.707797, p=0.565026\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.024238, MSE=0.024238, MAE=0.122546\n",
      "[19/30] training 39.4%: Loss=0.028731, MSE=0.028731, MAE=0.132962\n",
      "[19/30] training 57.6%: Loss=0.0278525, MSE=0.0278525, MAE=0.130926\n",
      "[19/30] training 75.8%: Loss=0.0259573, MSE=0.0259573, MAE=0.126589\n",
      "[19/30] training 97.0%: Loss=0.0259122, MSE=0.0259122, MAE=0.126979\n",
      "-----------------------------------Finished Epoch 19/30: Loss=123.72, RMSE=1.1918, MAE=1.62371, r_2=-0.889418, p=0.564689\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0289462, MSE=0.0289462, MAE=0.134568\n",
      "[20/30] training 39.4%: Loss=0.0261185, MSE=0.0261185, MAE=0.125824\n",
      "[20/30] training 57.6%: Loss=0.0253036, MSE=0.0253036, MAE=0.125174\n",
      "[20/30] training 75.8%: Loss=0.0245808, MSE=0.0245808, MAE=0.123259\n",
      "[20/30] training 97.0%: Loss=0.0238276, MSE=0.0238276, MAE=0.121246\n",
      "-----------------------------------Finished Epoch 20/30: Loss=123.518, RMSE=1.15966, MAE=1.59446, r_2=-0.753893, p=0.573092\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0272208, MSE=0.0272208, MAE=0.129873\n",
      "[21/30] training 39.4%: Loss=0.0269811, MSE=0.0269811, MAE=0.128115\n",
      "[21/30] training 57.6%: Loss=0.024702, MSE=0.024702, MAE=0.12141\n",
      "[21/30] training 75.8%: Loss=0.0255316, MSE=0.0255316, MAE=0.125053\n",
      "[21/30] training 97.0%: Loss=0.0247265, MSE=0.0247265, MAE=0.123971\n",
      "-----------------------------------Finished Epoch 21/30: Loss=123.183, RMSE=1.11286, MAE=1.57072, r_2=-0.539976, p=0.5816\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0270629, MSE=0.0270629, MAE=0.131537\n",
      "[22/30] training 39.4%: Loss=0.0247814, MSE=0.0247814, MAE=0.123227\n",
      "[22/30] training 57.6%: Loss=0.0230451, MSE=0.0230451, MAE=0.119507\n",
      "[22/30] training 75.8%: Loss=0.0234281, MSE=0.0234281, MAE=0.12085\n",
      "[22/30] training 97.0%: Loss=0.023435, MSE=0.023435, MAE=0.120395\n",
      "-----------------------------------Finished Epoch 22/30: Loss=123.232, RMSE=1.12892, MAE=1.58215, r_2=-0.60188, p=0.573855\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0244053, MSE=0.0244053, MAE=0.124571\n",
      "[23/30] training 39.4%: Loss=0.0232584, MSE=0.0232584, MAE=0.122983\n",
      "[23/30] training 57.6%: Loss=0.0222252, MSE=0.0222252, MAE=0.119207\n",
      "[23/30] training 75.8%: Loss=0.0216481, MSE=0.0216481, MAE=0.116647\n",
      "[23/30] training 97.0%: Loss=0.0218227, MSE=0.0218227, MAE=0.115994\n",
      "-----------------------------------Finished Epoch 23/30: Loss=123.422, RMSE=1.11891, MAE=1.57798, r_2=-0.488804, p=0.585422\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.023519, MSE=0.023519, MAE=0.121973\n",
      "[24/30] training 39.4%: Loss=0.0235924, MSE=0.0235924, MAE=0.124219\n",
      "[24/30] training 57.6%: Loss=0.0234879, MSE=0.0234879, MAE=0.121089\n",
      "[24/30] training 75.8%: Loss=0.0223023, MSE=0.0223023, MAE=0.117299\n",
      "[24/30] training 97.0%: Loss=0.0222594, MSE=0.0222594, MAE=0.117016\n",
      "-----------------------------------Finished Epoch 24/30: Loss=123.485, RMSE=1.1164, MAE=1.57198, r_2=-0.486384, p=0.591937\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0219148, MSE=0.0219148, MAE=0.118057\n",
      "[25/30] training 39.4%: Loss=0.0203652, MSE=0.0203652, MAE=0.11195\n",
      "[25/30] training 57.6%: Loss=0.0218465, MSE=0.0218465, MAE=0.116517\n",
      "[25/30] training 75.8%: Loss=0.0222452, MSE=0.0222452, MAE=0.117483\n",
      "[25/30] training 97.0%: Loss=0.0230561, MSE=0.0230561, MAE=0.119045\n",
      "-----------------------------------Finished Epoch 25/30: Loss=123.255, RMSE=1.09683, MAE=1.55107, r_2=-0.460964, p=0.596577\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0247278, MSE=0.0247278, MAE=0.124007\n",
      "[26/30] training 39.4%: Loss=0.0250658, MSE=0.0250658, MAE=0.123746\n",
      "[26/30] training 57.6%: Loss=0.022772, MSE=0.022772, MAE=0.118923\n",
      "[26/30] training 75.8%: Loss=0.0226277, MSE=0.0226277, MAE=0.118375\n",
      "[26/30] training 97.0%: Loss=0.0214242, MSE=0.0214242, MAE=0.114676\n",
      "-----------------------------------Finished Epoch 26/30: Loss=122.823, RMSE=1.07788, MAE=1.55095, r_2=-0.459711, p=0.598134\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0169716, MSE=0.0169716, MAE=0.100444\n",
      "[27/30] training 39.4%: Loss=0.0190538, MSE=0.0190538, MAE=0.105311\n",
      "[27/30] training 57.6%: Loss=0.0203612, MSE=0.0203612, MAE=0.112277\n",
      "[27/30] training 75.8%: Loss=0.0211021, MSE=0.0211021, MAE=0.114229\n",
      "[27/30] training 97.0%: Loss=0.0211438, MSE=0.0211438, MAE=0.114675\n",
      "-----------------------------------Finished Epoch 27/30: Loss=123.004, RMSE=1.08818, MAE=1.5581, r_2=-0.44431, p=0.591118\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0219683, MSE=0.0219683, MAE=0.119737\n",
      "[28/30] training 39.4%: Loss=0.0219051, MSE=0.0219051, MAE=0.11905\n",
      "[28/30] training 57.6%: Loss=0.0202398, MSE=0.0202398, MAE=0.111749\n",
      "[28/30] training 75.8%: Loss=0.0202334, MSE=0.0202334, MAE=0.111328\n",
      "[28/30] training 97.0%: Loss=0.0209981, MSE=0.0209981, MAE=0.113444\n",
      "-----------------------------------Finished Epoch 28/30: Loss=123.132, RMSE=1.08245, MAE=1.54515, r_2=-0.367993, p=0.600788\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0249693, MSE=0.0249693, MAE=0.126785\n",
      "[29/30] training 39.4%: Loss=0.0224401, MSE=0.0224401, MAE=0.120472\n",
      "[29/30] training 57.6%: Loss=0.021734, MSE=0.021734, MAE=0.118712\n",
      "[29/30] training 75.8%: Loss=0.0236977, MSE=0.0236977, MAE=0.121792\n",
      "[29/30] training 97.0%: Loss=0.0215676, MSE=0.0215676, MAE=0.116074\n",
      "-----------------------------------Finished Epoch 29/30: Loss=123.038, RMSE=1.06618, MAE=1.55058, r_2=-0.294587, p=0.600286\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0177259, MSE=0.0177259, MAE=0.101419\n",
      "[30/30] training 39.4%: Loss=0.0191971, MSE=0.0191971, MAE=0.106557\n",
      "[30/30] training 57.6%: Loss=0.0200522, MSE=0.0200522, MAE=0.108045\n",
      "[30/30] training 75.8%: Loss=0.021261, MSE=0.021261, MAE=0.111895\n",
      "[30/30] training 97.0%: Loss=0.0212564, MSE=0.0212564, MAE=0.112219\n",
      "-----------------------------------Finished Epoch 30/30: Loss=123.205, RMSE=1.05014, MAE=1.52944, r_2=-0.303273, p=0.612498\n",
      "Saving final model to saved_models/model_fold2_final.pth\n",
      "******************************** FOLD 3 ******************************\n",
      "******************************** FOLD 3 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0403229, MSE=0.0403229, MAE=0.156957\n",
      "[1/30] training 39.4%: Loss=0.0358085, MSE=0.0358085, MAE=0.150809\n",
      "[1/30] training 57.6%: Loss=0.0374734, MSE=0.0374734, MAE=0.156449\n",
      "[1/30] training 75.8%: Loss=0.0385234, MSE=0.0385234, MAE=0.159092\n",
      "[1/30] training 97.0%: Loss=0.0367341, MSE=0.0367341, MAE=0.153853\n",
      "-----------------------------------Finished Epoch 1/30: Loss=107.77, RMSE=1.6335, MAE=2.09227, r_2=-41.1555, p=0.256007\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0310706, MSE=0.0310706, MAE=0.147381\n",
      "[2/30] training 39.4%: Loss=0.0342753, MSE=0.0342753, MAE=0.150508\n",
      "[2/30] training 57.6%: Loss=0.034074, MSE=0.034074, MAE=0.147727\n",
      "[2/30] training 75.8%: Loss=0.0341023, MSE=0.0341023, MAE=0.149689\n",
      "[2/30] training 97.0%: Loss=0.0337969, MSE=0.0337969, MAE=0.147946\n",
      "-----------------------------------Finished Epoch 2/30: Loss=107.633, RMSE=1.60216, MAE=2.06016, r_2=-16.0834, p=0.342934\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0442917, MSE=0.0442917, MAE=0.169337\n",
      "[3/30] training 39.4%: Loss=0.0405978, MSE=0.0405978, MAE=0.162656\n",
      "[3/30] training 57.6%: Loss=0.0366011, MSE=0.0366011, MAE=0.152982\n",
      "[3/30] training 75.8%: Loss=0.0354473, MSE=0.0354473, MAE=0.150213\n",
      "[3/30] training 97.0%: Loss=0.0329854, MSE=0.0329854, MAE=0.145683\n",
      "-----------------------------------Finished Epoch 3/30: Loss=107.534, RMSE=1.53641, MAE=1.99888, r_2=-5.35805, p=0.420902\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0233602, MSE=0.0233602, MAE=0.122387\n",
      "[4/30] training 39.4%: Loss=0.0252332, MSE=0.0252332, MAE=0.12838\n",
      "[4/30] training 57.6%: Loss=0.029335, MSE=0.029335, MAE=0.13623\n",
      "[4/30] training 75.8%: Loss=0.0298566, MSE=0.0298566, MAE=0.137503\n",
      "[4/30] training 97.0%: Loss=0.0302597, MSE=0.0302597, MAE=0.138316\n",
      "-----------------------------------Finished Epoch 4/30: Loss=107.732, RMSE=1.46933, MAE=1.92208, r_2=-2.85865, p=0.453044\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0320776, MSE=0.0320776, MAE=0.139144\n",
      "[5/30] training 39.4%: Loss=0.029473, MSE=0.029473, MAE=0.133733\n",
      "[5/30] training 57.6%: Loss=0.0310223, MSE=0.0310223, MAE=0.141483\n",
      "[5/30] training 75.8%: Loss=0.0320689, MSE=0.0320689, MAE=0.142562\n",
      "[5/30] training 97.0%: Loss=0.0311283, MSE=0.0311283, MAE=0.140334\n",
      "-----------------------------------Finished Epoch 5/30: Loss=108.311, RMSE=1.41221, MAE=1.83055, r_2=-2.53164, p=0.466347\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0301539, MSE=0.0301539, MAE=0.138459\n",
      "[6/30] training 39.4%: Loss=0.0297493, MSE=0.0297493, MAE=0.138864\n",
      "[6/30] training 57.6%: Loss=0.0283614, MSE=0.0283614, MAE=0.133685\n",
      "[6/30] training 75.8%: Loss=0.0291641, MSE=0.0291641, MAE=0.136833\n",
      "[6/30] training 97.0%: Loss=0.0288666, MSE=0.0288666, MAE=0.135435\n",
      "-----------------------------------Finished Epoch 6/30: Loss=108.229, RMSE=1.40206, MAE=1.83217, r_2=-2.33745, p=0.471379\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0258273, MSE=0.0258273, MAE=0.135617\n",
      "[7/30] training 39.4%: Loss=0.0274578, MSE=0.0274578, MAE=0.134576\n",
      "[7/30] training 57.6%: Loss=0.0296926, MSE=0.0296926, MAE=0.137024\n",
      "[7/30] training 75.8%: Loss=0.0296655, MSE=0.0296655, MAE=0.137556\n",
      "[7/30] training 97.0%: Loss=0.0296829, MSE=0.0296829, MAE=0.137707\n",
      "-----------------------------------Finished Epoch 7/30: Loss=108.858, RMSE=1.37679, MAE=1.80736, r_2=-2.34623, p=0.471884\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.0277021, MSE=0.0277021, MAE=0.130705\n",
      "[8/30] training 39.4%: Loss=0.0296216, MSE=0.0296216, MAE=0.137772\n",
      "[8/30] training 57.6%: Loss=0.029679, MSE=0.029679, MAE=0.137802\n",
      "[8/30] training 75.8%: Loss=0.0298123, MSE=0.0298123, MAE=0.1377\n",
      "[8/30] training 97.0%: Loss=0.0278115, MSE=0.0278115, MAE=0.133584\n",
      "-----------------------------------Finished Epoch 8/30: Loss=108.715, RMSE=1.38497, MAE=1.82477, r_2=-1.75024, p=0.460903\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0262117, MSE=0.0262117, MAE=0.130422\n",
      "[9/30] training 39.4%: Loss=0.0260828, MSE=0.0260828, MAE=0.130437\n",
      "[9/30] training 57.6%: Loss=0.0283104, MSE=0.0283104, MAE=0.136804\n",
      "[9/30] training 75.8%: Loss=0.0272391, MSE=0.0272391, MAE=0.131602\n",
      "[9/30] training 97.0%: Loss=0.0275849, MSE=0.0275849, MAE=0.13277\n",
      "-----------------------------------Finished Epoch 9/30: Loss=108.409, RMSE=1.35569, MAE=1.79349, r_2=-2.2261, p=0.492455\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0271787, MSE=0.0271787, MAE=0.133971\n",
      "[10/30] training 39.4%: Loss=0.0301617, MSE=0.0301617, MAE=0.138955\n",
      "[10/30] training 57.6%: Loss=0.0267787, MSE=0.0267787, MAE=0.129717\n",
      "[10/30] training 75.8%: Loss=0.0271367, MSE=0.0271367, MAE=0.130653\n",
      "[10/30] training 97.0%: Loss=0.0272348, MSE=0.0272348, MAE=0.130774\n",
      "-----------------------------------Finished Epoch 10/30: Loss=107.627, RMSE=1.43232, MAE=1.91323, r_2=-1.91926, p=0.479644\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.029623, MSE=0.029623, MAE=0.133019\n",
      "[11/30] training 39.4%: Loss=0.0283991, MSE=0.0283991, MAE=0.133586\n",
      "[11/30] training 57.6%: Loss=0.0280744, MSE=0.0280744, MAE=0.132859\n",
      "[11/30] training 75.8%: Loss=0.0259571, MSE=0.0259571, MAE=0.127598\n",
      "[11/30] training 97.0%: Loss=0.0263285, MSE=0.0263285, MAE=0.129262\n",
      "-----------------------------------Finished Epoch 11/30: Loss=107.852, RMSE=1.39071, MAE=1.83837, r_2=-2.11673, p=0.506818\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0260737, MSE=0.0260737, MAE=0.132296\n",
      "[12/30] training 39.4%: Loss=0.027379, MSE=0.027379, MAE=0.134774\n",
      "[12/30] training 57.6%: Loss=0.0265809, MSE=0.0265809, MAE=0.130792\n",
      "[12/30] training 75.8%: Loss=0.0263998, MSE=0.0263998, MAE=0.130888\n",
      "[12/30] training 97.0%: Loss=0.0263119, MSE=0.0263119, MAE=0.130663\n",
      "-----------------------------------Finished Epoch 12/30: Loss=108.094, RMSE=1.36102, MAE=1.83405, r_2=-1.55109, p=0.485632\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0246922, MSE=0.0246922, MAE=0.129963\n",
      "[13/30] training 39.4%: Loss=0.0260515, MSE=0.0260515, MAE=0.129013\n",
      "[13/30] training 57.6%: Loss=0.0242456, MSE=0.0242456, MAE=0.124647\n",
      "[13/30] training 75.8%: Loss=0.0246627, MSE=0.0246627, MAE=0.125866\n",
      "[13/30] training 97.0%: Loss=0.0262376, MSE=0.0262376, MAE=0.130384\n",
      "-----------------------------------Finished Epoch 13/30: Loss=108.059, RMSE=1.36736, MAE=1.8139, r_2=-1.81341, p=0.502358\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0292292, MSE=0.0292292, MAE=0.133632\n",
      "[14/30] training 39.4%: Loss=0.0256935, MSE=0.0256935, MAE=0.1257\n",
      "[14/30] training 57.6%: Loss=0.026646, MSE=0.0266461, MAE=0.128998\n",
      "[14/30] training 75.8%: Loss=0.0259774, MSE=0.0259774, MAE=0.127932\n",
      "[14/30] training 97.0%: Loss=0.025442, MSE=0.025442, MAE=0.12695\n",
      "-----------------------------------Finished Epoch 14/30: Loss=108.828, RMSE=1.35485, MAE=1.79205, r_2=-1.67122, p=0.487679\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0284631, MSE=0.0284631, MAE=0.132172\n",
      "[15/30] training 39.4%: Loss=0.0303958, MSE=0.0303958, MAE=0.13846\n",
      "[15/30] training 57.6%: Loss=0.0271362, MSE=0.0271362, MAE=0.131357\n",
      "[15/30] training 75.8%: Loss=0.0268542, MSE=0.0268542, MAE=0.130907\n",
      "[15/30] training 97.0%: Loss=0.0260815, MSE=0.0260815, MAE=0.12895\n",
      "-----------------------------------Finished Epoch 15/30: Loss=108.146, RMSE=1.36985, MAE=1.81594, r_2=-1.47747, p=0.495095\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0220085, MSE=0.0220085, MAE=0.11898\n",
      "[16/30] training 39.4%: Loss=0.022808, MSE=0.022808, MAE=0.121624\n",
      "[16/30] training 57.6%: Loss=0.0230139, MSE=0.0230139, MAE=0.123195\n",
      "[16/30] training 75.8%: Loss=0.0256112, MSE=0.0256112, MAE=0.129397\n",
      "[16/30] training 97.0%: Loss=0.0263645, MSE=0.0263645, MAE=0.131031\n",
      "-----------------------------------Finished Epoch 16/30: Loss=108.251, RMSE=1.37189, MAE=1.82357, r_2=-1.52281, p=0.481474\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0226994, MSE=0.0226994, MAE=0.116401\n",
      "[17/30] training 39.4%: Loss=0.0243964, MSE=0.0243964, MAE=0.123991\n",
      "[17/30] training 57.6%: Loss=0.0243355, MSE=0.0243355, MAE=0.124244\n",
      "[17/30] training 75.8%: Loss=0.025882, MSE=0.025882, MAE=0.12795\n",
      "[17/30] training 97.0%: Loss=0.0252532, MSE=0.0252532, MAE=0.125905\n",
      "-----------------------------------Finished Epoch 17/30: Loss=107.451, RMSE=1.49762, MAE=1.93384, r_2=-2.08722, p=0.492228\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0273234, MSE=0.0273234, MAE=0.129858\n",
      "[18/30] training 39.4%: Loss=0.0281713, MSE=0.0281713, MAE=0.133205\n",
      "[18/30] training 57.6%: Loss=0.0255249, MSE=0.0255249, MAE=0.12512\n",
      "[18/30] training 75.8%: Loss=0.0243312, MSE=0.0243312, MAE=0.121584\n",
      "[18/30] training 97.0%: Loss=0.0254412, MSE=0.0254412, MAE=0.124862\n",
      "-----------------------------------Finished Epoch 18/30: Loss=107.563, RMSE=1.47693, MAE=1.90997, r_2=-1.94339, p=0.492201\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0278633, MSE=0.0278633, MAE=0.12914\n",
      "[19/30] training 39.4%: Loss=0.02553, MSE=0.02553, MAE=0.124568\n",
      "[19/30] training 57.6%: Loss=0.0247405, MSE=0.0247405, MAE=0.122669\n",
      "[19/30] training 75.8%: Loss=0.0237096, MSE=0.0237096, MAE=0.119833\n",
      "[19/30] training 97.0%: Loss=0.0245945, MSE=0.0245945, MAE=0.122476\n",
      "-----------------------------------Finished Epoch 19/30: Loss=109.264, RMSE=1.37115, MAE=1.81752, r_2=-1.52855, p=0.490905\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0237704, MSE=0.0237704, MAE=0.124443\n",
      "[20/30] training 39.4%: Loss=0.0222972, MSE=0.0222972, MAE=0.119385\n",
      "[20/30] training 57.6%: Loss=0.0229537, MSE=0.0229537, MAE=0.121008\n",
      "[20/30] training 75.8%: Loss=0.0234914, MSE=0.0234914, MAE=0.123105\n",
      "[20/30] training 97.0%: Loss=0.024285, MSE=0.024285, MAE=0.124336\n",
      "-----------------------------------Finished Epoch 20/30: Loss=108.232, RMSE=1.36989, MAE=1.82325, r_2=-1.44983, p=0.483961\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.026318, MSE=0.026318, MAE=0.127629\n",
      "[21/30] training 39.4%: Loss=0.0247687, MSE=0.0247687, MAE=0.122679\n",
      "[21/30] training 57.6%: Loss=0.0243326, MSE=0.0243326, MAE=0.12088\n",
      "[21/30] training 75.8%: Loss=0.0240152, MSE=0.0240152, MAE=0.120563\n",
      "[21/30] training 97.0%: Loss=0.0237913, MSE=0.0237913, MAE=0.120731\n",
      "-----------------------------------Finished Epoch 21/30: Loss=108.41, RMSE=1.37042, MAE=1.83788, r_2=-1.29084, p=0.467451\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0203456, MSE=0.0203456, MAE=0.111207\n",
      "[22/30] training 39.4%: Loss=0.019586, MSE=0.019586, MAE=0.109076\n",
      "[22/30] training 57.6%: Loss=0.0218401, MSE=0.0218401, MAE=0.115267\n",
      "[22/30] training 75.8%: Loss=0.0214712, MSE=0.0214712, MAE=0.114495\n",
      "[22/30] training 97.0%: Loss=0.0234073, MSE=0.0234073, MAE=0.120511\n",
      "-----------------------------------Finished Epoch 22/30: Loss=107.789, RMSE=1.42027, MAE=1.88012, r_2=-1.53688, p=0.485188\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0293929, MSE=0.0293929, MAE=0.134383\n",
      "[23/30] training 39.4%: Loss=0.0269237, MSE=0.0269237, MAE=0.129028\n",
      "[23/30] training 57.6%: Loss=0.0257134, MSE=0.0257134, MAE=0.126419\n",
      "[23/30] training 75.8%: Loss=0.0245237, MSE=0.0245237, MAE=0.12372\n",
      "[23/30] training 97.0%: Loss=0.0243847, MSE=0.0243847, MAE=0.123928\n",
      "-----------------------------------Finished Epoch 23/30: Loss=108.614, RMSE=1.32784, MAE=1.78739, r_2=-1.39458, p=0.49439\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.025827, MSE=0.025827, MAE=0.130615\n",
      "[24/30] training 39.4%: Loss=0.0239166, MSE=0.0239166, MAE=0.124325\n",
      "[24/30] training 57.6%: Loss=0.023868, MSE=0.023868, MAE=0.123043\n",
      "[24/30] training 75.8%: Loss=0.0228981, MSE=0.0228981, MAE=0.119874\n",
      "[24/30] training 97.0%: Loss=0.0229983, MSE=0.0229983, MAE=0.119893\n",
      "-----------------------------------Finished Epoch 24/30: Loss=107.809, RMSE=1.41809, MAE=1.87206, r_2=-1.46363, p=0.489314\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0235069, MSE=0.0235069, MAE=0.118317\n",
      "[25/30] training 39.4%: Loss=0.0228528, MSE=0.0228528, MAE=0.119768\n",
      "[25/30] training 57.6%: Loss=0.022771, MSE=0.022771, MAE=0.119837\n",
      "[25/30] training 75.8%: Loss=0.0229122, MSE=0.0229122, MAE=0.119148\n",
      "[25/30] training 97.0%: Loss=0.0233624, MSE=0.0233624, MAE=0.120725\n",
      "-----------------------------------Finished Epoch 25/30: Loss=107.497, RMSE=1.49943, MAE=1.94314, r_2=-1.79944, p=0.478783\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.023302, MSE=0.023302, MAE=0.111667\n",
      "[26/30] training 39.4%: Loss=0.022453, MSE=0.022453, MAE=0.113724\n",
      "[26/30] training 57.6%: Loss=0.0217135, MSE=0.0217135, MAE=0.111766\n",
      "[26/30] training 75.8%: Loss=0.023323, MSE=0.023323, MAE=0.118312\n",
      "[26/30] training 97.0%: Loss=0.022539, MSE=0.022539, MAE=0.116589\n",
      "-----------------------------------Finished Epoch 26/30: Loss=108.927, RMSE=1.34767, MAE=1.80336, r_2=-1.28592, p=0.487523\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0259205, MSE=0.0259205, MAE=0.128804\n",
      "[27/30] training 39.4%: Loss=0.0241393, MSE=0.0241393, MAE=0.123581\n",
      "[27/30] training 57.6%: Loss=0.0245061, MSE=0.0245061, MAE=0.124307\n",
      "[27/30] training 75.8%: Loss=0.0222101, MSE=0.0222101, MAE=0.11701\n",
      "[27/30] training 97.0%: Loss=0.0227192, MSE=0.0227192, MAE=0.117882\n",
      "-----------------------------------Finished Epoch 27/30: Loss=107.498, RMSE=1.50816, MAE=1.94707, r_2=-1.76082, p=0.476202\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0198735, MSE=0.0198735, MAE=0.117357\n",
      "[28/30] training 39.4%: Loss=0.0209359, MSE=0.0209359, MAE=0.115747\n",
      "[28/30] training 57.6%: Loss=0.0222289, MSE=0.0222289, MAE=0.117528\n",
      "[28/30] training 75.8%: Loss=0.0233789, MSE=0.0233789, MAE=0.120624\n",
      "[28/30] training 97.0%: Loss=0.0225049, MSE=0.0225049, MAE=0.11873\n",
      "-----------------------------------Finished Epoch 28/30: Loss=107.981, RMSE=1.40376, MAE=1.84748, r_2=-1.41071, p=0.488343\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0251834, MSE=0.0251834, MAE=0.129352\n",
      "[29/30] training 39.4%: Loss=0.0243685, MSE=0.0243685, MAE=0.1255\n",
      "[29/30] training 57.6%: Loss=0.0234065, MSE=0.0234065, MAE=0.121692\n",
      "[29/30] training 75.8%: Loss=0.0221428, MSE=0.0221428, MAE=0.117209\n",
      "[29/30] training 97.0%: Loss=0.0221653, MSE=0.0221653, MAE=0.117109\n",
      "-----------------------------------Finished Epoch 29/30: Loss=107.738, RMSE=1.43914, MAE=1.86068, r_2=-1.40954, p=0.506608\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0206452, MSE=0.0206452, MAE=0.10974\n",
      "[30/30] training 39.4%: Loss=0.0207073, MSE=0.0207073, MAE=0.111338\n",
      "[30/30] training 57.6%: Loss=0.0219728, MSE=0.0219728, MAE=0.115648\n",
      "[30/30] training 75.8%: Loss=0.0207879, MSE=0.0207879, MAE=0.112747\n",
      "[30/30] training 97.0%: Loss=0.0210978, MSE=0.0210978, MAE=0.113775\n",
      "-----------------------------------Finished Epoch 30/30: Loss=108.807, RMSE=1.3328, MAE=1.77824, r_2=-1.17728, p=0.504376\n",
      "Saving final model to saved_models/model_fold3_final.pth\n",
      "******************************** FOLD 4 ******************************\n",
      "******************************** FOLD 4 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0409865, MSE=0.0409865, MAE=0.164903\n",
      "[1/30] training 39.4%: Loss=0.0436062, MSE=0.0436062, MAE=0.165307\n",
      "[1/30] training 57.6%: Loss=0.0415862, MSE=0.0415862, MAE=0.164317\n",
      "[1/30] training 75.8%: Loss=0.0417021, MSE=0.0417021, MAE=0.164548\n",
      "[1/30] training 97.0%: Loss=0.0406153, MSE=0.0406153, MAE=0.163097\n",
      "-----------------------------------Finished Epoch 1/30: Loss=114.313, RMSE=1.63947, MAE=2.02123, r_2=-50.7795, p=0.274465\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0323936, MSE=0.0323936, MAE=0.143288\n",
      "[2/30] training 39.4%: Loss=0.0368046, MSE=0.0368046, MAE=0.152132\n",
      "[2/30] training 57.6%: Loss=0.0377189, MSE=0.0377189, MAE=0.157079\n",
      "[2/30] training 75.8%: Loss=0.037296, MSE=0.037296, MAE=0.15574\n",
      "[2/30] training 97.0%: Loss=0.036098, MSE=0.036098, MAE=0.153232\n",
      "-----------------------------------Finished Epoch 2/30: Loss=114.001, RMSE=1.61514, MAE=1.99014, r_2=-11.2522, p=0.311923\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0330185, MSE=0.0330185, MAE=0.146011\n",
      "[3/30] training 39.4%: Loss=0.0367014, MSE=0.0367013, MAE=0.154991\n",
      "[3/30] training 57.6%: Loss=0.0356685, MSE=0.0356685, MAE=0.151627\n",
      "[3/30] training 75.8%: Loss=0.0358247, MSE=0.0358247, MAE=0.151284\n",
      "[3/30] training 97.0%: Loss=0.0356086, MSE=0.0356086, MAE=0.150918\n",
      "-----------------------------------Finished Epoch 3/30: Loss=113.988, RMSE=1.59193, MAE=1.95889, r_2=-5.26561, p=0.351921\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0326526, MSE=0.0326526, MAE=0.14837\n",
      "[4/30] training 39.4%: Loss=0.0293759, MSE=0.0293759, MAE=0.139349\n",
      "[4/30] training 57.6%: Loss=0.0301202, MSE=0.0301202, MAE=0.140438\n",
      "[4/30] training 75.8%: Loss=0.0334594, MSE=0.0334594, MAE=0.147833\n",
      "[4/30] training 97.0%: Loss=0.0336955, MSE=0.0336955, MAE=0.148256\n",
      "-----------------------------------Finished Epoch 4/30: Loss=114.264, RMSE=1.56634, MAE=1.92615, r_2=-4.33426, p=0.377893\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0350098, MSE=0.0350098, MAE=0.15028\n",
      "[5/30] training 39.4%: Loss=0.0301144, MSE=0.0301144, MAE=0.141441\n",
      "[5/30] training 57.6%: Loss=0.0300315, MSE=0.0300315, MAE=0.140117\n",
      "[5/30] training 75.8%: Loss=0.0311122, MSE=0.0311122, MAE=0.142815\n",
      "[5/30] training 97.0%: Loss=0.0329734, MSE=0.0329734, MAE=0.146814\n",
      "-----------------------------------Finished Epoch 5/30: Loss=114.026, RMSE=1.57046, MAE=1.92385, r_2=-4.20554, p=0.389826\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0288884, MSE=0.0288884, MAE=0.132705\n",
      "[6/30] training 39.4%: Loss=0.0269407, MSE=0.0269407, MAE=0.12959\n",
      "[6/30] training 57.6%: Loss=0.0279188, MSE=0.0279188, MAE=0.131867\n",
      "[6/30] training 75.8%: Loss=0.0292216, MSE=0.0292216, MAE=0.135661\n",
      "[6/30] training 97.0%: Loss=0.0322905, MSE=0.0322905, MAE=0.143562\n",
      "-----------------------------------Finished Epoch 6/30: Loss=114.225, RMSE=1.54339, MAE=1.89892, r_2=-3.83353, p=0.408444\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0308085, MSE=0.0308085, MAE=0.138491\n",
      "[7/30] training 39.4%: Loss=0.0324096, MSE=0.0324096, MAE=0.143103\n",
      "[7/30] training 57.6%: Loss=0.032858, MSE=0.032858, MAE=0.144547\n",
      "[7/30] training 75.8%: Loss=0.0315296, MSE=0.0315296, MAE=0.141581\n",
      "[7/30] training 97.0%: Loss=0.0318985, MSE=0.0318985, MAE=0.141953\n",
      "-----------------------------------Finished Epoch 7/30: Loss=114.23, RMSE=1.53928, MAE=1.88527, r_2=-3.68026, p=0.422576\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.0325995, MSE=0.0325995, MAE=0.142528\n",
      "[8/30] training 39.4%: Loss=0.033499, MSE=0.033499, MAE=0.141756\n",
      "[8/30] training 57.6%: Loss=0.0317441, MSE=0.0317441, MAE=0.140126\n",
      "[8/30] training 75.8%: Loss=0.0312825, MSE=0.0312825, MAE=0.140424\n",
      "[8/30] training 97.0%: Loss=0.0296367, MSE=0.0296367, MAE=0.136416\n",
      "-----------------------------------Finished Epoch 8/30: Loss=114.021, RMSE=1.52819, MAE=1.87019, r_2=-3.06867, p=0.444646\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0316646, MSE=0.0316646, MAE=0.141343\n",
      "[9/30] training 39.4%: Loss=0.0310359, MSE=0.0310359, MAE=0.142197\n",
      "[9/30] training 57.6%: Loss=0.0298368, MSE=0.0298368, MAE=0.138649\n",
      "[9/30] training 75.8%: Loss=0.0286996, MSE=0.0286996, MAE=0.135457\n",
      "[9/30] training 97.0%: Loss=0.028332, MSE=0.028332, MAE=0.133756\n",
      "-----------------------------------Finished Epoch 9/30: Loss=114.143, RMSE=1.51855, MAE=1.86614, r_2=-3.01569, p=0.443584\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0303955, MSE=0.0303955, MAE=0.139072\n",
      "[10/30] training 39.4%: Loss=0.0286668, MSE=0.0286668, MAE=0.133559\n",
      "[10/30] training 57.6%: Loss=0.0293229, MSE=0.0293229, MAE=0.135865\n",
      "[10/30] training 75.8%: Loss=0.0283297, MSE=0.0283297, MAE=0.133523\n",
      "[10/30] training 97.0%: Loss=0.0282534, MSE=0.0282534, MAE=0.134422\n",
      "-----------------------------------Finished Epoch 10/30: Loss=113.994, RMSE=1.51502, MAE=1.84474, r_2=-3.08039, p=0.470721\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0261919, MSE=0.0261919, MAE=0.129757\n",
      "[11/30] training 39.4%: Loss=0.0280475, MSE=0.0280475, MAE=0.133652\n",
      "[11/30] training 57.6%: Loss=0.028183, MSE=0.028183, MAE=0.133846\n",
      "[11/30] training 75.8%: Loss=0.0279871, MSE=0.0279871, MAE=0.131721\n",
      "[11/30] training 97.0%: Loss=0.0273436, MSE=0.0273436, MAE=0.131087\n",
      "-----------------------------------Finished Epoch 11/30: Loss=113.766, RMSE=1.53013, MAE=1.86616, r_2=-2.9356, p=0.465957\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0270027, MSE=0.0270027, MAE=0.128077\n",
      "[12/30] training 39.4%: Loss=0.0268649, MSE=0.0268649, MAE=0.129872\n",
      "[12/30] training 57.6%: Loss=0.0261917, MSE=0.0261917, MAE=0.12772\n",
      "[12/30] training 75.8%: Loss=0.0262275, MSE=0.0262275, MAE=0.127842\n",
      "[12/30] training 97.0%: Loss=0.028403, MSE=0.028403, MAE=0.132982\n",
      "-----------------------------------Finished Epoch 12/30: Loss=113.997, RMSE=1.50658, MAE=1.84222, r_2=-2.79567, p=0.471835\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0243604, MSE=0.0243604, MAE=0.125981\n",
      "[13/30] training 39.4%: Loss=0.0255586, MSE=0.0255586, MAE=0.13038\n",
      "[13/30] training 57.6%: Loss=0.0264379, MSE=0.0264379, MAE=0.131225\n",
      "[13/30] training 75.8%: Loss=0.0263743, MSE=0.0263743, MAE=0.129241\n",
      "[13/30] training 97.0%: Loss=0.0272311, MSE=0.0272311, MAE=0.131069\n",
      "-----------------------------------Finished Epoch 13/30: Loss=113.792, RMSE=1.51642, MAE=1.85435, r_2=-3.1206, p=0.475884\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0260231, MSE=0.0260231, MAE=0.126425\n",
      "[14/30] training 39.4%: Loss=0.0244443, MSE=0.0244443, MAE=0.122583\n",
      "[14/30] training 57.6%: Loss=0.0252081, MSE=0.0252081, MAE=0.125707\n",
      "[14/30] training 75.8%: Loss=0.0264224, MSE=0.0264224, MAE=0.127811\n",
      "[14/30] training 97.0%: Loss=0.0264153, MSE=0.0264153, MAE=0.128236\n",
      "-----------------------------------Finished Epoch 14/30: Loss=113.989, RMSE=1.51985, MAE=1.84278, r_2=-3.09917, p=0.472974\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0269751, MSE=0.0269751, MAE=0.125474\n",
      "[15/30] training 39.4%: Loss=0.0284011, MSE=0.0284011, MAE=0.131592\n",
      "[15/30] training 57.6%: Loss=0.025643, MSE=0.025643, MAE=0.126746\n",
      "[15/30] training 75.8%: Loss=0.0256205, MSE=0.0256205, MAE=0.127202\n",
      "[15/30] training 97.0%: Loss=0.0250875, MSE=0.0250875, MAE=0.125311\n",
      "-----------------------------------Finished Epoch 15/30: Loss=113.601, RMSE=1.52832, MAE=1.86771, r_2=-3.0817, p=0.482002\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0228632, MSE=0.0228632, MAE=0.119981\n",
      "[16/30] training 39.4%: Loss=0.022569, MSE=0.022569, MAE=0.117332\n",
      "[16/30] training 57.6%: Loss=0.0227241, MSE=0.0227241, MAE=0.116628\n",
      "[16/30] training 75.8%: Loss=0.0253588, MSE=0.0253588, MAE=0.124115\n",
      "[16/30] training 97.0%: Loss=0.024724, MSE=0.024724, MAE=0.123423\n",
      "-----------------------------------Finished Epoch 16/30: Loss=113.894, RMSE=1.50408, MAE=1.82279, r_2=-2.7381, p=0.495805\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0205571, MSE=0.0205571, MAE=0.10882\n",
      "[17/30] training 39.4%: Loss=0.025676, MSE=0.025676, MAE=0.124188\n",
      "[17/30] training 57.6%: Loss=0.026291, MSE=0.026291, MAE=0.128064\n",
      "[17/30] training 75.8%: Loss=0.0259094, MSE=0.0259094, MAE=0.125962\n",
      "[17/30] training 97.0%: Loss=0.0253009, MSE=0.0253009, MAE=0.124826\n",
      "-----------------------------------Finished Epoch 17/30: Loss=113.802, RMSE=1.50941, MAE=1.84186, r_2=-2.38998, p=0.483563\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0243338, MSE=0.0243338, MAE=0.123776\n",
      "[18/30] training 39.4%: Loss=0.0273315, MSE=0.0273315, MAE=0.132163\n",
      "[18/30] training 57.6%: Loss=0.025953, MSE=0.025953, MAE=0.128898\n",
      "[18/30] training 75.8%: Loss=0.0246365, MSE=0.0246365, MAE=0.125996\n",
      "[18/30] training 97.0%: Loss=0.0246724, MSE=0.0246724, MAE=0.125214\n",
      "-----------------------------------Finished Epoch 18/30: Loss=113.394, RMSE=1.54036, MAE=1.89534, r_2=-2.57722, p=0.480145\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0278363, MSE=0.0278363, MAE=0.130053\n",
      "[19/30] training 39.4%: Loss=0.0285547, MSE=0.0285547, MAE=0.1324\n",
      "[19/30] training 57.6%: Loss=0.0265624, MSE=0.0265624, MAE=0.127546\n",
      "[19/30] training 75.8%: Loss=0.0267319, MSE=0.0267319, MAE=0.129718\n",
      "[19/30] training 97.0%: Loss=0.0254081, MSE=0.0254081, MAE=0.126277\n",
      "-----------------------------------Finished Epoch 19/30: Loss=113.832, RMSE=1.51767, MAE=1.84065, r_2=-2.39897, p=0.482397\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0234259, MSE=0.0234259, MAE=0.114437\n",
      "[20/30] training 39.4%: Loss=0.0222686, MSE=0.0222686, MAE=0.11306\n",
      "[20/30] training 57.6%: Loss=0.0226531, MSE=0.0226531, MAE=0.116374\n",
      "[20/30] training 75.8%: Loss=0.0232388, MSE=0.0232388, MAE=0.118347\n",
      "[20/30] training 97.0%: Loss=0.0236458, MSE=0.0236458, MAE=0.120886\n",
      "-----------------------------------Finished Epoch 20/30: Loss=113.983, RMSE=1.52122, MAE=1.84017, r_2=-2.47803, p=0.473757\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0251295, MSE=0.0251295, MAE=0.125847\n",
      "[21/30] training 39.4%: Loss=0.0236799, MSE=0.0236799, MAE=0.121276\n",
      "[21/30] training 57.6%: Loss=0.0238181, MSE=0.0238181, MAE=0.119416\n",
      "[21/30] training 75.8%: Loss=0.0232648, MSE=0.0232649, MAE=0.118658\n",
      "[21/30] training 97.0%: Loss=0.0238341, MSE=0.0238341, MAE=0.121296\n",
      "-----------------------------------Finished Epoch 21/30: Loss=113.544, RMSE=1.54459, MAE=1.88979, r_2=-2.5177, p=0.466366\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0216556, MSE=0.0216556, MAE=0.115908\n",
      "[22/30] training 39.4%: Loss=0.0222931, MSE=0.0222931, MAE=0.116587\n",
      "[22/30] training 57.6%: Loss=0.0217574, MSE=0.0217574, MAE=0.115832\n",
      "[22/30] training 75.8%: Loss=0.022142, MSE=0.022142, MAE=0.11672\n",
      "[22/30] training 97.0%: Loss=0.0227676, MSE=0.0227676, MAE=0.118025\n",
      "-----------------------------------Finished Epoch 22/30: Loss=113.07, RMSE=1.59003, MAE=1.94785, r_2=-2.71878, p=0.484937\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0219542, MSE=0.0219542, MAE=0.111909\n",
      "[23/30] training 39.4%: Loss=0.0217148, MSE=0.0217148, MAE=0.113277\n",
      "[23/30] training 57.6%: Loss=0.0203506, MSE=0.0203506, MAE=0.110829\n",
      "[23/30] training 75.8%: Loss=0.0216161, MSE=0.0216161, MAE=0.115043\n",
      "[23/30] training 97.0%: Loss=0.023274, MSE=0.023274, MAE=0.118949\n",
      "-----------------------------------Finished Epoch 23/30: Loss=113.558, RMSE=1.52884, MAE=1.87689, r_2=-2.32386, p=0.475877\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.0220214, MSE=0.0220214, MAE=0.11451\n",
      "[24/30] training 39.4%: Loss=0.0231806, MSE=0.0231806, MAE=0.118902\n",
      "[24/30] training 57.6%: Loss=0.0226723, MSE=0.0226723, MAE=0.118389\n",
      "[24/30] training 75.8%: Loss=0.0222542, MSE=0.0222542, MAE=0.116494\n",
      "[24/30] training 97.0%: Loss=0.02256, MSE=0.02256, MAE=0.117561\n",
      "-----------------------------------Finished Epoch 24/30: Loss=113.591, RMSE=1.51853, MAE=1.86569, r_2=-2.39718, p=0.482019\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0229738, MSE=0.0229738, MAE=0.11784\n",
      "[25/30] training 39.4%: Loss=0.0205828, MSE=0.0205828, MAE=0.111246\n",
      "[25/30] training 57.6%: Loss=0.0201231, MSE=0.0201231, MAE=0.109694\n",
      "[25/30] training 75.8%: Loss=0.0218485, MSE=0.0218485, MAE=0.115471\n",
      "[25/30] training 97.0%: Loss=0.0227267, MSE=0.0227267, MAE=0.117532\n",
      "-----------------------------------Finished Epoch 25/30: Loss=113.437, RMSE=1.52511, MAE=1.88358, r_2=-2.4306, p=0.484477\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0190336, MSE=0.0190336, MAE=0.107238\n",
      "[26/30] training 39.4%: Loss=0.022793, MSE=0.022793, MAE=0.121266\n",
      "[26/30] training 57.6%: Loss=0.022264, MSE=0.022264, MAE=0.119019\n",
      "[26/30] training 75.8%: Loss=0.0229263, MSE=0.0229263, MAE=0.119486\n",
      "[26/30] training 97.0%: Loss=0.0225433, MSE=0.0225433, MAE=0.117882\n",
      "-----------------------------------Finished Epoch 26/30: Loss=113.204, RMSE=1.55237, MAE=1.91453, r_2=-2.43271, p=0.490761\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0249812, MSE=0.0249812, MAE=0.12458\n",
      "[27/30] training 39.4%: Loss=0.0221615, MSE=0.0221615, MAE=0.116382\n",
      "[27/30] training 57.6%: Loss=0.0216143, MSE=0.0216143, MAE=0.114539\n",
      "[27/30] training 75.8%: Loss=0.020606, MSE=0.020606, MAE=0.111977\n",
      "[27/30] training 97.0%: Loss=0.0229412, MSE=0.0229412, MAE=0.117985\n",
      "-----------------------------------Finished Epoch 27/30: Loss=113.299, RMSE=1.54334, MAE=1.90875, r_2=-2.33073, p=0.481415\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0218812, MSE=0.0218812, MAE=0.11606\n",
      "[28/30] training 39.4%: Loss=0.0207894, MSE=0.0207894, MAE=0.112855\n",
      "[28/30] training 57.6%: Loss=0.0209233, MSE=0.0209233, MAE=0.113342\n",
      "[28/30] training 75.8%: Loss=0.019688, MSE=0.019688, MAE=0.108943\n",
      "[28/30] training 97.0%: Loss=0.0203626, MSE=0.0203626, MAE=0.111305\n",
      "-----------------------------------Finished Epoch 28/30: Loss=113.236, RMSE=1.56578, MAE=1.91747, r_2=-2.36507, p=0.483311\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0224545, MSE=0.0224545, MAE=0.113611\n",
      "[29/30] training 39.4%: Loss=0.0233024, MSE=0.0233024, MAE=0.11735\n",
      "[29/30] training 57.6%: Loss=0.0206999, MSE=0.0206999, MAE=0.109709\n",
      "[29/30] training 75.8%: Loss=0.0211383, MSE=0.0211383, MAE=0.11348\n",
      "[29/30] training 97.0%: Loss=0.0211093, MSE=0.0211093, MAE=0.113969\n",
      "-----------------------------------Finished Epoch 29/30: Loss=112.879, RMSE=1.60429, MAE=1.97962, r_2=-2.51663, p=0.492456\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0216108, MSE=0.0216108, MAE=0.119223\n",
      "[30/30] training 39.4%: Loss=0.020856, MSE=0.020856, MAE=0.115457\n",
      "[30/30] training 57.6%: Loss=0.021225, MSE=0.021225, MAE=0.115077\n",
      "[30/30] training 75.8%: Loss=0.0217209, MSE=0.0217209, MAE=0.116236\n",
      "[30/30] training 97.0%: Loss=0.0215214, MSE=0.0215214, MAE=0.115074\n",
      "-----------------------------------Finished Epoch 30/30: Loss=113.408, RMSE=1.50937, MAE=1.86217, r_2=-2.08827, p=0.505132\n",
      "Saving final model to saved_models/model_fold4_final.pth\n",
      "******************************** FOLD 5 ******************************\n",
      "******************************** FOLD 5 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0419392, MSE=0.0419392, MAE=0.166102\n",
      "[1/30] training 39.4%: Loss=0.0439737, MSE=0.0439737, MAE=0.170033\n",
      "[1/30] training 57.6%: Loss=0.0469922, MSE=0.0469922, MAE=0.1747\n",
      "[1/30] training 75.8%: Loss=0.0453926, MSE=0.0453926, MAE=0.171095\n",
      "[1/30] training 97.0%: Loss=0.0450615, MSE=0.0450615, MAE=0.170789\n",
      "-----------------------------------Finished Epoch 1/30: Loss=122.94, RMSE=1.62554, MAE=2.01318, r_2=-60.8316, p=-0.204568\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0495509, MSE=0.0495509, MAE=0.18329\n",
      "[2/30] training 39.4%: Loss=0.0480233, MSE=0.0480233, MAE=0.178946\n",
      "[2/30] training 57.6%: Loss=0.0460238, MSE=0.0460238, MAE=0.172822\n",
      "[2/30] training 75.8%: Loss=0.0438124, MSE=0.0438124, MAE=0.167376\n",
      "[2/30] training 97.0%: Loss=0.0422992, MSE=0.0422992, MAE=0.164027\n",
      "-----------------------------------Finished Epoch 2/30: Loss=122.866, RMSE=1.59438, MAE=1.97659, r_2=-48.5439, p=-0.0644474\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0400436, MSE=0.0400436, MAE=0.159301\n",
      "[3/30] training 39.4%: Loss=0.0417756, MSE=0.0417756, MAE=0.162072\n",
      "[3/30] training 57.6%: Loss=0.0421227, MSE=0.0421227, MAE=0.161482\n",
      "[3/30] training 75.8%: Loss=0.0392247, MSE=0.0392247, MAE=0.155005\n",
      "[3/30] training 97.0%: Loss=0.0393037, MSE=0.0393037, MAE=0.15636\n",
      "-----------------------------------Finished Epoch 3/30: Loss=122.878, RMSE=1.5419, MAE=1.91077, r_2=-22.6884, p=0.189198\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0312118, MSE=0.0312118, MAE=0.144005\n",
      "[4/30] training 39.4%: Loss=0.0369305, MSE=0.0369305, MAE=0.152646\n",
      "[4/30] training 57.6%: Loss=0.0366748, MSE=0.0366748, MAE=0.152162\n",
      "[4/30] training 75.8%: Loss=0.0350254, MSE=0.0350254, MAE=0.14873\n",
      "[4/30] training 97.0%: Loss=0.0369282, MSE=0.0369282, MAE=0.152499\n",
      "-----------------------------------Finished Epoch 4/30: Loss=122.926, RMSE=1.55843, MAE=1.88325, r_2=-9.05428, p=0.275961\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0403529, MSE=0.0403529, MAE=0.157014\n",
      "[5/30] training 39.4%: Loss=0.0374975, MSE=0.0374975, MAE=0.151245\n",
      "[5/30] training 57.6%: Loss=0.0363849, MSE=0.0363849, MAE=0.149618\n",
      "[5/30] training 75.8%: Loss=0.0363472, MSE=0.0363472, MAE=0.149653\n",
      "[5/30] training 97.0%: Loss=0.0362545, MSE=0.0362545, MAE=0.14982\n",
      "-----------------------------------Finished Epoch 5/30: Loss=122.758, RMSE=1.52788, MAE=1.84126, r_2=-6.18691, p=0.317075\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.031544, MSE=0.031544, MAE=0.137988\n",
      "[6/30] training 39.4%: Loss=0.0311851, MSE=0.0311851, MAE=0.139545\n",
      "[6/30] training 57.6%: Loss=0.0320773, MSE=0.0320773, MAE=0.142191\n",
      "[6/30] training 75.8%: Loss=0.032221, MSE=0.032221, MAE=0.142943\n",
      "[6/30] training 97.0%: Loss=0.0340599, MSE=0.0340599, MAE=0.146403\n",
      "-----------------------------------Finished Epoch 6/30: Loss=122.812, RMSE=1.5259, MAE=1.83758, r_2=-6.11694, p=0.332172\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0262341, MSE=0.0262341, MAE=0.123638\n",
      "[7/30] training 39.4%: Loss=0.0280569, MSE=0.0280569, MAE=0.130854\n",
      "[7/30] training 57.6%: Loss=0.0325677, MSE=0.0325677, MAE=0.141645\n",
      "[7/30] training 75.8%: Loss=0.033949, MSE=0.033949, MAE=0.144401\n",
      "[7/30] training 97.0%: Loss=0.0331259, MSE=0.0331259, MAE=0.142928\n",
      "-----------------------------------Finished Epoch 7/30: Loss=122.458, RMSE=1.48063, MAE=1.79118, r_2=-4.41495, p=0.353485\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.0397854, MSE=0.0397854, MAE=0.156881\n",
      "[8/30] training 39.4%: Loss=0.036471, MSE=0.036471, MAE=0.149712\n",
      "[8/30] training 57.6%: Loss=0.0328068, MSE=0.0328068, MAE=0.143371\n",
      "[8/30] training 75.8%: Loss=0.0322477, MSE=0.0322477, MAE=0.141189\n",
      "[8/30] training 97.0%: Loss=0.0320004, MSE=0.0320004, MAE=0.14081\n",
      "-----------------------------------Finished Epoch 8/30: Loss=122.926, RMSE=1.50839, MAE=1.81804, r_2=-5.71983, p=0.383309\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0349926, MSE=0.0349926, MAE=0.145816\n",
      "[9/30] training 39.4%: Loss=0.037145, MSE=0.037145, MAE=0.150287\n",
      "[9/30] training 57.6%: Loss=0.034302, MSE=0.034302, MAE=0.145299\n",
      "[9/30] training 75.8%: Loss=0.0338658, MSE=0.0338658, MAE=0.144894\n",
      "[9/30] training 97.0%: Loss=0.0320899, MSE=0.0320899, MAE=0.140565\n",
      "-----------------------------------Finished Epoch 9/30: Loss=122.34, RMSE=1.41261, MAE=1.74078, r_2=-5.04119, p=0.407229\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0250581, MSE=0.0250581, MAE=0.117682\n",
      "[10/30] training 39.4%: Loss=0.0261848, MSE=0.0261848, MAE=0.126708\n",
      "[10/30] training 57.6%: Loss=0.0282971, MSE=0.0282971, MAE=0.131259\n",
      "[10/30] training 75.8%: Loss=0.0316625, MSE=0.0316625, MAE=0.140208\n",
      "[10/30] training 97.0%: Loss=0.0304359, MSE=0.0304359, MAE=0.138059\n",
      "-----------------------------------Finished Epoch 10/30: Loss=122.31, RMSE=1.40362, MAE=1.7305, r_2=-5.09452, p=0.419024\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0324682, MSE=0.0324682, MAE=0.140348\n",
      "[11/30] training 39.4%: Loss=0.0337839, MSE=0.0337839, MAE=0.144374\n",
      "[11/30] training 57.6%: Loss=0.0307247, MSE=0.0307247, MAE=0.137927\n",
      "[11/30] training 75.8%: Loss=0.030529, MSE=0.030529, MAE=0.137673\n",
      "[11/30] training 97.0%: Loss=0.0298268, MSE=0.0298268, MAE=0.135561\n",
      "-----------------------------------Finished Epoch 11/30: Loss=122.41, RMSE=1.42026, MAE=1.73541, r_2=-4.08152, p=0.418177\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0270048, MSE=0.0270048, MAE=0.126282\n",
      "[12/30] training 39.4%: Loss=0.028004, MSE=0.028004, MAE=0.128547\n",
      "[12/30] training 57.6%: Loss=0.0288302, MSE=0.0288302, MAE=0.131938\n",
      "[12/30] training 75.8%: Loss=0.028711, MSE=0.028711, MAE=0.132766\n",
      "[12/30] training 97.0%: Loss=0.0286995, MSE=0.0286995, MAE=0.133695\n",
      "-----------------------------------Finished Epoch 12/30: Loss=122.856, RMSE=1.44971, MAE=1.7749, r_2=-5.04703, p=0.429026\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0307228, MSE=0.0307228, MAE=0.140748\n",
      "[13/30] training 39.4%: Loss=0.0306772, MSE=0.0306772, MAE=0.138266\n",
      "[13/30] training 57.6%: Loss=0.0281424, MSE=0.0281424, MAE=0.132231\n",
      "[13/30] training 75.8%: Loss=0.0279165, MSE=0.0279165, MAE=0.130584\n",
      "[13/30] training 97.0%: Loss=0.0274819, MSE=0.0274819, MAE=0.1302\n",
      "-----------------------------------Finished Epoch 13/30: Loss=122.435, RMSE=1.38943, MAE=1.71857, r_2=-3.66506, p=0.440074\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0311536, MSE=0.0311536, MAE=0.144854\n",
      "[14/30] training 39.4%: Loss=0.0263562, MSE=0.0263562, MAE=0.130737\n",
      "[14/30] training 57.6%: Loss=0.0284816, MSE=0.0284816, MAE=0.133376\n",
      "[14/30] training 75.8%: Loss=0.028901, MSE=0.028901, MAE=0.135886\n",
      "[14/30] training 97.0%: Loss=0.0273514, MSE=0.0273514, MAE=0.131432\n",
      "-----------------------------------Finished Epoch 14/30: Loss=122.002, RMSE=1.33807, MAE=1.69367, r_2=-4.0679, p=0.451769\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0221341, MSE=0.0221341, MAE=0.116677\n",
      "[15/30] training 39.4%: Loss=0.023713, MSE=0.023713, MAE=0.12106\n",
      "[15/30] training 57.6%: Loss=0.0281117, MSE=0.0281117, MAE=0.130986\n",
      "[15/30] training 75.8%: Loss=0.0281082, MSE=0.0281082, MAE=0.130783\n",
      "[15/30] training 97.0%: Loss=0.0279943, MSE=0.0279943, MAE=0.130599\n",
      "-----------------------------------Finished Epoch 15/30: Loss=122.306, RMSE=1.36414, MAE=1.69416, r_2=-3.76547, p=0.459483\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0296194, MSE=0.0296194, MAE=0.136106\n",
      "[16/30] training 39.4%: Loss=0.026376, MSE=0.026376, MAE=0.127962\n",
      "[16/30] training 57.6%: Loss=0.0255809, MSE=0.0255809, MAE=0.12447\n",
      "[16/30] training 75.8%: Loss=0.0258855, MSE=0.0258855, MAE=0.124296\n",
      "[16/30] training 97.0%: Loss=0.0265522, MSE=0.0265522, MAE=0.127398\n",
      "-----------------------------------Finished Epoch 16/30: Loss=122.42, RMSE=1.35242, MAE=1.69914, r_2=-3.90038, p=0.462721\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0244346, MSE=0.0244346, MAE=0.121224\n",
      "[17/30] training 39.4%: Loss=0.0274097, MSE=0.0274097, MAE=0.129872\n",
      "[17/30] training 57.6%: Loss=0.0263443, MSE=0.0263443, MAE=0.128805\n",
      "[17/30] training 75.8%: Loss=0.0257082, MSE=0.0257082, MAE=0.127149\n",
      "[17/30] training 97.0%: Loss=0.0259563, MSE=0.0259563, MAE=0.127128\n",
      "-----------------------------------Finished Epoch 17/30: Loss=122.325, RMSE=1.33492, MAE=1.67443, r_2=-3.52033, p=0.482748\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0240062, MSE=0.0240062, MAE=0.121951\n",
      "[18/30] training 39.4%: Loss=0.026094, MSE=0.026094, MAE=0.123231\n",
      "[18/30] training 57.6%: Loss=0.0262385, MSE=0.0262385, MAE=0.1239\n",
      "[18/30] training 75.8%: Loss=0.0252961, MSE=0.0252961, MAE=0.123363\n",
      "[18/30] training 97.0%: Loss=0.0251399, MSE=0.0251399, MAE=0.124216\n",
      "-----------------------------------Finished Epoch 18/30: Loss=122.02, RMSE=1.3178, MAE=1.66784, r_2=-3.33117, p=0.477907\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0239154, MSE=0.0239154, MAE=0.121261\n",
      "[19/30] training 39.4%: Loss=0.0246633, MSE=0.0246633, MAE=0.12453\n",
      "[19/30] training 57.6%: Loss=0.0237588, MSE=0.0237588, MAE=0.121773\n",
      "[19/30] training 75.8%: Loss=0.024031, MSE=0.024031, MAE=0.120672\n",
      "[19/30] training 97.0%: Loss=0.0243321, MSE=0.0243321, MAE=0.122625\n",
      "-----------------------------------Finished Epoch 19/30: Loss=122.417, RMSE=1.34002, MAE=1.68648, r_2=-4.01325, p=0.478821\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0213269, MSE=0.0213269, MAE=0.111199\n",
      "[20/30] training 39.4%: Loss=0.0243712, MSE=0.0243712, MAE=0.120201\n",
      "[20/30] training 57.6%: Loss=0.0257293, MSE=0.0257293, MAE=0.12413\n",
      "[20/30] training 75.8%: Loss=0.0246226, MSE=0.0246226, MAE=0.122317\n",
      "[20/30] training 97.0%: Loss=0.0244477, MSE=0.0244477, MAE=0.123047\n",
      "-----------------------------------Finished Epoch 20/30: Loss=122.114, RMSE=1.33844, MAE=1.69202, r_2=-3.17839, p=0.451822\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0269668, MSE=0.0269668, MAE=0.133435\n",
      "[21/30] training 39.4%: Loss=0.0256722, MSE=0.0256722, MAE=0.129408\n",
      "[21/30] training 57.6%: Loss=0.0260513, MSE=0.0260513, MAE=0.12891\n",
      "[21/30] training 75.8%: Loss=0.0252496, MSE=0.0252496, MAE=0.125527\n",
      "[21/30] training 97.0%: Loss=0.024692, MSE=0.024692, MAE=0.123537\n",
      "-----------------------------------Finished Epoch 21/30: Loss=122.074, RMSE=1.31797, MAE=1.68117, r_2=-3.80388, p=0.465947\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0284257, MSE=0.0284257, MAE=0.12903\n",
      "[22/30] training 39.4%: Loss=0.0277051, MSE=0.0277051, MAE=0.130304\n",
      "[22/30] training 57.6%: Loss=0.026218, MSE=0.026218, MAE=0.126104\n",
      "[22/30] training 75.8%: Loss=0.0244313, MSE=0.0244313, MAE=0.121622\n",
      "[22/30] training 97.0%: Loss=0.0241996, MSE=0.0241996, MAE=0.12046\n",
      "-----------------------------------Finished Epoch 22/30: Loss=122.42, RMSE=1.32191, MAE=1.69038, r_2=-4.1519, p=0.475238\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0204129, MSE=0.0204129, MAE=0.110646\n",
      "[23/30] training 39.4%: Loss=0.022555, MSE=0.022555, MAE=0.118576\n",
      "[23/30] training 57.6%: Loss=0.0254244, MSE=0.0254244, MAE=0.125898\n",
      "[23/30] training 75.8%: Loss=0.0247588, MSE=0.0247588, MAE=0.122924\n",
      "[23/30] training 97.0%: Loss=0.0237581, MSE=0.0237581, MAE=0.120871\n",
      "-----------------------------------Finished Epoch 23/30: Loss=121.906, RMSE=1.30279, MAE=1.66903, r_2=-3.33371, p=0.477114\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.0222967, MSE=0.0222967, MAE=0.116797\n",
      "[24/30] training 39.4%: Loss=0.0213662, MSE=0.0213662, MAE=0.113591\n",
      "[24/30] training 57.6%: Loss=0.0223073, MSE=0.0223073, MAE=0.116675\n",
      "[24/30] training 75.8%: Loss=0.0227438, MSE=0.0227438, MAE=0.116888\n",
      "[24/30] training 97.0%: Loss=0.023422, MSE=0.023422, MAE=0.119196\n",
      "-----------------------------------Finished Epoch 24/30: Loss=122.068, RMSE=1.29845, MAE=1.66101, r_2=-3.5214, p=0.487682\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0196894, MSE=0.0196894, MAE=0.113469\n",
      "[25/30] training 39.4%: Loss=0.0207955, MSE=0.0207955, MAE=0.115374\n",
      "[25/30] training 57.6%: Loss=0.0211168, MSE=0.0211168, MAE=0.115424\n",
      "[25/30] training 75.8%: Loss=0.0225411, MSE=0.0225411, MAE=0.11986\n",
      "[25/30] training 97.0%: Loss=0.0227552, MSE=0.0227552, MAE=0.119177\n",
      "-----------------------------------Finished Epoch 25/30: Loss=121.612, RMSE=1.28929, MAE=1.68159, r_2=-2.92154, p=0.473737\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0229949, MSE=0.0229949, MAE=0.1165\n",
      "[26/30] training 39.4%: Loss=0.0215814, MSE=0.0215814, MAE=0.114665\n",
      "[26/30] training 57.6%: Loss=0.0216098, MSE=0.0216098, MAE=0.113749\n",
      "[26/30] training 75.8%: Loss=0.0225809, MSE=0.0225809, MAE=0.116706\n",
      "[26/30] training 97.0%: Loss=0.0225393, MSE=0.0225393, MAE=0.117852\n",
      "-----------------------------------Finished Epoch 26/30: Loss=122.318, RMSE=1.31415, MAE=1.68642, r_2=-3.50781, p=0.468134\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0277232, MSE=0.0277232, MAE=0.127769\n",
      "[27/30] training 39.4%: Loss=0.0243711, MSE=0.0243711, MAE=0.120904\n",
      "[27/30] training 57.6%: Loss=0.0226275, MSE=0.0226275, MAE=0.116925\n",
      "[27/30] training 75.8%: Loss=0.0212796, MSE=0.0212796, MAE=0.113182\n",
      "[27/30] training 97.0%: Loss=0.022449, MSE=0.022449, MAE=0.116837\n",
      "-----------------------------------Finished Epoch 27/30: Loss=121.853, RMSE=1.2896, MAE=1.66616, r_2=-2.94543, p=0.479105\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0282175, MSE=0.0282175, MAE=0.135354\n",
      "[28/30] training 39.4%: Loss=0.024922, MSE=0.024922, MAE=0.124408\n",
      "[28/30] training 57.6%: Loss=0.0250599, MSE=0.0250599, MAE=0.125463\n",
      "[28/30] training 75.8%: Loss=0.0226372, MSE=0.0226372, MAE=0.117516\n",
      "[28/30] training 97.0%: Loss=0.0217316, MSE=0.0217316, MAE=0.115189\n",
      "-----------------------------------Finished Epoch 28/30: Loss=122.095, RMSE=1.28259, MAE=1.64523, r_2=-3.07901, p=0.50252\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0218533, MSE=0.0218533, MAE=0.114982\n",
      "[29/30] training 39.4%: Loss=0.0201598, MSE=0.0201598, MAE=0.11047\n",
      "[29/30] training 57.6%: Loss=0.019886, MSE=0.019886, MAE=0.108844\n",
      "[29/30] training 75.8%: Loss=0.0207087, MSE=0.0207087, MAE=0.111477\n",
      "[29/30] training 97.0%: Loss=0.021927, MSE=0.021927, MAE=0.113668\n",
      "-----------------------------------Finished Epoch 29/30: Loss=121.957, RMSE=1.28197, MAE=1.66261, r_2=-2.76289, p=0.480602\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.025045, MSE=0.025045, MAE=0.126868\n",
      "[30/30] training 39.4%: Loss=0.0261801, MSE=0.0261801, MAE=0.125358\n",
      "[30/30] training 57.6%: Loss=0.0250734, MSE=0.0250734, MAE=0.123373\n",
      "[30/30] training 75.8%: Loss=0.0237951, MSE=0.0237951, MAE=0.120547\n",
      "[30/30] training 97.0%: Loss=0.0223999, MSE=0.0223999, MAE=0.117412\n",
      "-----------------------------------Finished Epoch 30/30: Loss=121.886, RMSE=1.27736, MAE=1.66135, r_2=-2.75585, p=0.482618\n",
      "Saving final model to saved_models/model_fold5_final.pth\n",
      "******************************** FOLD 6 ******************************\n",
      "******************************** FOLD 6 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0490081, MSE=0.0490081, MAE=0.176302\n",
      "[1/30] training 39.4%: Loss=0.0457599, MSE=0.0457599, MAE=0.171605\n",
      "[1/30] training 57.6%: Loss=0.0462571, MSE=0.0462571, MAE=0.17463\n",
      "[1/30] training 75.8%: Loss=0.0436587, MSE=0.0436587, MAE=0.169639\n",
      "[1/30] training 97.0%: Loss=0.0419086, MSE=0.0419086, MAE=0.164291\n",
      "-----------------------------------Finished Epoch 1/30: Loss=119.746, RMSE=1.75431, MAE=2.25483, r_2=-47.767, p=0.388315\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0374018, MSE=0.0374018, MAE=0.153826\n",
      "[2/30] training 39.4%: Loss=0.0433417, MSE=0.0433417, MAE=0.166673\n",
      "[2/30] training 57.6%: Loss=0.0426927, MSE=0.0426927, MAE=0.164024\n",
      "[2/30] training 75.8%: Loss=0.0401213, MSE=0.0401213, MAE=0.158488\n",
      "[2/30] training 97.0%: Loss=0.0382829, MSE=0.0382829, MAE=0.154644\n",
      "-----------------------------------Finished Epoch 2/30: Loss=119.667, RMSE=1.7093, MAE=2.14766, r_2=-12.0158, p=0.467881\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0303426, MSE=0.0303426, MAE=0.140068\n",
      "[3/30] training 39.4%: Loss=0.0347695, MSE=0.0347695, MAE=0.145456\n",
      "[3/30] training 57.6%: Loss=0.0384847, MSE=0.0384847, MAE=0.154921\n",
      "[3/30] training 75.8%: Loss=0.0379566, MSE=0.0379566, MAE=0.154245\n",
      "[3/30] training 97.0%: Loss=0.036048, MSE=0.036048, MAE=0.15075\n",
      "-----------------------------------Finished Epoch 3/30: Loss=119.818, RMSE=1.66593, MAE=2.08717, r_2=-6.53123, p=0.526829\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0342793, MSE=0.0342793, MAE=0.145389\n",
      "[4/30] training 39.4%: Loss=0.0326158, MSE=0.0326158, MAE=0.142743\n",
      "[4/30] training 57.6%: Loss=0.0335825, MSE=0.0335825, MAE=0.145523\n",
      "[4/30] training 75.8%: Loss=0.0345235, MSE=0.0345235, MAE=0.147719\n",
      "[4/30] training 97.0%: Loss=0.0353056, MSE=0.0353056, MAE=0.149633\n",
      "-----------------------------------Finished Epoch 4/30: Loss=119.938, RMSE=1.64501, MAE=2.06757, r_2=-5.04492, p=0.550581\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.026056, MSE=0.026056, MAE=0.128538\n",
      "[5/30] training 39.4%: Loss=0.0298353, MSE=0.0298353, MAE=0.135427\n",
      "[5/30] training 57.6%: Loss=0.0308932, MSE=0.0308932, MAE=0.137976\n",
      "[5/30] training 75.8%: Loss=0.0306099, MSE=0.0306099, MAE=0.137234\n",
      "[5/30] training 97.0%: Loss=0.0316741, MSE=0.0316741, MAE=0.141095\n",
      "-----------------------------------Finished Epoch 5/30: Loss=119.831, RMSE=1.61473, MAE=2.0144, r_2=-3.59922, p=0.57198\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0312473, MSE=0.0312473, MAE=0.141454\n",
      "[6/30] training 39.4%: Loss=0.0324578, MSE=0.0324578, MAE=0.145629\n",
      "[6/30] training 57.6%: Loss=0.0301378, MSE=0.0301378, MAE=0.138691\n",
      "[6/30] training 75.8%: Loss=0.031307, MSE=0.031307, MAE=0.141144\n",
      "[6/30] training 97.0%: Loss=0.0320506, MSE=0.0320506, MAE=0.142313\n",
      "-----------------------------------Finished Epoch 6/30: Loss=119.886, RMSE=1.6033, MAE=2.00986, r_2=-3.05404, p=0.575189\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0287593, MSE=0.0287593, MAE=0.134331\n",
      "[7/30] training 39.4%: Loss=0.0286001, MSE=0.0286001, MAE=0.135008\n",
      "[7/30] training 57.6%: Loss=0.029769, MSE=0.029769, MAE=0.136453\n",
      "[7/30] training 75.8%: Loss=0.0312583, MSE=0.0312583, MAE=0.139704\n",
      "[7/30] training 97.0%: Loss=0.0311902, MSE=0.0311902, MAE=0.138868\n",
      "-----------------------------------Finished Epoch 7/30: Loss=119.838, RMSE=1.57707, MAE=1.9775, r_2=-2.81617, p=0.596032\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.0291738, MSE=0.0291738, MAE=0.138251\n",
      "[8/30] training 39.4%: Loss=0.0272905, MSE=0.0272905, MAE=0.130876\n",
      "[8/30] training 57.6%: Loss=0.0274516, MSE=0.0274516, MAE=0.131586\n",
      "[8/30] training 75.8%: Loss=0.029256, MSE=0.029256, MAE=0.137301\n",
      "[8/30] training 97.0%: Loss=0.0300071, MSE=0.0300071, MAE=0.137709\n",
      "-----------------------------------Finished Epoch 8/30: Loss=119.697, RMSE=1.55117, MAE=1.9488, r_2=-2.61132, p=0.602508\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0276684, MSE=0.0276684, MAE=0.131286\n",
      "[9/30] training 39.4%: Loss=0.0267342, MSE=0.0267342, MAE=0.1294\n",
      "[9/30] training 57.6%: Loss=0.0272317, MSE=0.0272317, MAE=0.131\n",
      "[9/30] training 75.8%: Loss=0.0282059, MSE=0.0282059, MAE=0.133156\n",
      "[9/30] training 97.0%: Loss=0.0298236, MSE=0.0298236, MAE=0.135321\n",
      "-----------------------------------Finished Epoch 9/30: Loss=119.806, RMSE=1.54909, MAE=1.95069, r_2=-2.15015, p=0.604255\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0264727, MSE=0.0264727, MAE=0.131298\n",
      "[10/30] training 39.4%: Loss=0.0286284, MSE=0.0286284, MAE=0.131848\n",
      "[10/30] training 57.6%: Loss=0.0296035, MSE=0.0296035, MAE=0.13391\n",
      "[10/30] training 75.8%: Loss=0.0288171, MSE=0.0288171, MAE=0.132891\n",
      "[10/30] training 97.0%: Loss=0.0286201, MSE=0.0286201, MAE=0.133603\n",
      "-----------------------------------Finished Epoch 10/30: Loss=119.541, RMSE=1.51624, MAE=1.91787, r_2=-1.92388, p=0.601579\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0228461, MSE=0.0228461, MAE=0.12173\n",
      "[11/30] training 39.4%: Loss=0.0236759, MSE=0.0236759, MAE=0.125329\n",
      "[11/30] training 57.6%: Loss=0.0256337, MSE=0.0256337, MAE=0.129223\n",
      "[11/30] training 75.8%: Loss=0.0283138, MSE=0.0283138, MAE=0.135826\n",
      "[11/30] training 97.0%: Loss=0.0283206, MSE=0.0283206, MAE=0.13382\n",
      "-----------------------------------Finished Epoch 11/30: Loss=119.788, RMSE=1.54229, MAE=1.92888, r_2=-1.54615, p=0.610221\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0336237, MSE=0.0336237, MAE=0.141528\n",
      "[12/30] training 39.4%: Loss=0.03088, MSE=0.03088, MAE=0.136041\n",
      "[12/30] training 57.6%: Loss=0.0287861, MSE=0.0287861, MAE=0.132481\n",
      "[12/30] training 75.8%: Loss=0.0288463, MSE=0.0288463, MAE=0.133643\n",
      "[12/30] training 97.0%: Loss=0.0283578, MSE=0.0283578, MAE=0.132731\n",
      "-----------------------------------Finished Epoch 12/30: Loss=119.811, RMSE=1.53795, MAE=1.91854, r_2=-1.49478, p=0.619805\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0271618, MSE=0.0271618, MAE=0.123692\n",
      "[13/30] training 39.4%: Loss=0.0251269, MSE=0.0251269, MAE=0.122256\n",
      "[13/30] training 57.6%: Loss=0.0263858, MSE=0.0263858, MAE=0.127877\n",
      "[13/30] training 75.8%: Loss=0.027868, MSE=0.027868, MAE=0.131927\n",
      "[13/30] training 97.0%: Loss=0.0273371, MSE=0.0273371, MAE=0.130399\n",
      "-----------------------------------Finished Epoch 13/30: Loss=119.554, RMSE=1.50615, MAE=1.88089, r_2=-1.24455, p=0.619515\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0300673, MSE=0.0300673, MAE=0.136202\n",
      "[14/30] training 39.4%: Loss=0.0302385, MSE=0.0302385, MAE=0.134307\n",
      "[14/30] training 57.6%: Loss=0.0306238, MSE=0.0306238, MAE=0.134507\n",
      "[14/30] training 75.8%: Loss=0.0287519, MSE=0.0287519, MAE=0.131705\n",
      "[14/30] training 97.0%: Loss=0.0276346, MSE=0.0276346, MAE=0.129641\n",
      "-----------------------------------Finished Epoch 14/30: Loss=119.746, RMSE=1.50521, MAE=1.89461, r_2=-1.31383, p=0.627858\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0274875, MSE=0.0274875, MAE=0.128311\n",
      "[15/30] training 39.4%: Loss=0.0272893, MSE=0.0272893, MAE=0.128368\n",
      "[15/30] training 57.6%: Loss=0.0264707, MSE=0.0264707, MAE=0.127689\n",
      "[15/30] training 75.8%: Loss=0.0264506, MSE=0.0264506, MAE=0.127109\n",
      "[15/30] training 97.0%: Loss=0.027142, MSE=0.027142, MAE=0.128966\n",
      "-----------------------------------Finished Epoch 15/30: Loss=119.509, RMSE=1.4757, MAE=1.87322, r_2=-1.39028, p=0.623994\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0233566, MSE=0.0233566, MAE=0.122464\n",
      "[16/30] training 39.4%: Loss=0.0256685, MSE=0.0256685, MAE=0.127046\n",
      "[16/30] training 57.6%: Loss=0.0248714, MSE=0.0248714, MAE=0.124481\n",
      "[16/30] training 75.8%: Loss=0.0243795, MSE=0.0243795, MAE=0.122937\n",
      "[16/30] training 97.0%: Loss=0.0253422, MSE=0.0253422, MAE=0.124859\n",
      "-----------------------------------Finished Epoch 16/30: Loss=119.531, RMSE=1.46138, MAE=1.84344, r_2=-1.31255, p=0.645867\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0228792, MSE=0.0228792, MAE=0.118227\n",
      "[17/30] training 39.4%: Loss=0.0254888, MSE=0.0254888, MAE=0.125452\n",
      "[17/30] training 57.6%: Loss=0.0260907, MSE=0.0260907, MAE=0.125857\n",
      "[17/30] training 75.8%: Loss=0.0270087, MSE=0.0270087, MAE=0.128695\n",
      "[17/30] training 97.0%: Loss=0.0254278, MSE=0.0254278, MAE=0.125783\n",
      "-----------------------------------Finished Epoch 17/30: Loss=119.27, RMSE=1.42944, MAE=1.80289, r_2=-1.14505, p=0.654531\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0250559, MSE=0.0250559, MAE=0.123048\n",
      "[18/30] training 39.4%: Loss=0.0229426, MSE=0.0229426, MAE=0.119542\n",
      "[18/30] training 57.6%: Loss=0.0237404, MSE=0.0237404, MAE=0.120527\n",
      "[18/30] training 75.8%: Loss=0.0239643, MSE=0.0239643, MAE=0.121071\n",
      "[18/30] training 97.0%: Loss=0.0238708, MSE=0.0238708, MAE=0.121068\n",
      "-----------------------------------Finished Epoch 18/30: Loss=119.522, RMSE=1.43841, MAE=1.81461, r_2=-1.01762, p=0.658806\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0231727, MSE=0.0231727, MAE=0.120324\n",
      "[19/30] training 39.4%: Loss=0.0224129, MSE=0.0224129, MAE=0.117696\n",
      "[19/30] training 57.6%: Loss=0.0252689, MSE=0.0252689, MAE=0.126302\n",
      "[19/30] training 75.8%: Loss=0.0246847, MSE=0.0246847, MAE=0.124868\n",
      "[19/30] training 97.0%: Loss=0.0240803, MSE=0.0240803, MAE=0.122422\n",
      "-----------------------------------Finished Epoch 19/30: Loss=119.249, RMSE=1.40799, MAE=1.7847, r_2=-0.871242, p=0.658944\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0201962, MSE=0.0201962, MAE=0.116452\n",
      "[20/30] training 39.4%: Loss=0.0220313, MSE=0.0220313, MAE=0.12107\n",
      "[20/30] training 57.6%: Loss=0.0219477, MSE=0.0219477, MAE=0.119765\n",
      "[20/30] training 75.8%: Loss=0.022594, MSE=0.022594, MAE=0.120633\n",
      "[20/30] training 97.0%: Loss=0.0225054, MSE=0.0225054, MAE=0.11992\n",
      "-----------------------------------Finished Epoch 20/30: Loss=119.12, RMSE=1.38406, MAE=1.74361, r_2=-0.650111, p=0.674441\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0277423, MSE=0.0277423, MAE=0.132222\n",
      "[21/30] training 39.4%: Loss=0.0241495, MSE=0.0241495, MAE=0.122101\n",
      "[21/30] training 57.6%: Loss=0.0238598, MSE=0.0238598, MAE=0.120988\n",
      "[21/30] training 75.8%: Loss=0.0233022, MSE=0.0233022, MAE=0.121196\n",
      "[21/30] training 97.0%: Loss=0.0227277, MSE=0.0227277, MAE=0.118818\n",
      "-----------------------------------Finished Epoch 21/30: Loss=119.422, RMSE=1.40174, MAE=1.76414, r_2=-0.871448, p=0.682119\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0207199, MSE=0.0207199, MAE=0.111993\n",
      "[22/30] training 39.4%: Loss=0.0196869, MSE=0.0196869, MAE=0.108308\n",
      "[22/30] training 57.6%: Loss=0.0210634, MSE=0.0210634, MAE=0.112703\n",
      "[22/30] training 75.8%: Loss=0.0212435, MSE=0.0212435, MAE=0.113315\n",
      "[22/30] training 97.0%: Loss=0.0222422, MSE=0.0222422, MAE=0.116271\n",
      "-----------------------------------Finished Epoch 22/30: Loss=119.414, RMSE=1.39788, MAE=1.7523, r_2=-0.736942, p=0.68543\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.022908, MSE=0.022908, MAE=0.119039\n",
      "[23/30] training 39.4%: Loss=0.021072, MSE=0.021072, MAE=0.114333\n",
      "[23/30] training 57.6%: Loss=0.0214126, MSE=0.0214126, MAE=0.114976\n",
      "[23/30] training 75.8%: Loss=0.022892, MSE=0.022892, MAE=0.118574\n",
      "[23/30] training 97.0%: Loss=0.0230317, MSE=0.0230317, MAE=0.119669\n",
      "-----------------------------------Finished Epoch 23/30: Loss=119.182, RMSE=1.37697, MAE=1.73046, r_2=-0.840699, p=0.690995\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.0218618, MSE=0.0218618, MAE=0.117162\n",
      "[24/30] training 39.4%: Loss=0.0225792, MSE=0.0225792, MAE=0.118258\n",
      "[24/30] training 57.6%: Loss=0.0224653, MSE=0.0224653, MAE=0.11642\n",
      "[24/30] training 75.8%: Loss=0.0213183, MSE=0.0213183, MAE=0.113788\n",
      "[24/30] training 97.0%: Loss=0.0213546, MSE=0.0213546, MAE=0.114824\n",
      "-----------------------------------Finished Epoch 24/30: Loss=119.251, RMSE=1.38402, MAE=1.7237, r_2=-0.666877, p=0.692431\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.021632, MSE=0.021632, MAE=0.112568\n",
      "[25/30] training 39.4%: Loss=0.0212104, MSE=0.0212104, MAE=0.112933\n",
      "[25/30] training 57.6%: Loss=0.0242818, MSE=0.0242818, MAE=0.123347\n",
      "[25/30] training 75.8%: Loss=0.0235989, MSE=0.0235989, MAE=0.121052\n",
      "[25/30] training 97.0%: Loss=0.022748, MSE=0.022748, MAE=0.117801\n",
      "-----------------------------------Finished Epoch 25/30: Loss=119.074, RMSE=1.36825, MAE=1.71663, r_2=-0.651335, p=0.689673\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0228129, MSE=0.0228129, MAE=0.118829\n",
      "[26/30] training 39.4%: Loss=0.0207945, MSE=0.0207945, MAE=0.115069\n",
      "[26/30] training 57.6%: Loss=0.0218522, MSE=0.0218522, MAE=0.117022\n",
      "[26/30] training 75.8%: Loss=0.0208998, MSE=0.0208998, MAE=0.114238\n",
      "[26/30] training 97.0%: Loss=0.0210321, MSE=0.0210321, MAE=0.114194\n",
      "-----------------------------------Finished Epoch 26/30: Loss=119.508, RMSE=1.39924, MAE=1.75125, r_2=-0.732701, p=0.692499\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0240235, MSE=0.0240235, MAE=0.119459\n",
      "[27/30] training 39.4%: Loss=0.0225111, MSE=0.0225111, MAE=0.118973\n",
      "[27/30] training 57.6%: Loss=0.0208649, MSE=0.0208649, MAE=0.11268\n",
      "[27/30] training 75.8%: Loss=0.0209347, MSE=0.0209347, MAE=0.111591\n",
      "[27/30] training 97.0%: Loss=0.0205259, MSE=0.0205259, MAE=0.110551\n",
      "-----------------------------------Finished Epoch 27/30: Loss=119.397, RMSE=1.38286, MAE=1.71275, r_2=-0.533397, p=0.70304\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0166859, MSE=0.0166859, MAE=0.102496\n",
      "[28/30] training 39.4%: Loss=0.0208629, MSE=0.0208629, MAE=0.112146\n",
      "[28/30] training 57.6%: Loss=0.0210833, MSE=0.0210833, MAE=0.113703\n",
      "[28/30] training 75.8%: Loss=0.0217703, MSE=0.0217703, MAE=0.116343\n",
      "[28/30] training 97.0%: Loss=0.0216804, MSE=0.0216804, MAE=0.115102\n",
      "-----------------------------------Finished Epoch 28/30: Loss=119.179, RMSE=1.35185, MAE=1.69883, r_2=-0.5409, p=0.700545\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0239331, MSE=0.0239331, MAE=0.118605\n",
      "[29/30] training 39.4%: Loss=0.0219116, MSE=0.0219116, MAE=0.114571\n",
      "[29/30] training 57.6%: Loss=0.0222999, MSE=0.0222999, MAE=0.118073\n",
      "[29/30] training 75.8%: Loss=0.0226814, MSE=0.0226814, MAE=0.120065\n",
      "[29/30] training 97.0%: Loss=0.0219066, MSE=0.0219066, MAE=0.117317\n",
      "-----------------------------------Finished Epoch 29/30: Loss=119.418, RMSE=1.37127, MAE=1.71689, r_2=-0.707557, p=0.707596\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0234967, MSE=0.0234967, MAE=0.119128\n",
      "[30/30] training 39.4%: Loss=0.0239971, MSE=0.0239971, MAE=0.123233\n",
      "[30/30] training 57.6%: Loss=0.0224468, MSE=0.0224469, MAE=0.118697\n",
      "[30/30] training 75.8%: Loss=0.0213116, MSE=0.0213116, MAE=0.116636\n",
      "[30/30] training 97.0%: Loss=0.0217116, MSE=0.0217116, MAE=0.116445\n",
      "-----------------------------------Finished Epoch 30/30: Loss=118.982, RMSE=1.34123, MAE=1.68867, r_2=-0.606432, p=0.703241\n",
      "Saving final model to saved_models/model_fold6_final.pth\n",
      "******************************** FOLD 7 ******************************\n",
      "******************************** FOLD 7 ******************************\n",
      "Loaded 520 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0511241, MSE=0.0511241, MAE=0.180849\n",
      "[1/30] training 39.4%: Loss=0.0443288, MSE=0.0443288, MAE=0.167109\n",
      "[1/30] training 57.6%: Loss=0.0436241, MSE=0.0436241, MAE=0.168449\n",
      "[1/30] training 75.8%: Loss=0.0419717, MSE=0.0419717, MAE=0.164696\n",
      "[1/30] training 97.0%: Loss=0.0413847, MSE=0.0413847, MAE=0.162877\n",
      "-----------------------------------Finished Epoch 1/30: Loss=117.812, RMSE=1.60567, MAE=1.95354, r_2=-51.4613, p=0.354953\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0381146, MSE=0.0381146, MAE=0.151445\n",
      "[2/30] training 39.4%: Loss=0.0415891, MSE=0.0415891, MAE=0.160484\n",
      "[2/30] training 57.6%: Loss=0.0405973, MSE=0.0405973, MAE=0.159943\n",
      "[2/30] training 75.8%: Loss=0.0387176, MSE=0.0387176, MAE=0.156555\n",
      "[2/30] training 97.0%: Loss=0.0377518, MSE=0.0377518, MAE=0.154052\n",
      "-----------------------------------Finished Epoch 2/30: Loss=117.733, RMSE=1.53591, MAE=1.85556, r_2=-11.0563, p=0.448815\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.034151, MSE=0.034151, MAE=0.145421\n",
      "[3/30] training 39.4%: Loss=0.0342646, MSE=0.0342646, MAE=0.1465\n",
      "[3/30] training 57.6%: Loss=0.0342138, MSE=0.0342138, MAE=0.146979\n",
      "[3/30] training 75.8%: Loss=0.0344648, MSE=0.0344648, MAE=0.148216\n",
      "[3/30] training 97.0%: Loss=0.0331623, MSE=0.0331623, MAE=0.145481\n",
      "-----------------------------------Finished Epoch 3/30: Loss=117.769, RMSE=1.47762, MAE=1.79304, r_2=-4.76465, p=0.486355\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0330063, MSE=0.0330063, MAE=0.142999\n",
      "[4/30] training 39.4%: Loss=0.030823, MSE=0.030823, MAE=0.136115\n",
      "[4/30] training 57.6%: Loss=0.0317243, MSE=0.0317243, MAE=0.140236\n",
      "[4/30] training 75.8%: Loss=0.0343645, MSE=0.0343645, MAE=0.146785\n",
      "[4/30] training 97.0%: Loss=0.0333643, MSE=0.0333643, MAE=0.144319\n",
      "-----------------------------------Finished Epoch 4/30: Loss=117.708, RMSE=1.47755, MAE=1.78249, r_2=-4.21355, p=0.493024\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0329427, MSE=0.0329427, MAE=0.144951\n",
      "[5/30] training 39.4%: Loss=0.0330326, MSE=0.0330326, MAE=0.143959\n",
      "[5/30] training 57.6%: Loss=0.0339263, MSE=0.0339263, MAE=0.14567\n",
      "[5/30] training 75.8%: Loss=0.0332205, MSE=0.0332205, MAE=0.144045\n",
      "[5/30] training 97.0%: Loss=0.032381, MSE=0.032381, MAE=0.14321\n",
      "-----------------------------------Finished Epoch 5/30: Loss=117.89, RMSE=1.45507, MAE=1.77291, r_2=-3.83187, p=0.503919\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0297485, MSE=0.0297485, MAE=0.12861\n",
      "[6/30] training 39.4%: Loss=0.0298246, MSE=0.0298246, MAE=0.133811\n",
      "[6/30] training 57.6%: Loss=0.0302249, MSE=0.0302249, MAE=0.136512\n",
      "[6/30] training 75.8%: Loss=0.0310642, MSE=0.0310642, MAE=0.13835\n",
      "[6/30] training 97.0%: Loss=0.0307815, MSE=0.0307815, MAE=0.137804\n",
      "-----------------------------------Finished Epoch 6/30: Loss=117.651, RMSE=1.45524, MAE=1.75588, r_2=-2.87282, p=0.50881\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0280921, MSE=0.0280921, MAE=0.128188\n",
      "[7/30] training 39.4%: Loss=0.0269972, MSE=0.0269972, MAE=0.127327\n",
      "[7/30] training 57.6%: Loss=0.0282901, MSE=0.0282901, MAE=0.131477\n",
      "[7/30] training 75.8%: Loss=0.0294145, MSE=0.0294145, MAE=0.134732\n",
      "[7/30] training 97.0%: Loss=0.0294818, MSE=0.0294818, MAE=0.135851\n",
      "-----------------------------------Finished Epoch 7/30: Loss=117.831, RMSE=1.44157, MAE=1.76271, r_2=-2.88847, p=0.504115\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.0296911, MSE=0.0296911, MAE=0.139962\n",
      "[8/30] training 39.4%: Loss=0.0319142, MSE=0.0319142, MAE=0.144274\n",
      "[8/30] training 57.6%: Loss=0.0318343, MSE=0.0318343, MAE=0.139946\n",
      "[8/30] training 75.8%: Loss=0.0301897, MSE=0.0301897, MAE=0.136451\n",
      "[8/30] training 97.0%: Loss=0.0291799, MSE=0.0291799, MAE=0.134922\n",
      "-----------------------------------Finished Epoch 8/30: Loss=117.733, RMSE=1.45579, MAE=1.76827, r_2=-2.84552, p=0.496421\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0249342, MSE=0.0249342, MAE=0.124562\n",
      "[9/30] training 39.4%: Loss=0.0281622, MSE=0.0281622, MAE=0.133053\n",
      "[9/30] training 57.6%: Loss=0.0275126, MSE=0.0275126, MAE=0.13358\n",
      "[9/30] training 75.8%: Loss=0.0281207, MSE=0.0281207, MAE=0.13418\n",
      "[9/30] training 97.0%: Loss=0.0286411, MSE=0.0286411, MAE=0.133742\n",
      "-----------------------------------Finished Epoch 9/30: Loss=117.732, RMSE=1.43975, MAE=1.75551, r_2=-2.3478, p=0.505735\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.02788, MSE=0.02788, MAE=0.131266\n",
      "[10/30] training 39.4%: Loss=0.0305116, MSE=0.0305116, MAE=0.140376\n",
      "[10/30] training 57.6%: Loss=0.0294357, MSE=0.0294357, MAE=0.137273\n",
      "[10/30] training 75.8%: Loss=0.0289579, MSE=0.0289579, MAE=0.135703\n",
      "[10/30] training 97.0%: Loss=0.0271574, MSE=0.0271574, MAE=0.129768\n",
      "-----------------------------------Finished Epoch 10/30: Loss=117.695, RMSE=1.46118, MAE=1.77401, r_2=-2.65086, p=0.489476\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0274866, MSE=0.0274866, MAE=0.133325\n",
      "[11/30] training 39.4%: Loss=0.0260439, MSE=0.0260439, MAE=0.128032\n",
      "[11/30] training 57.6%: Loss=0.0277777, MSE=0.0277777, MAE=0.133874\n",
      "[11/30] training 75.8%: Loss=0.0277786, MSE=0.0277786, MAE=0.132063\n",
      "[11/30] training 97.0%: Loss=0.0274083, MSE=0.0274083, MAE=0.129982\n",
      "-----------------------------------Finished Epoch 11/30: Loss=117.688, RMSE=1.45328, MAE=1.77359, r_2=-2.32571, p=0.488853\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0335615, MSE=0.0335615, MAE=0.144888\n",
      "[12/30] training 39.4%: Loss=0.027449, MSE=0.027449, MAE=0.130005\n",
      "[12/30] training 57.6%: Loss=0.0273686, MSE=0.0273686, MAE=0.130754\n",
      "[12/30] training 75.8%: Loss=0.0271477, MSE=0.0271477, MAE=0.130814\n",
      "[12/30] training 97.0%: Loss=0.0262179, MSE=0.0262179, MAE=0.128318\n",
      "-----------------------------------Finished Epoch 12/30: Loss=117.549, RMSE=1.47432, MAE=1.77097, r_2=-2.13693, p=0.491966\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0233697, MSE=0.0233697, MAE=0.118885\n",
      "[13/30] training 39.4%: Loss=0.0244746, MSE=0.0244746, MAE=0.120942\n",
      "[13/30] training 57.6%: Loss=0.0254429, MSE=0.0254429, MAE=0.124259\n",
      "[13/30] training 75.8%: Loss=0.024562, MSE=0.024562, MAE=0.121681\n",
      "[13/30] training 97.0%: Loss=0.0255198, MSE=0.0255198, MAE=0.124646\n",
      "-----------------------------------Finished Epoch 13/30: Loss=117.503, RMSE=1.45777, MAE=1.75095, r_2=-1.92924, p=0.509769\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0265175, MSE=0.0265175, MAE=0.12929\n",
      "[14/30] training 39.4%: Loss=0.0261371, MSE=0.0261371, MAE=0.128759\n",
      "[14/30] training 57.6%: Loss=0.0258033, MSE=0.0258033, MAE=0.128378\n",
      "[14/30] training 75.8%: Loss=0.026071, MSE=0.026071, MAE=0.129057\n",
      "[14/30] training 97.0%: Loss=0.0251334, MSE=0.0251334, MAE=0.125454\n",
      "-----------------------------------Finished Epoch 14/30: Loss=117.37, RMSE=1.4739, MAE=1.75751, r_2=-1.82215, p=0.508379\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.0230934, MSE=0.0230934, MAE=0.120736\n",
      "[15/30] training 39.4%: Loss=0.0260863, MSE=0.0260863, MAE=0.128827\n",
      "[15/30] training 57.6%: Loss=0.0257181, MSE=0.0257181, MAE=0.125311\n",
      "[15/30] training 75.8%: Loss=0.0244274, MSE=0.0244274, MAE=0.122618\n",
      "[15/30] training 97.0%: Loss=0.0248619, MSE=0.0248619, MAE=0.124309\n",
      "-----------------------------------Finished Epoch 15/30: Loss=117.685, RMSE=1.44648, MAE=1.77244, r_2=-1.92589, p=0.490168\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.027086, MSE=0.027086, MAE=0.129948\n",
      "[16/30] training 39.4%: Loss=0.0269529, MSE=0.0269529, MAE=0.128996\n",
      "[16/30] training 57.6%: Loss=0.0255302, MSE=0.0255302, MAE=0.12497\n",
      "[16/30] training 75.8%: Loss=0.0258403, MSE=0.0258403, MAE=0.126214\n",
      "[16/30] training 97.0%: Loss=0.0250506, MSE=0.0250506, MAE=0.124206\n",
      "-----------------------------------Finished Epoch 16/30: Loss=117.519, RMSE=1.45521, MAE=1.76947, r_2=-1.54374, p=0.495993\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0219465, MSE=0.0219465, MAE=0.112515\n",
      "[17/30] training 39.4%: Loss=0.0215894, MSE=0.0215894, MAE=0.114141\n",
      "[17/30] training 57.6%: Loss=0.0210666, MSE=0.0210666, MAE=0.11202\n",
      "[17/30] training 75.8%: Loss=0.0219551, MSE=0.0219551, MAE=0.112595\n",
      "[17/30] training 97.0%: Loss=0.0240165, MSE=0.0240165, MAE=0.118928\n",
      "-----------------------------------Finished Epoch 17/30: Loss=117.256, RMSE=1.50359, MAE=1.78398, r_2=-1.98615, p=0.492118\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0217908, MSE=0.0217908, MAE=0.115673\n",
      "[18/30] training 39.4%: Loss=0.0254259, MSE=0.0254259, MAE=0.125537\n",
      "[18/30] training 57.6%: Loss=0.0260167, MSE=0.0260167, MAE=0.125641\n",
      "[18/30] training 75.8%: Loss=0.0260346, MSE=0.0260346, MAE=0.126423\n",
      "[18/30] training 97.0%: Loss=0.0248951, MSE=0.0248951, MAE=0.123364\n",
      "-----------------------------------Finished Epoch 18/30: Loss=117.633, RMSE=1.44657, MAE=1.7654, r_2=-1.8252, p=0.496283\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0224478, MSE=0.0224478, MAE=0.116425\n",
      "[19/30] training 39.4%: Loss=0.0213765, MSE=0.0213765, MAE=0.114222\n",
      "[19/30] training 57.6%: Loss=0.0237984, MSE=0.0237984, MAE=0.120743\n",
      "[19/30] training 75.8%: Loss=0.0235472, MSE=0.0235472, MAE=0.120528\n",
      "[19/30] training 97.0%: Loss=0.0234414, MSE=0.0234414, MAE=0.119421\n",
      "-----------------------------------Finished Epoch 19/30: Loss=117.239, RMSE=1.49087, MAE=1.77298, r_2=-1.71423, p=0.502508\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0192262, MSE=0.0192262, MAE=0.108396\n",
      "[20/30] training 39.4%: Loss=0.0251104, MSE=0.0251104, MAE=0.123285\n",
      "[20/30] training 57.6%: Loss=0.0229543, MSE=0.0229543, MAE=0.118924\n",
      "[20/30] training 75.8%: Loss=0.0221499, MSE=0.0221499, MAE=0.116669\n",
      "[20/30] training 97.0%: Loss=0.021853, MSE=0.021853, MAE=0.116602\n",
      "-----------------------------------Finished Epoch 20/30: Loss=117.398, RMSE=1.45546, MAE=1.76169, r_2=-1.45531, p=0.505537\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0207278, MSE=0.0207278, MAE=0.116383\n",
      "[21/30] training 39.4%: Loss=0.0220503, MSE=0.0220503, MAE=0.115604\n",
      "[21/30] training 57.6%: Loss=0.0231175, MSE=0.0231175, MAE=0.118794\n",
      "[21/30] training 75.8%: Loss=0.023197, MSE=0.023197, MAE=0.118417\n",
      "[21/30] training 97.0%: Loss=0.0237771, MSE=0.0237771, MAE=0.119958\n",
      "-----------------------------------Finished Epoch 21/30: Loss=117.719, RMSE=1.41948, MAE=1.7691, r_2=-1.60631, p=0.495003\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0239698, MSE=0.0239698, MAE=0.116807\n",
      "[22/30] training 39.4%: Loss=0.0229021, MSE=0.0229021, MAE=0.116761\n",
      "[22/30] training 57.6%: Loss=0.0231552, MSE=0.0231552, MAE=0.117095\n",
      "[22/30] training 75.8%: Loss=0.0234748, MSE=0.0234748, MAE=0.118574\n",
      "[22/30] training 97.0%: Loss=0.0224904, MSE=0.0224904, MAE=0.116638\n",
      "-----------------------------------Finished Epoch 22/30: Loss=117.367, RMSE=1.47027, MAE=1.77589, r_2=-1.60101, p=0.494836\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0271252, MSE=0.0271252, MAE=0.128398\n",
      "[23/30] training 39.4%: Loss=0.0257143, MSE=0.0257143, MAE=0.126052\n",
      "[23/30] training 57.6%: Loss=0.0238132, MSE=0.0238132, MAE=0.121179\n",
      "[23/30] training 75.8%: Loss=0.0222511, MSE=0.0222511, MAE=0.115527\n",
      "[23/30] training 97.0%: Loss=0.0218098, MSE=0.0218098, MAE=0.11564\n",
      "-----------------------------------Finished Epoch 23/30: Loss=117.156, RMSE=1.47873, MAE=1.77606, r_2=-1.4412, p=0.506706\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.0186892, MSE=0.0186892, MAE=0.0995431\n",
      "[24/30] training 39.4%: Loss=0.0200291, MSE=0.0200291, MAE=0.107292\n",
      "[24/30] training 57.6%: Loss=0.0199459, MSE=0.0199459, MAE=0.109275\n",
      "[24/30] training 75.8%: Loss=0.0198595, MSE=0.0198595, MAE=0.110379\n",
      "[24/30] training 97.0%: Loss=0.0208671, MSE=0.0208671, MAE=0.112727\n",
      "-----------------------------------Finished Epoch 24/30: Loss=117.684, RMSE=1.4372, MAE=1.76576, r_2=-1.39869, p=0.499471\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0193331, MSE=0.0193331, MAE=0.107446\n",
      "[25/30] training 39.4%: Loss=0.0195475, MSE=0.0195475, MAE=0.109976\n",
      "[25/30] training 57.6%: Loss=0.0220366, MSE=0.0220366, MAE=0.116193\n",
      "[25/30] training 75.8%: Loss=0.0210594, MSE=0.0210594, MAE=0.1133\n",
      "[25/30] training 97.0%: Loss=0.0223446, MSE=0.0223446, MAE=0.116821\n",
      "-----------------------------------Finished Epoch 25/30: Loss=117.217, RMSE=1.48809, MAE=1.76947, r_2=-1.1536, p=0.511282\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0235985, MSE=0.0235985, MAE=0.120212\n",
      "[26/30] training 39.4%: Loss=0.0215798, MSE=0.0215798, MAE=0.113228\n",
      "[26/30] training 57.6%: Loss=0.0214077, MSE=0.0214077, MAE=0.11463\n",
      "[26/30] training 75.8%: Loss=0.0209174, MSE=0.0209174, MAE=0.113392\n",
      "[26/30] training 97.0%: Loss=0.0202069, MSE=0.0202069, MAE=0.111415\n",
      "-----------------------------------Finished Epoch 26/30: Loss=117.277, RMSE=1.47984, MAE=1.7669, r_2=-1.15526, p=0.510174\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0161952, MSE=0.0161952, MAE=0.0982064\n",
      "[27/30] training 39.4%: Loss=0.0207229, MSE=0.0207229, MAE=0.110853\n",
      "[27/30] training 57.6%: Loss=0.02031, MSE=0.02031, MAE=0.110347\n",
      "[27/30] training 75.8%: Loss=0.0211689, MSE=0.0211689, MAE=0.113347\n",
      "[27/30] training 97.0%: Loss=0.0212225, MSE=0.0212225, MAE=0.114113\n",
      "-----------------------------------Finished Epoch 27/30: Loss=117.207, RMSE=1.48322, MAE=1.77288, r_2=-1.24572, p=0.508133\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0223291, MSE=0.0223291, MAE=0.112171\n",
      "[28/30] training 39.4%: Loss=0.0215333, MSE=0.0215333, MAE=0.112889\n",
      "[28/30] training 57.6%: Loss=0.0213307, MSE=0.0213307, MAE=0.113051\n",
      "[28/30] training 75.8%: Loss=0.0208177, MSE=0.0208177, MAE=0.112651\n",
      "[28/30] training 97.0%: Loss=0.0210839, MSE=0.0210839, MAE=0.113346\n",
      "-----------------------------------Finished Epoch 28/30: Loss=117.547, RMSE=1.44497, MAE=1.76666, r_2=-1.01423, p=0.506343\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.019204, MSE=0.019204, MAE=0.106813\n",
      "[29/30] training 39.4%: Loss=0.0188688, MSE=0.0188688, MAE=0.104592\n",
      "[29/30] training 57.6%: Loss=0.0211472, MSE=0.0211472, MAE=0.111596\n",
      "[29/30] training 75.8%: Loss=0.0215362, MSE=0.0215362, MAE=0.112271\n",
      "[29/30] training 97.0%: Loss=0.0207092, MSE=0.0207092, MAE=0.111662\n",
      "-----------------------------------Finished Epoch 29/30: Loss=117.143, RMSE=1.49781, MAE=1.78777, r_2=-1.22186, p=0.501994\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0192634, MSE=0.0192634, MAE=0.104215\n",
      "[30/30] training 39.4%: Loss=0.0176894, MSE=0.0176894, MAE=0.102149\n",
      "[30/30] training 57.6%: Loss=0.0190848, MSE=0.0190848, MAE=0.10688\n",
      "[30/30] training 75.8%: Loss=0.019439, MSE=0.019439, MAE=0.108911\n",
      "[30/30] training 97.0%: Loss=0.0195391, MSE=0.0195391, MAE=0.108823\n",
      "-----------------------------------Finished Epoch 30/30: Loss=117.03, RMSE=1.50538, MAE=1.78973, r_2=-1.18011, p=0.508856\n",
      "Saving final model to saved_models/model_fold7_final.pth\n",
      "******************************** FOLD 8 ******************************\n",
      "******************************** FOLD 8 ******************************\n",
      "Loaded 521 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0414744, MSE=0.0414744, MAE=0.161902\n",
      "[1/30] training 39.4%: Loss=0.0412004, MSE=0.0412004, MAE=0.162122\n",
      "[1/30] training 57.6%: Loss=0.0409623, MSE=0.0409623, MAE=0.160233\n",
      "[1/30] training 75.8%: Loss=0.0438118, MSE=0.0438118, MAE=0.167158\n",
      "[1/30] training 97.0%: Loss=0.0422188, MSE=0.0422188, MAE=0.164069\n",
      "-----------------------------------Finished Epoch 1/30: Loss=109.255, RMSE=1.67353, MAE=2.12659, r_2=-37.653, p=0.0440472\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0403953, MSE=0.0403953, MAE=0.162893\n",
      "[2/30] training 39.4%: Loss=0.0377805, MSE=0.0377805, MAE=0.155508\n",
      "[2/30] training 57.6%: Loss=0.0379798, MSE=0.0379798, MAE=0.155161\n",
      "[2/30] training 75.8%: Loss=0.0388061, MSE=0.0388061, MAE=0.156699\n",
      "[2/30] training 97.0%: Loss=0.0395236, MSE=0.0395236, MAE=0.159009\n",
      "-----------------------------------Finished Epoch 2/30: Loss=109.188, RMSE=1.62103, MAE=2.09368, r_2=-16.1141, p=0.165777\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0393934, MSE=0.0393934, MAE=0.154524\n",
      "[3/30] training 39.4%: Loss=0.038897, MSE=0.038897, MAE=0.154135\n",
      "[3/30] training 57.6%: Loss=0.0377659, MSE=0.0377659, MAE=0.154346\n",
      "[3/30] training 75.8%: Loss=0.0391983, MSE=0.0391983, MAE=0.156344\n",
      "[3/30] training 97.0%: Loss=0.0378367, MSE=0.0378367, MAE=0.153761\n",
      "-----------------------------------Finished Epoch 3/30: Loss=108.947, RMSE=1.59604, MAE=2.07835, r_2=-5.59388, p=0.259681\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.0395701, MSE=0.0395701, MAE=0.156161\n",
      "[4/30] training 39.4%: Loss=0.033935, MSE=0.033935, MAE=0.14504\n",
      "[4/30] training 57.6%: Loss=0.0328737, MSE=0.0328737, MAE=0.144491\n",
      "[4/30] training 75.8%: Loss=0.0353646, MSE=0.0353646, MAE=0.148504\n",
      "[4/30] training 97.0%: Loss=0.0353167, MSE=0.0353167, MAE=0.148117\n",
      "-----------------------------------Finished Epoch 4/30: Loss=109.21, RMSE=1.56871, MAE=2.03406, r_2=-3.84263, p=0.302353\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0389004, MSE=0.0389004, MAE=0.160471\n",
      "[5/30] training 39.4%: Loss=0.0373743, MSE=0.0373743, MAE=0.155051\n",
      "[5/30] training 57.6%: Loss=0.0340201, MSE=0.0340201, MAE=0.146383\n",
      "[5/30] training 75.8%: Loss=0.0338627, MSE=0.0338627, MAE=0.147737\n",
      "[5/30] training 97.0%: Loss=0.034447, MSE=0.034447, MAE=0.148219\n",
      "-----------------------------------Finished Epoch 5/30: Loss=108.948, RMSE=1.56781, MAE=2.04454, r_2=-4.24632, p=0.307856\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0290907, MSE=0.0290907, MAE=0.141389\n",
      "[6/30] training 39.4%: Loss=0.0315112, MSE=0.0315112, MAE=0.143335\n",
      "[6/30] training 57.6%: Loss=0.0328255, MSE=0.0328255, MAE=0.143772\n",
      "[6/30] training 75.8%: Loss=0.0338978, MSE=0.0338978, MAE=0.148855\n",
      "[6/30] training 97.0%: Loss=0.0331143, MSE=0.0331143, MAE=0.146004\n",
      "-----------------------------------Finished Epoch 6/30: Loss=109.035, RMSE=1.5453, MAE=2.03559, r_2=-4.25348, p=0.308565\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.0370241, MSE=0.0370241, MAE=0.148075\n",
      "[7/30] training 39.4%: Loss=0.031906, MSE=0.031906, MAE=0.13841\n",
      "[7/30] training 57.6%: Loss=0.0347649, MSE=0.0347649, MAE=0.146069\n",
      "[7/30] training 75.8%: Loss=0.0343189, MSE=0.0343189, MAE=0.145573\n",
      "[7/30] training 97.0%: Loss=0.0324134, MSE=0.0324134, MAE=0.141926\n",
      "-----------------------------------Finished Epoch 7/30: Loss=109.015, RMSE=1.52689, MAE=2.00827, r_2=-3.56693, p=0.343727\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.034437, MSE=0.034437, MAE=0.142469\n",
      "[8/30] training 39.4%: Loss=0.0342456, MSE=0.0342456, MAE=0.144605\n",
      "[8/30] training 57.6%: Loss=0.0329446, MSE=0.0329446, MAE=0.143661\n",
      "[8/30] training 75.8%: Loss=0.0321589, MSE=0.0321589, MAE=0.141218\n",
      "[8/30] training 97.0%: Loss=0.0314104, MSE=0.0314104, MAE=0.140235\n",
      "-----------------------------------Finished Epoch 8/30: Loss=108.938, RMSE=1.50441, MAE=1.98905, r_2=-3.63728, p=0.368394\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0230827, MSE=0.0230827, MAE=0.115057\n",
      "[9/30] training 39.4%: Loss=0.0236424, MSE=0.0236424, MAE=0.121482\n",
      "[9/30] training 57.6%: Loss=0.0267074, MSE=0.0267074, MAE=0.127761\n",
      "[9/30] training 75.8%: Loss=0.0289578, MSE=0.0289578, MAE=0.135088\n",
      "[9/30] training 97.0%: Loss=0.0304469, MSE=0.0304469, MAE=0.139295\n",
      "-----------------------------------Finished Epoch 9/30: Loss=109.008, RMSE=1.49895, MAE=1.98072, r_2=-3.98515, p=0.36822\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0275414, MSE=0.0275414, MAE=0.128508\n",
      "[10/30] training 39.4%: Loss=0.030891, MSE=0.030891, MAE=0.139038\n",
      "[10/30] training 57.6%: Loss=0.0303324, MSE=0.0303324, MAE=0.138566\n",
      "[10/30] training 75.8%: Loss=0.0298993, MSE=0.0298993, MAE=0.137234\n",
      "[10/30] training 97.0%: Loss=0.0286331, MSE=0.0286331, MAE=0.134185\n",
      "-----------------------------------Finished Epoch 10/30: Loss=108.919, RMSE=1.5, MAE=1.9876, r_2=-3.55756, p=0.372139\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0280897, MSE=0.0280897, MAE=0.131564\n",
      "[11/30] training 39.4%: Loss=0.0304618, MSE=0.0304618, MAE=0.139165\n",
      "[11/30] training 57.6%: Loss=0.0278568, MSE=0.0278568, MAE=0.132419\n",
      "[11/30] training 75.8%: Loss=0.0294279, MSE=0.0294279, MAE=0.137324\n",
      "[11/30] training 97.0%: Loss=0.0279232, MSE=0.0279232, MAE=0.132798\n",
      "-----------------------------------Finished Epoch 11/30: Loss=108.889, RMSE=1.48767, MAE=1.97498, r_2=-3.15411, p=0.389458\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0242463, MSE=0.0242463, MAE=0.122942\n",
      "[12/30] training 39.4%: Loss=0.025421, MSE=0.025421, MAE=0.124528\n",
      "[12/30] training 57.6%: Loss=0.0284133, MSE=0.0284133, MAE=0.134393\n",
      "[12/30] training 75.8%: Loss=0.0282368, MSE=0.0282368, MAE=0.133956\n",
      "[12/30] training 97.0%: Loss=0.0285028, MSE=0.0285028, MAE=0.134021\n",
      "-----------------------------------Finished Epoch 12/30: Loss=108.689, RMSE=1.50801, MAE=1.99481, r_2=-3.60317, p=0.391787\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0317521, MSE=0.0317521, MAE=0.144956\n",
      "[13/30] training 39.4%: Loss=0.0284914, MSE=0.0284914, MAE=0.13501\n",
      "[13/30] training 57.6%: Loss=0.0269901, MSE=0.0269901, MAE=0.132175\n",
      "[13/30] training 75.8%: Loss=0.0261013, MSE=0.0261013, MAE=0.130081\n",
      "[13/30] training 97.0%: Loss=0.0262431, MSE=0.0262431, MAE=0.130035\n",
      "-----------------------------------Finished Epoch 13/30: Loss=108.45, RMSE=1.54459, MAE=2.02599, r_2=-3.22952, p=0.399746\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0242835, MSE=0.0242835, MAE=0.124104\n",
      "[14/30] training 39.4%: Loss=0.0266795, MSE=0.0266795, MAE=0.129374\n",
      "[14/30] training 57.6%: Loss=0.0264522, MSE=0.0264522, MAE=0.129746\n",
      "[14/30] training 75.8%: Loss=0.0257592, MSE=0.0257592, MAE=0.12957\n",
      "[14/30] training 97.0%: Loss=0.0246814, MSE=0.0246814, MAE=0.126323\n",
      "-----------------------------------Finished Epoch 14/30: Loss=108.643, RMSE=1.51361, MAE=2.0082, r_2=-3.25051, p=0.387186\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.023957, MSE=0.023957, MAE=0.124496\n",
      "[15/30] training 39.4%: Loss=0.0259052, MSE=0.0259052, MAE=0.130101\n",
      "[15/30] training 57.6%: Loss=0.0256874, MSE=0.0256874, MAE=0.129613\n",
      "[15/30] training 75.8%: Loss=0.0263348, MSE=0.0263348, MAE=0.129611\n",
      "[15/30] training 97.0%: Loss=0.0257413, MSE=0.0257413, MAE=0.128126\n",
      "-----------------------------------Finished Epoch 15/30: Loss=108.544, RMSE=1.51525, MAE=2.00121, r_2=-3.39767, p=0.407147\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0259103, MSE=0.0259103, MAE=0.126085\n",
      "[16/30] training 39.4%: Loss=0.0260583, MSE=0.0260583, MAE=0.12761\n",
      "[16/30] training 57.6%: Loss=0.0281927, MSE=0.0281927, MAE=0.132781\n",
      "[16/30] training 75.8%: Loss=0.0255941, MSE=0.0255941, MAE=0.125099\n",
      "[16/30] training 97.0%: Loss=0.0252947, MSE=0.0252947, MAE=0.125116\n",
      "-----------------------------------Finished Epoch 16/30: Loss=108.732, RMSE=1.49498, MAE=1.99249, r_2=-2.86179, p=0.393038\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.021218, MSE=0.021218, MAE=0.113353\n",
      "[17/30] training 39.4%: Loss=0.0220579, MSE=0.0220579, MAE=0.116959\n",
      "[17/30] training 57.6%: Loss=0.0235869, MSE=0.0235869, MAE=0.122181\n",
      "[17/30] training 75.8%: Loss=0.0246761, MSE=0.0246761, MAE=0.124902\n",
      "[17/30] training 97.0%: Loss=0.0244743, MSE=0.0244743, MAE=0.123063\n",
      "-----------------------------------Finished Epoch 17/30: Loss=108.825, RMSE=1.46908, MAE=1.95991, r_2=-3.14773, p=0.409716\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0214222, MSE=0.0214222, MAE=0.113857\n",
      "[18/30] training 39.4%: Loss=0.0246379, MSE=0.0246379, MAE=0.121326\n",
      "[18/30] training 57.6%: Loss=0.022846, MSE=0.022846, MAE=0.116221\n",
      "[18/30] training 75.8%: Loss=0.024093, MSE=0.024093, MAE=0.121401\n",
      "[18/30] training 97.0%: Loss=0.0246541, MSE=0.0246541, MAE=0.123435\n",
      "-----------------------------------Finished Epoch 18/30: Loss=108.56, RMSE=1.51244, MAE=2.00327, r_2=-3.03515, p=0.404407\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0200446, MSE=0.0200446, MAE=0.109755\n",
      "[19/30] training 39.4%: Loss=0.0232262, MSE=0.0232262, MAE=0.121259\n",
      "[19/30] training 57.6%: Loss=0.0222175, MSE=0.0222175, MAE=0.1192\n",
      "[19/30] training 75.8%: Loss=0.0236686, MSE=0.0236686, MAE=0.12203\n",
      "[19/30] training 97.0%: Loss=0.0233638, MSE=0.0233638, MAE=0.120596\n",
      "-----------------------------------Finished Epoch 19/30: Loss=108.629, RMSE=1.50412, MAE=1.98632, r_2=-2.96966, p=0.410336\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.0213423, MSE=0.0213423, MAE=0.115669\n",
      "[20/30] training 39.4%: Loss=0.0229023, MSE=0.0229023, MAE=0.11929\n",
      "[20/30] training 57.6%: Loss=0.024086, MSE=0.024086, MAE=0.122827\n",
      "[20/30] training 75.8%: Loss=0.0230559, MSE=0.0230559, MAE=0.119165\n",
      "[20/30] training 97.0%: Loss=0.0231964, MSE=0.0231964, MAE=0.12063\n",
      "-----------------------------------Finished Epoch 20/30: Loss=108.641, RMSE=1.50487, MAE=2.00612, r_2=-2.53299, p=0.395995\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0224812, MSE=0.0224812, MAE=0.125283\n",
      "[21/30] training 39.4%: Loss=0.0240637, MSE=0.0240637, MAE=0.126202\n",
      "[21/30] training 57.6%: Loss=0.0221944, MSE=0.0221944, MAE=0.120409\n",
      "[21/30] training 75.8%: Loss=0.0221043, MSE=0.0221043, MAE=0.119085\n",
      "[21/30] training 97.0%: Loss=0.0229308, MSE=0.0229308, MAE=0.120995\n",
      "-----------------------------------Finished Epoch 21/30: Loss=108.798, RMSE=1.50443, MAE=1.99121, r_2=-2.4224, p=0.392289\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.0304435, MSE=0.0304435, MAE=0.140036\n",
      "[22/30] training 39.4%: Loss=0.0242324, MSE=0.0242324, MAE=0.123299\n",
      "[22/30] training 57.6%: Loss=0.0235878, MSE=0.0235878, MAE=0.121289\n",
      "[22/30] training 75.8%: Loss=0.0235005, MSE=0.0235005, MAE=0.121097\n",
      "[22/30] training 97.0%: Loss=0.0227755, MSE=0.0227755, MAE=0.117967\n",
      "-----------------------------------Finished Epoch 22/30: Loss=108.702, RMSE=1.51057, MAE=1.99654, r_2=-2.60997, p=0.395702\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0235214, MSE=0.0235214, MAE=0.119415\n",
      "[23/30] training 39.4%: Loss=0.0217059, MSE=0.0217059, MAE=0.115948\n",
      "[23/30] training 57.6%: Loss=0.020109, MSE=0.020109, MAE=0.112374\n",
      "[23/30] training 75.8%: Loss=0.0198717, MSE=0.0198717, MAE=0.111909\n",
      "[23/30] training 97.0%: Loss=0.0216115, MSE=0.0216115, MAE=0.116502\n",
      "-----------------------------------Finished Epoch 23/30: Loss=108.613, RMSE=1.53488, MAE=2.02735, r_2=-2.87117, p=0.377867\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.0273544, MSE=0.0273544, MAE=0.131363\n",
      "[24/30] training 39.4%: Loss=0.0253489, MSE=0.0253489, MAE=0.128997\n",
      "[24/30] training 57.6%: Loss=0.0238182, MSE=0.0238182, MAE=0.122803\n",
      "[24/30] training 75.8%: Loss=0.023478, MSE=0.023478, MAE=0.121752\n",
      "[24/30] training 97.0%: Loss=0.0221056, MSE=0.0221056, MAE=0.117735\n",
      "-----------------------------------Finished Epoch 24/30: Loss=108.735, RMSE=1.53125, MAE=2.02017, r_2=-2.62282, p=0.372488\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0218332, MSE=0.0218332, MAE=0.116023\n",
      "[25/30] training 39.4%: Loss=0.0197744, MSE=0.0197744, MAE=0.111373\n",
      "[25/30] training 57.6%: Loss=0.0197768, MSE=0.0197768, MAE=0.110567\n",
      "[25/30] training 75.8%: Loss=0.0208887, MSE=0.0208887, MAE=0.1134\n",
      "[25/30] training 97.0%: Loss=0.0203242, MSE=0.0203242, MAE=0.112883\n",
      "-----------------------------------Finished Epoch 25/30: Loss=108.77, RMSE=1.54516, MAE=2.0164, r_2=-2.62259, p=0.371793\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0224102, MSE=0.0224102, MAE=0.120746\n",
      "[26/30] training 39.4%: Loss=0.0212251, MSE=0.0212251, MAE=0.113584\n",
      "[26/30] training 57.6%: Loss=0.0217356, MSE=0.0217356, MAE=0.114055\n",
      "[26/30] training 75.8%: Loss=0.0211916, MSE=0.0211916, MAE=0.112886\n",
      "[26/30] training 97.0%: Loss=0.0209548, MSE=0.0209548, MAE=0.112681\n",
      "-----------------------------------Finished Epoch 26/30: Loss=108.411, RMSE=1.58447, MAE=2.06843, r_2=-2.38253, p=0.380121\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0202316, MSE=0.0202316, MAE=0.113982\n",
      "[27/30] training 39.4%: Loss=0.0236718, MSE=0.0236718, MAE=0.122598\n",
      "[27/30] training 57.6%: Loss=0.0221528, MSE=0.0221528, MAE=0.118416\n",
      "[27/30] training 75.8%: Loss=0.0208374, MSE=0.0208374, MAE=0.114651\n",
      "[27/30] training 97.0%: Loss=0.0211473, MSE=0.0211473, MAE=0.115176\n",
      "-----------------------------------Finished Epoch 27/30: Loss=108.571, RMSE=1.57009, MAE=2.05137, r_2=-2.18013, p=0.375883\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0231483, MSE=0.0231483, MAE=0.120112\n",
      "[28/30] training 39.4%: Loss=0.021989, MSE=0.021989, MAE=0.119227\n",
      "[28/30] training 57.6%: Loss=0.0205405, MSE=0.0205405, MAE=0.115561\n",
      "[28/30] training 75.8%: Loss=0.0212562, MSE=0.0212562, MAE=0.115941\n",
      "[28/30] training 97.0%: Loss=0.020075, MSE=0.020075, MAE=0.112521\n",
      "-----------------------------------Finished Epoch 28/30: Loss=108.446, RMSE=1.56834, MAE=2.06072, r_2=-2.29829, p=0.38253\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0210432, MSE=0.0210432, MAE=0.11439\n",
      "[29/30] training 39.4%: Loss=0.0207467, MSE=0.0207467, MAE=0.113018\n",
      "[29/30] training 57.6%: Loss=0.0214412, MSE=0.0214412, MAE=0.113218\n",
      "[29/30] training 75.8%: Loss=0.0199342, MSE=0.0199342, MAE=0.108308\n",
      "[29/30] training 97.0%: Loss=0.0193796, MSE=0.0193796, MAE=0.107194\n",
      "-----------------------------------Finished Epoch 29/30: Loss=108.575, RMSE=1.5621, MAE=2.04838, r_2=-2.27471, p=0.375488\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.0167805, MSE=0.0167805, MAE=0.100684\n",
      "[30/30] training 39.4%: Loss=0.0185962, MSE=0.0185962, MAE=0.104205\n",
      "[30/30] training 57.6%: Loss=0.0199733, MSE=0.0199733, MAE=0.110057\n",
      "[30/30] training 75.8%: Loss=0.0194197, MSE=0.0194197, MAE=0.109328\n",
      "[30/30] training 97.0%: Loss=0.0194935, MSE=0.0194935, MAE=0.109503\n",
      "-----------------------------------Finished Epoch 30/30: Loss=108.347, RMSE=1.59086, MAE=2.08417, r_2=-2.31967, p=0.378776\n",
      "Saving final model to saved_models/model_fold8_final.pth\n",
      "******************************** FOLD 9 ******************************\n",
      "******************************** FOLD 9 ******************************\n",
      "Loaded 521 training pairs\n",
      "Embedded successfully...\n",
      "lr: 1e-05\n",
      "[1/30] training 21.2%: Loss=0.0387769, MSE=0.0387769, MAE=0.152929\n",
      "[1/30] training 39.4%: Loss=0.0393107, MSE=0.0393107, MAE=0.154406\n",
      "[1/30] training 57.6%: Loss=0.0384431, MSE=0.0384431, MAE=0.152506\n",
      "[1/30] training 75.8%: Loss=0.0404304, MSE=0.0404304, MAE=0.158557\n",
      "[1/30] training 97.0%: Loss=0.0390662, MSE=0.0390662, MAE=0.156233\n",
      "-----------------------------------Finished Epoch 1/30: Loss=100.546, RMSE=1.58669, MAE=2.03869, r_2=-27.1364, p=0.114184\n",
      "lr: 1e-05\n",
      "[2/30] training 21.2%: Loss=0.0425965, MSE=0.0425965, MAE=0.156719\n",
      "[2/30] training 39.4%: Loss=0.0377933, MSE=0.0377933, MAE=0.149535\n",
      "[2/30] training 57.6%: Loss=0.0367766, MSE=0.0367766, MAE=0.149221\n",
      "[2/30] training 75.8%: Loss=0.0364741, MSE=0.0364741, MAE=0.149472\n",
      "[2/30] training 97.0%: Loss=0.036449, MSE=0.036449, MAE=0.150655\n",
      "-----------------------------------Finished Epoch 2/30: Loss=100.375, RMSE=1.56629, MAE=2.0525, r_2=-9.49377, p=0.194346\n",
      "lr: 1e-05\n",
      "[3/30] training 21.2%: Loss=0.0335818, MSE=0.0335818, MAE=0.145912\n",
      "[3/30] training 39.4%: Loss=0.0330846, MSE=0.0330846, MAE=0.144211\n",
      "[3/30] training 57.6%: Loss=0.0347652, MSE=0.0347652, MAE=0.147868\n",
      "[3/30] training 75.8%: Loss=0.0357264, MSE=0.0357264, MAE=0.149125\n",
      "[3/30] training 97.0%: Loss=0.0348635, MSE=0.0348635, MAE=0.147231\n",
      "-----------------------------------Finished Epoch 3/30: Loss=100.423, RMSE=1.51565, MAE=2.01115, r_2=-6.88253, p=0.25273\n",
      "lr: 1e-05\n",
      "[4/30] training 21.2%: Loss=0.03318, MSE=0.03318, MAE=0.145968\n",
      "[4/30] training 39.4%: Loss=0.0308318, MSE=0.0308318, MAE=0.140383\n",
      "[4/30] training 57.6%: Loss=0.0298097, MSE=0.0298097, MAE=0.138367\n",
      "[4/30] training 75.8%: Loss=0.0325977, MSE=0.0325977, MAE=0.143272\n",
      "[4/30] training 97.0%: Loss=0.032137, MSE=0.032137, MAE=0.14216\n",
      "-----------------------------------Finished Epoch 4/30: Loss=100.257, RMSE=1.52135, MAE=2.03672, r_2=-6.44684, p=0.267802\n",
      "lr: 1e-05\n",
      "[5/30] training 21.2%: Loss=0.0316296, MSE=0.0316296, MAE=0.144698\n",
      "[5/30] training 39.4%: Loss=0.0298247, MSE=0.0298247, MAE=0.138307\n",
      "[5/30] training 57.6%: Loss=0.0329739, MSE=0.0329739, MAE=0.144909\n",
      "[5/30] training 75.8%: Loss=0.0328963, MSE=0.0328963, MAE=0.143345\n",
      "[5/30] training 97.0%: Loss=0.0317356, MSE=0.0317356, MAE=0.14062\n",
      "-----------------------------------Finished Epoch 5/30: Loss=100.27, RMSE=1.52002, MAE=2.02257, r_2=-6.16333, p=0.283359\n",
      "lr: 1e-05\n",
      "[6/30] training 21.2%: Loss=0.0279516, MSE=0.0279516, MAE=0.129397\n",
      "[6/30] training 39.4%: Loss=0.0323336, MSE=0.0323336, MAE=0.140936\n",
      "[6/30] training 57.6%: Loss=0.0312302, MSE=0.0312302, MAE=0.139479\n",
      "[6/30] training 75.8%: Loss=0.0291826, MSE=0.0291826, MAE=0.134954\n",
      "[6/30] training 97.0%: Loss=0.0294057, MSE=0.0294057, MAE=0.135306\n",
      "-----------------------------------Finished Epoch 6/30: Loss=100.095, RMSE=1.58718, MAE=2.0596, r_2=-7.08578, p=0.28486\n",
      "lr: 1e-05\n",
      "[7/30] training 21.2%: Loss=0.029696, MSE=0.029696, MAE=0.136734\n",
      "[7/30] training 39.4%: Loss=0.0285465, MSE=0.0285465, MAE=0.134585\n",
      "[7/30] training 57.6%: Loss=0.030993, MSE=0.030993, MAE=0.141123\n",
      "[7/30] training 75.8%: Loss=0.0310726, MSE=0.0310726, MAE=0.139308\n",
      "[7/30] training 97.0%: Loss=0.0302144, MSE=0.0302144, MAE=0.136092\n",
      "-----------------------------------Finished Epoch 7/30: Loss=100.18, RMSE=1.56622, MAE=2.04535, r_2=-5.87466, p=0.282772\n",
      "lr: 1e-05\n",
      "[8/30] training 21.2%: Loss=0.0375718, MSE=0.0375718, MAE=0.159466\n",
      "[8/30] training 39.4%: Loss=0.0341083, MSE=0.0341083, MAE=0.147831\n",
      "[8/30] training 57.6%: Loss=0.0309572, MSE=0.0309572, MAE=0.139575\n",
      "[8/30] training 75.8%: Loss=0.0328003, MSE=0.0328003, MAE=0.144041\n",
      "[8/30] training 97.0%: Loss=0.0307413, MSE=0.0307413, MAE=0.13977\n",
      "-----------------------------------Finished Epoch 8/30: Loss=100.009, RMSE=1.61637, MAE=2.09334, r_2=-6.21217, p=0.274694\n",
      "lr: 1e-05\n",
      "[9/30] training 21.2%: Loss=0.0231974, MSE=0.0231974, MAE=0.119699\n",
      "[9/30] training 39.4%: Loss=0.0259497, MSE=0.0259497, MAE=0.126408\n",
      "[9/30] training 57.6%: Loss=0.0260469, MSE=0.0260469, MAE=0.126629\n",
      "[9/30] training 75.8%: Loss=0.02756, MSE=0.02756, MAE=0.131377\n",
      "[9/30] training 97.0%: Loss=0.028294, MSE=0.028294, MAE=0.132733\n",
      "-----------------------------------Finished Epoch 9/30: Loss=100.072, RMSE=1.59445, MAE=2.0697, r_2=-5.84221, p=0.285748\n",
      "lr: 1e-05\n",
      "[10/30] training 21.2%: Loss=0.0263514, MSE=0.0263514, MAE=0.129355\n",
      "[10/30] training 39.4%: Loss=0.027733, MSE=0.027733, MAE=0.131661\n",
      "[10/30] training 57.6%: Loss=0.0290258, MSE=0.0290258, MAE=0.135537\n",
      "[10/30] training 75.8%: Loss=0.0271003, MSE=0.0271003, MAE=0.130211\n",
      "[10/30] training 97.0%: Loss=0.0271021, MSE=0.0271021, MAE=0.129974\n",
      "-----------------------------------Finished Epoch 10/30: Loss=100.029, RMSE=1.61577, MAE=2.08887, r_2=-5.17164, p=0.281946\n",
      "lr: 1.0000000000000002e-06\n",
      "[11/30] training 21.2%: Loss=0.0224114, MSE=0.0224114, MAE=0.114492\n",
      "[11/30] training 39.4%: Loss=0.0255067, MSE=0.0255067, MAE=0.12777\n",
      "[11/30] training 57.6%: Loss=0.0236156, MSE=0.0236156, MAE=0.122265\n",
      "[11/30] training 75.8%: Loss=0.0248819, MSE=0.0248819, MAE=0.125337\n",
      "[11/30] training 97.0%: Loss=0.0258514, MSE=0.0258514, MAE=0.127491\n",
      "-----------------------------------Finished Epoch 11/30: Loss=99.9611, RMSE=1.63644, MAE=2.10901, r_2=-5.09378, p=0.280559\n",
      "lr: 1.0000000000000002e-06\n",
      "[12/30] training 21.2%: Loss=0.0222968, MSE=0.0222968, MAE=0.116193\n",
      "[12/30] training 39.4%: Loss=0.0242926, MSE=0.0242926, MAE=0.1215\n",
      "[12/30] training 57.6%: Loss=0.0258044, MSE=0.0258044, MAE=0.127352\n",
      "[12/30] training 75.8%: Loss=0.0257079, MSE=0.0257079, MAE=0.1257\n",
      "[12/30] training 97.0%: Loss=0.0267634, MSE=0.0267634, MAE=0.1285\n",
      "-----------------------------------Finished Epoch 12/30: Loss=100.083, RMSE=1.60161, MAE=2.07438, r_2=-4.82672, p=0.285766\n",
      "lr: 1.0000000000000002e-06\n",
      "[13/30] training 21.2%: Loss=0.0263793, MSE=0.0263793, MAE=0.128855\n",
      "[13/30] training 39.4%: Loss=0.0248162, MSE=0.0248162, MAE=0.125976\n",
      "[13/30] training 57.6%: Loss=0.0265762, MSE=0.0265762, MAE=0.128407\n",
      "[13/30] training 75.8%: Loss=0.0268871, MSE=0.0268871, MAE=0.129542\n",
      "[13/30] training 97.0%: Loss=0.0259457, MSE=0.0259457, MAE=0.127641\n",
      "-----------------------------------Finished Epoch 13/30: Loss=99.9432, RMSE=1.62614, MAE=2.10538, r_2=-5.42224, p=0.28711\n",
      "lr: 1.0000000000000002e-06\n",
      "[14/30] training 21.2%: Loss=0.0229138, MSE=0.0229138, MAE=0.120394\n",
      "[14/30] training 39.4%: Loss=0.0227813, MSE=0.0227813, MAE=0.11899\n",
      "[14/30] training 57.6%: Loss=0.023412, MSE=0.023412, MAE=0.121428\n",
      "[14/30] training 75.8%: Loss=0.0261634, MSE=0.0261634, MAE=0.129167\n",
      "[14/30] training 97.0%: Loss=0.0258763, MSE=0.0258763, MAE=0.12762\n",
      "-----------------------------------Finished Epoch 14/30: Loss=100.124, RMSE=1.59719, MAE=2.06854, r_2=-4.82395, p=0.280863\n",
      "lr: 1.0000000000000002e-06\n",
      "[15/30] training 21.2%: Loss=0.028486, MSE=0.028486, MAE=0.135721\n",
      "[15/30] training 39.4%: Loss=0.0273565, MSE=0.0273565, MAE=0.133127\n",
      "[15/30] training 57.6%: Loss=0.0271285, MSE=0.0271285, MAE=0.132106\n",
      "[15/30] training 75.8%: Loss=0.0274235, MSE=0.0274235, MAE=0.132718\n",
      "[15/30] training 97.0%: Loss=0.0260432, MSE=0.0260432, MAE=0.127996\n",
      "-----------------------------------Finished Epoch 15/30: Loss=99.8302, RMSE=1.6813, MAE=2.16421, r_2=-4.84733, p=0.264234\n",
      "lr: 1.0000000000000002e-06\n",
      "[16/30] training 21.2%: Loss=0.0259176, MSE=0.0259176, MAE=0.126303\n",
      "[16/30] training 39.4%: Loss=0.0238675, MSE=0.0238675, MAE=0.122036\n",
      "[16/30] training 57.6%: Loss=0.0242101, MSE=0.0242101, MAE=0.12255\n",
      "[16/30] training 75.8%: Loss=0.025406, MSE=0.025406, MAE=0.125695\n",
      "[16/30] training 97.0%: Loss=0.024938, MSE=0.024938, MAE=0.124658\n",
      "-----------------------------------Finished Epoch 16/30: Loss=99.7779, RMSE=1.68421, MAE=2.16747, r_2=-4.39271, p=0.283211\n",
      "lr: 1.0000000000000002e-06\n",
      "[17/30] training 21.2%: Loss=0.0267343, MSE=0.0267343, MAE=0.129123\n",
      "[17/30] training 39.4%: Loss=0.0258263, MSE=0.0258263, MAE=0.129626\n",
      "[17/30] training 57.6%: Loss=0.0237349, MSE=0.0237349, MAE=0.123402\n",
      "[17/30] training 75.8%: Loss=0.023763, MSE=0.023763, MAE=0.121941\n",
      "[17/30] training 97.0%: Loss=0.0241554, MSE=0.0241554, MAE=0.122567\n",
      "-----------------------------------Finished Epoch 17/30: Loss=99.854, RMSE=1.66846, MAE=2.15516, r_2=-4.57594, p=0.270077\n",
      "lr: 1.0000000000000002e-06\n",
      "[18/30] training 21.2%: Loss=0.0245415, MSE=0.0245415, MAE=0.123635\n",
      "[18/30] training 39.4%: Loss=0.0232402, MSE=0.0232402, MAE=0.120045\n",
      "[18/30] training 57.6%: Loss=0.0228596, MSE=0.0228596, MAE=0.118579\n",
      "[18/30] training 75.8%: Loss=0.0228801, MSE=0.0228801, MAE=0.118859\n",
      "[18/30] training 97.0%: Loss=0.0240123, MSE=0.0240123, MAE=0.121131\n",
      "-----------------------------------Finished Epoch 18/30: Loss=99.8535, RMSE=1.64613, MAE=2.13546, r_2=-4.68885, p=0.289184\n",
      "lr: 1.0000000000000002e-06\n",
      "[19/30] training 21.2%: Loss=0.0231784, MSE=0.0231784, MAE=0.117519\n",
      "[19/30] training 39.4%: Loss=0.0240443, MSE=0.0240443, MAE=0.123095\n",
      "[19/30] training 57.6%: Loss=0.0223713, MSE=0.0223713, MAE=0.118566\n",
      "[19/30] training 75.8%: Loss=0.0222508, MSE=0.0222508, MAE=0.117972\n",
      "[19/30] training 97.0%: Loss=0.0229715, MSE=0.0229715, MAE=0.119807\n",
      "-----------------------------------Finished Epoch 19/30: Loss=99.8103, RMSE=1.66999, MAE=2.16591, r_2=-4.35807, p=0.275496\n",
      "lr: 1.0000000000000002e-06\n",
      "[20/30] training 21.2%: Loss=0.016263, MSE=0.016263, MAE=0.0958259\n",
      "[20/30] training 39.4%: Loss=0.0184841, MSE=0.0184841, MAE=0.105141\n",
      "[20/30] training 57.6%: Loss=0.020041, MSE=0.020041, MAE=0.110827\n",
      "[20/30] training 75.8%: Loss=0.0204269, MSE=0.0204269, MAE=0.112703\n",
      "[20/30] training 97.0%: Loss=0.0228668, MSE=0.0228668, MAE=0.118297\n",
      "-----------------------------------Finished Epoch 20/30: Loss=99.8607, RMSE=1.68539, MAE=2.1879, r_2=-4.30007, p=0.23996\n",
      "lr: 1.0000000000000002e-06\n",
      "[21/30] training 21.2%: Loss=0.0240458, MSE=0.0240458, MAE=0.124498\n",
      "[21/30] training 39.4%: Loss=0.0226898, MSE=0.0226898, MAE=0.118725\n",
      "[21/30] training 57.6%: Loss=0.0225415, MSE=0.0225415, MAE=0.117748\n",
      "[21/30] training 75.8%: Loss=0.0225867, MSE=0.0225867, MAE=0.11821\n",
      "[21/30] training 97.0%: Loss=0.0228762, MSE=0.0228762, MAE=0.119047\n",
      "-----------------------------------Finished Epoch 21/30: Loss=99.8278, RMSE=1.6823, MAE=2.19111, r_2=-4.18632, p=0.248579\n",
      "lr: 1.0000000000000002e-06\n",
      "[22/30] training 21.2%: Loss=0.027008, MSE=0.027008, MAE=0.124677\n",
      "[22/30] training 39.4%: Loss=0.0234668, MSE=0.0234668, MAE=0.117191\n",
      "[22/30] training 57.6%: Loss=0.0240061, MSE=0.0240061, MAE=0.122183\n",
      "[22/30] training 75.8%: Loss=0.022771, MSE=0.022771, MAE=0.119555\n",
      "[22/30] training 97.0%: Loss=0.022467, MSE=0.022467, MAE=0.118509\n",
      "-----------------------------------Finished Epoch 22/30: Loss=100.1, RMSE=1.61305, MAE=2.11239, r_2=-3.73277, p=0.260154\n",
      "lr: 1.0000000000000002e-06\n",
      "[23/30] training 21.2%: Loss=0.0206957, MSE=0.0206957, MAE=0.117989\n",
      "[23/30] training 39.4%: Loss=0.0231481, MSE=0.0231481, MAE=0.122685\n",
      "[23/30] training 57.6%: Loss=0.0230688, MSE=0.0230688, MAE=0.122036\n",
      "[23/30] training 75.8%: Loss=0.0221191, MSE=0.0221191, MAE=0.11928\n",
      "[23/30] training 97.0%: Loss=0.0221926, MSE=0.0221926, MAE=0.119141\n",
      "-----------------------------------Finished Epoch 23/30: Loss=99.8315, RMSE=1.67629, MAE=2.17807, r_2=-3.89559, p=0.265642\n",
      "lr: 1.0000000000000002e-06\n",
      "[24/30] training 21.2%: Loss=0.019836, MSE=0.019836, MAE=0.112217\n",
      "[24/30] training 39.4%: Loss=0.0200791, MSE=0.0200791, MAE=0.112009\n",
      "[24/30] training 57.6%: Loss=0.0212337, MSE=0.0212337, MAE=0.115062\n",
      "[24/30] training 75.8%: Loss=0.0209499, MSE=0.0209499, MAE=0.114697\n",
      "[24/30] training 97.0%: Loss=0.0214854, MSE=0.0214854, MAE=0.116117\n",
      "-----------------------------------Finished Epoch 24/30: Loss=100.092, RMSE=1.60551, MAE=2.109, r_2=-3.3129, p=0.274505\n",
      "lr: 1.0000000000000002e-06\n",
      "[25/30] training 21.2%: Loss=0.0206732, MSE=0.0206732, MAE=0.110567\n",
      "[25/30] training 39.4%: Loss=0.0218882, MSE=0.0218882, MAE=0.112503\n",
      "[25/30] training 57.6%: Loss=0.0201447, MSE=0.0201447, MAE=0.109038\n",
      "[25/30] training 75.8%: Loss=0.0212696, MSE=0.0212696, MAE=0.114071\n",
      "[25/30] training 97.0%: Loss=0.0221093, MSE=0.0221093, MAE=0.116522\n",
      "-----------------------------------Finished Epoch 25/30: Loss=100.074, RMSE=1.63995, MAE=2.13682, r_2=-3.46603, p=0.250197\n",
      "lr: 1.0000000000000002e-06\n",
      "[26/30] training 21.2%: Loss=0.0216818, MSE=0.0216818, MAE=0.113264\n",
      "[26/30] training 39.4%: Loss=0.0202152, MSE=0.0202152, MAE=0.110447\n",
      "[26/30] training 57.6%: Loss=0.0212697, MSE=0.0212697, MAE=0.114043\n",
      "[26/30] training 75.8%: Loss=0.0228599, MSE=0.0228599, MAE=0.119797\n",
      "[26/30] training 97.0%: Loss=0.0208092, MSE=0.0208092, MAE=0.113782\n",
      "-----------------------------------Finished Epoch 26/30: Loss=99.6805, RMSE=1.74266, MAE=2.25652, r_2=-3.89619, p=0.236589\n",
      "lr: 1.0000000000000002e-06\n",
      "[27/30] training 21.2%: Loss=0.0229627, MSE=0.0229627, MAE=0.122964\n",
      "[27/30] training 39.4%: Loss=0.0201467, MSE=0.0201467, MAE=0.115534\n",
      "[27/30] training 57.6%: Loss=0.0203763, MSE=0.0203763, MAE=0.115471\n",
      "[27/30] training 75.8%: Loss=0.0214285, MSE=0.0214285, MAE=0.116935\n",
      "[27/30] training 97.0%: Loss=0.021338, MSE=0.021338, MAE=0.114792\n",
      "-----------------------------------Finished Epoch 27/30: Loss=99.5984, RMSE=1.76598, MAE=2.28, r_2=-3.43903, p=0.251715\n",
      "lr: 1.0000000000000002e-06\n",
      "[28/30] training 21.2%: Loss=0.0216453, MSE=0.0216453, MAE=0.117144\n",
      "[28/30] training 39.4%: Loss=0.0187984, MSE=0.0187984, MAE=0.108496\n",
      "[28/30] training 57.6%: Loss=0.0193526, MSE=0.0193526, MAE=0.109535\n",
      "[28/30] training 75.8%: Loss=0.0206354, MSE=0.0206354, MAE=0.114201\n",
      "[28/30] training 97.0%: Loss=0.019677, MSE=0.019677, MAE=0.110048\n",
      "-----------------------------------Finished Epoch 28/30: Loss=99.7763, RMSE=1.70508, MAE=2.22824, r_2=-3.41123, p=0.2473\n",
      "lr: 1.0000000000000002e-06\n",
      "[29/30] training 21.2%: Loss=0.0169802, MSE=0.0169802, MAE=0.103256\n",
      "[29/30] training 39.4%: Loss=0.0192458, MSE=0.0192458, MAE=0.10829\n",
      "[29/30] training 57.6%: Loss=0.0182788, MSE=0.0182788, MAE=0.104771\n",
      "[29/30] training 75.8%: Loss=0.0183685, MSE=0.0183685, MAE=0.105891\n",
      "[29/30] training 97.0%: Loss=0.0192218, MSE=0.0192218, MAE=0.108365\n",
      "-----------------------------------Finished Epoch 29/30: Loss=99.6464, RMSE=1.75547, MAE=2.28793, r_2=-3.16476, p=0.240071\n",
      "lr: 1.0000000000000002e-06\n",
      "[30/30] training 21.2%: Loss=0.016839, MSE=0.016839, MAE=0.106031\n",
      "[30/30] training 39.4%: Loss=0.0176507, MSE=0.0176507, MAE=0.108082\n",
      "[30/30] training 57.6%: Loss=0.016554, MSE=0.016554, MAE=0.103338\n",
      "[30/30] training 75.8%: Loss=0.0195045, MSE=0.0195045, MAE=0.11077\n",
      "[30/30] training 97.0%: Loss=0.0197669, MSE=0.0197669, MAE=0.110499\n",
      "-----------------------------------Finished Epoch 30/30: Loss=99.7809, RMSE=1.69545, MAE=2.21724, r_2=-3.40995, p=0.255585\n",
      "Saving final model to saved_models/model_fold9_final.pth\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "from __future__ import annotations\n",
    "import time\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import IterableDataset, dataloader\n",
    "from multiprocessing.reduction import ForkingPickler\n",
    "from sklearn.metrics import average_precision_score as average_precision\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, NamedTuple, Optional\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.optim import Optimizer\n",
    "from src.models.mvsf import ModelAffinity\n",
    "from src.utils import *\n",
    "from torch.optim.lr_scheduler import LambdaLR, ReduceLROnPlateau\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torcheval.metrics.functional import r2_score\n",
    "\n",
    "# 解决使用multiprocessing模块时由于Tensor对象内部实现机制导致的序列化错误\n",
    "default_collate_func = dataloader.default_collate\n",
    "def default_collate_override(batch):\n",
    "    dataloader._use_shared_memory = False\n",
    "    return default_collate_func(batch)\n",
    "setattr(dataloader, 'default_collate', default_collate_override)\n",
    "for t in torch._storage_classes:\n",
    "    if sys.version_info[0] == 2:\n",
    "        if t in ForkingPickler.dispatch:\n",
    "            del ForkingPickler.dispatch[t]\n",
    "    else:\n",
    "        if t in ForkingPickler._extra_reducers:\n",
    "            del ForkingPickler._extra_reducers[t]\n",
    "\n",
    "# 替换TrainArguments和add_args为简单的类\n",
    "class Args:\n",
    "    # 默认参数值\n",
    "    device = 0  # 使用CUDA设备编号，-1表示使用CPU\n",
    "    train = \"datasets/pairs_sabdab.csv\"  # 训练数据路径\n",
    "    test = \"datasets/pairs_benchmark.csv\"  # 测试数据路径\n",
    "    seq_path = \"datasets/seq_natural.fasta\"  # 序列路径\n",
    "    feature_path = \"datasets/seq_natural_embedding.csv\"  # 特征路径\n",
    "    no_augment = True  # 不增强数据\n",
    "    augment_weight = 0.5  # 增强数据权重\n",
    "    weight_module1 = 1.0  # 模块1权重\n",
    "    weight_module2 = 1.0  # 模块2权重\n",
    "    num_epochs = 30  # 训练轮数\n",
    "    batch_size = 16  # 批大小\n",
    "    weight_decay = 0.00001  # L2正则化系数\n",
    "    lr = 0.00001  # 学习率\n",
    "    kfolds = 10  # 交叉验证折数\n",
    "    cross_validate = True  # 是否使用交叉验证\n",
    "    outfile = \"output.log\"  # 输出文件路径\n",
    "    save_prefix = \"saved_models/model\"  # 保存模型的路径前缀\n",
    "    checkpoint = None  # 加载检查点模型\n",
    "    seed = 42  # 随机种子\n",
    "\n",
    "# 创建参数实例\n",
    "args = Args()\n",
    "\n",
    "# 确保保存路径的目录存在\n",
    "os.makedirs(os.path.dirname(args.save_prefix), exist_ok=True)\n",
    "\n",
    "# 日志输出函数\n",
    "def log(message, file=None, print_also=True):\n",
    "    if print_also:\n",
    "        print(message)\n",
    "    if file and file != sys.stdout:\n",
    "        file.write(message + \"\\n\")\n",
    "        file.flush()\n",
    "\n",
    "# 预测亲和力函数\n",
    "def predict_affinity(model, Lchain, Hchain, antigen, embedding_tensor, aaindex_feature, use_cuda):\n",
    "    b = len(Hchain)\n",
    "    lchain_embeddings = []\n",
    "    hchain_embeddings = []\n",
    "    ag_embeddings = []\n",
    "\n",
    "    lchain_aaindex = []\n",
    "    hchain_aaindex = []\n",
    "    ag_aaindex = []\n",
    "\n",
    "    for i in range(b):\n",
    "        lchain_embedding = embedding_tensor[Lchain[i]]\n",
    "        hchain_embedding = embedding_tensor[Hchain[i]]\n",
    "        ag_embedding = embedding_tensor[antigen[i]]\n",
    "\n",
    "        lchain_aaindex.append(aaindex_feature[Lchain[i]])\n",
    "        hchain_aaindex.append(aaindex_feature[Hchain[i]])\n",
    "        ag_aaindex.append(aaindex_feature[antigen[i]])\n",
    "\n",
    "        lchain_embeddings.append(lchain_embedding)\n",
    "        hchain_embeddings.append(hchain_embedding)\n",
    "        ag_embeddings.append(ag_embedding)\n",
    "\n",
    "    if use_cuda:\n",
    "        lchain_embeddings = torch.stack(lchain_embeddings, 0).cuda()\n",
    "        hchain_embeddings = torch.stack(hchain_embeddings, 0).cuda()\n",
    "        ag_embeddings = torch.stack(ag_embeddings, 0).cuda()\n",
    "\n",
    "        lchain_aaindex = torch.stack(lchain_aaindex, 0).cuda()\n",
    "        hchain_aaindex = torch.stack(hchain_aaindex, 0).cuda()\n",
    "        ag_aaindex = torch.stack(ag_aaindex, 0).cuda()\n",
    "\n",
    "    ph = model.predict(lchain_aaindex, hchain_aaindex, ag_aaindex, lchain_embeddings, hchain_embeddings, ag_embeddings)\n",
    "    return ph\n",
    "\n",
    "# 模型评估函数\n",
    "def model_eval(model, test_iterator, embedding_tensors, aaindex_feature, write, weight1, weight2, use_cuda):\n",
    "    p_hat = []\n",
    "    true_y = []\n",
    "\n",
    "    for lchain, hchain, antigen, y in test_iterator:\n",
    "        ph = predict_affinity(model, lchain, hchain, antigen, embedding_tensors, aaindex_feature, use_cuda)\n",
    "        p_hat.append(ph)\n",
    "        true_y.append(y)\n",
    "\n",
    "    y = torch.cat(true_y, 0)\n",
    "\n",
    "    p_hat = torch.cat(p_hat, 0)\n",
    "    if use_cuda:\n",
    "        y.cuda()\n",
    "        p_hat = torch.Tensor([x.cuda() for x in p_hat])\n",
    "        p_hat.cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(p_hat.float(), y.float())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        p_hat = p_hat.float()\n",
    "        y = y.float()\n",
    "        max_val = 16.9138\n",
    "        min_val = 5.0400\n",
    "        p_hat = (p_hat * (max_val - min_val)) + min_val\n",
    "\n",
    "        # if write:\n",
    "        #     with open('pred_skempi.csv', 'a') as f:\n",
    "        #         for i in range(len(y)):\n",
    "        #             f.write(str(y[i].item()) + ',' + str(p_hat[i].item()) + '\\n')\n",
    "\n",
    "        rmse = torch.sqrt(torch.mean((y - p_hat) ** 2)).item()\n",
    "        mae = torch.mean(torch.abs(y - p_hat)).item()\n",
    "        r_2 = r2_score(y, p_hat).item()\n",
    "        p = pearsonr(y, p_hat).item()\n",
    "\n",
    "    return loss, rmse, mae, r_2, p\n",
    "\n",
    "# 训练模型函数\n",
    "def train_model(args, output):\n",
    "    # 创建数据集\n",
    "    batch_size = args.batch_size\n",
    "    use_cuda = (args.device > -1) and torch.cuda.is_available()\n",
    "    train_fi = args.train\n",
    "    train_df = pd.read_csv(train_fi)\n",
    "    test_fi = args.test\n",
    "    test_df = pd.read_csv(test_fi)\n",
    "\n",
    "    # 训练模型参数\n",
    "    lr = args.lr\n",
    "    num_epochs = args.num_epochs\n",
    "    batch_size = args.batch_size\n",
    "    digits = int(np.floor(np.log10(num_epochs))) + 1\n",
    "    save_prefix = args.save_prefix\n",
    "    weight1 = args.weight_module1\n",
    "    weight2 = args.weight_module2\n",
    "\n",
    "    # 打印参数信息\n",
    "    log(f'Using save prefix \"{save_prefix}\"', file=output)\n",
    "    log(f\"Training with SAM: lr={lr}\", file=output)\n",
    "    log(f\"\\tnum_epochs: {num_epochs}\", file=output)\n",
    "    log(f\"\\tbatch_size: {batch_size}\", file=output)\n",
    "    log(f\"\\tmodule 1 weight: {weight1}\", file=output)\n",
    "    log(f\"\\tmodule 2 weight: {weight2}\", file=output)\n",
    "    if output != sys.stdout:\n",
    "        output.flush()\n",
    "\n",
    "    if(args.cross_validate):\n",
    "    # ===============================================cross validation=================================================\n",
    "        k_folds = args.kfolds\n",
    "        kfold = KFold(n_splits=k_folds, shuffle=False)\n",
    "        for fold, (train_ids, test_ids) in enumerate(kfold.split(train_df)):\n",
    "            print(f'******************************** FOLD {fold} ******************************')\n",
    "            log(f'******************************** FOLD {fold} ******************************', file=output)\n",
    "            train_df_fold = train_df.iloc[train_ids]\n",
    "            test_df_fold = train_df.iloc[test_ids]\n",
    "            train_df_fold = train_df_fold.reset_index(drop=True)\n",
    "            test_df_fold = test_df_fold.reset_index(drop=True)\n",
    "\n",
    "            train_df_fold.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "            train_l_fold = train_df_fold[\"light\"]\n",
    "            train_h_fold = train_df_fold[\"heavy\"]\n",
    "            train_ag_fold = train_df_fold[\"antigen\"]\n",
    "            train_y_fold = torch.from_numpy(train_df_fold[\"delta_g\"].values)\n",
    "            train_y_fold = -train_y_fold\n",
    "\n",
    "            max_val = 16.05654\n",
    "            min_val = 5.0400\n",
    "            train_y_fold = (train_y_fold - min_val) / (max_val - min_val)\n",
    "\n",
    "            test_df_fold.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "            test_l_fold = test_df_fold[\"light\"]\n",
    "            test_h_fold = test_df_fold[\"heavy\"]\n",
    "            test_ag_fold = test_df_fold[\"antigen\"]\n",
    "            test_y_fold = torch.from_numpy(test_df_fold[\"delta_g\"].values)\n",
    "            test_y_fold = -test_y_fold\n",
    "\n",
    "            train_dataset_fold = PairedDataset(train_l_fold, train_h_fold, train_ag_fold, train_y_fold)\n",
    "            train_iterator_fold = torch.utils.data.DataLoader(\n",
    "                train_dataset_fold,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_paired_sequences,\n",
    "                shuffle=True,\n",
    "                pin_memory=False,\n",
    "                drop_last=False,\n",
    "                # num_workers=2,\n",
    "            )\n",
    "            log(f\"Loaded {len(train_l_fold)} training pairs\", file=output)\n",
    "            if output != sys.stdout:\n",
    "                output.flush()\n",
    "\n",
    "            test_dataset_fold = PairedDataset(test_l_fold, test_h_fold, test_ag_fold, test_y_fold)\n",
    "            test_iterator_fold = torch.utils.data.DataLoader(\n",
    "                test_dataset_fold,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=collate_paired_sequences,\n",
    "                shuffle=False,\n",
    "                pin_memory=False,\n",
    "                drop_last=False,\n",
    "                # num_workers=2,\n",
    "            )\n",
    "\n",
    "            all_proteins = set(train_l_fold).union(train_h_fold).union(train_ag_fold) \\\n",
    "                .union(test_l_fold).union(test_h_fold).union(test_ag_fold)\n",
    "            fastaPath = args.seq_path\n",
    "            embeddingPath = args.feature_path\n",
    "            embeddings = embed_dict(fastaPath, embeddingPath)\n",
    "            log(\"Embedded successfully...\", file=output)\n",
    "            aaindex_feature = seq_aaindex_dict(all_proteins, fastaPath)\n",
    "\n",
    "            model = ModelAffinity(batch_size, use_cuda)\n",
    "            model.use_cuda = use_cuda  # default is False\n",
    "            if use_cuda:\n",
    "                model.cuda()\n",
    "            params = [p for p in model.parameters() if p.requires_grad]\n",
    "            base_optimizer = optim.SGD\n",
    "            optimizer = SAM(params, base_optimizer, lr=lr, weight_decay=args.weight_decay)\n",
    "\n",
    "            batch_report_fmt = (\"[{}/{}] training {:.1%}: Loss={:.6}, MSE={:.6}, MAE={:.6}\")\n",
    "            epoch_report_fmt = (\n",
    "                \"-----------------------------------Finished Epoch {}/{}: Loss={:.6}, RMSE={:.6}, MAE={:.6}, r_2={:.6}, p={:.6}\")\n",
    "\n",
    "            N = len(train_iterator_fold) * batch_size\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                if epoch == 10:\n",
    "                    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / 10\n",
    "                print(\"lr:\", optimizer.param_groups[0]['lr'])\n",
    "                model.train()\n",
    "                n = 0\n",
    "                loss_accum = 0\n",
    "                # acc_accum = 0\n",
    "                mse_accum = 0\n",
    "                mae_accum = 0\n",
    "                optimizer.zero_grad()\n",
    "                all_y = []\n",
    "                all_p_hat = []\n",
    "                for (lchain, hchain, antigen, y) in train_iterator_fold:\n",
    "\n",
    "                    phat = predict_affinity(\n",
    "                        model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda=use_cuda)\n",
    "                    phat = phat.float().view(-1)\n",
    "\n",
    "                    if use_cuda:\n",
    "                        y = y.cuda()\n",
    "                    # y = Variable(y)\n",
    "                    y = y.float()\n",
    "\n",
    "                    criterion = nn.MSELoss()\n",
    "                    b = len(y)\n",
    "                    loss = criterion(phat, y)\n",
    "                    loss.requires_grad_(True)\n",
    "                    loss.backward()\n",
    "                    # scaler.scale(loss).backward()\n",
    "                    if use_cuda:\n",
    "                        y = y.cpu()\n",
    "                        phat = phat.cpu()\n",
    "                    all_y.append(y)\n",
    "                    all_p_hat.append(phat)\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        phat = phat.float()\n",
    "                        y = y.float()\n",
    "                        mse = torch.mean((y - phat) ** 2).item()\n",
    "                        mae = torch.mean(torch.abs(y - phat)).item()\n",
    "                    n += b\n",
    "                    delta = b * (loss.item() - loss_accum)\n",
    "                    loss_accum += delta / n\n",
    "                    delta = b * (mse - mse_accum)\n",
    "                    mse_accum += delta / n\n",
    "                    delta = b * (mae - mae_accum)\n",
    "                    mae_accum += delta / n\n",
    "                    report = (n - b) // 100 < n // 100\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if report:\n",
    "                        tokens = [epoch + 1, num_epochs, n / N, loss_accum, mse_accum, mae_accum, ]\n",
    "                        log(batch_report_fmt.format(*tokens), file=output)\n",
    "                        if output != sys.stdout:\n",
    "                            output.flush()\n",
    "\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    if epoch+1 == 30:\n",
    "                        write = True\n",
    "                    else:\n",
    "                        write = False\n",
    "                    (inter_loss, inter_rmse, inter_mae, inter_r_2, inter_p,) = model_eval(\n",
    "                        model, test_iterator_fold, embeddings, aaindex_feature, write, weight1, weight2, use_cuda=use_cuda)\n",
    "\n",
    "                    tokens = [epoch + 1, num_epochs, inter_loss, inter_mae, inter_rmse, inter_r_2, inter_p, ]\n",
    "\n",
    "                    # scheduler.step(inter_mse)\n",
    "                    log(epoch_report_fmt.format(*tokens), file=output)\n",
    "                    if output != sys.stdout:\n",
    "                        output.flush()\n",
    "                    '''\n",
    "                    # 保存模型 (每个epoch)\n",
    "                    if save_prefix is not None:\n",
    "                        save_path = (save_prefix + f\"_fold{fold}_epoch\" + str(epoch + 1).zfill(digits) + \".pth\")\n",
    "                        log(f\"Saving model to {save_path}\", file=output)\n",
    "                        model.cpu()\n",
    "                        torch.save(model.state_dict(), save_path)\n",
    "                        if use_cuda:\n",
    "                            model.cuda()\n",
    "                    '''\n",
    "            # 保存最终模型\n",
    "            if save_prefix is not None:\n",
    "                save_path = save_prefix + f\"_fold{fold}_final.pth\"\n",
    "                log(f\"Saving final model to {save_path}\", file=output)\n",
    "                model.cpu()\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                if use_cuda:\n",
    "                    model.cuda()\n",
    "    else:\n",
    "        num_samples = len(train_df)\n",
    "        train_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "        train_l = train_df[\"light\"]\n",
    "        train_h = train_df[\"heavy\"]\n",
    "        train_ag = train_df[\"antigen\"]\n",
    "        train_y = torch.from_numpy(train_df[\"delta_g\"].values)\n",
    "        train_y = -train_y\n",
    "        train_y = NormalizeData(train_y)\n",
    "\n",
    "        test_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "        test_l = test_df[\"light\"]\n",
    "        test_h = test_df[\"heavy\"]\n",
    "        test_ag = test_df[\"antigen\"]\n",
    "        test_y = torch.from_numpy(test_df[\"delta_g\"].values)\n",
    "        test_y = -test_y\n",
    "\n",
    "        train_dataset = PairedDataset(train_l, train_h, train_ag, train_y)\n",
    "        train_iterator = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_paired_sequences,\n",
    "            shuffle=True,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "            # num_workers=4,\n",
    "        )\n",
    "        log(f\"Loaded {len(train_l)} training pairs\", file=output)\n",
    "        if output != sys.stdout:\n",
    "            output.flush()\n",
    "\n",
    "        test_dataset = PairedDataset(test_l, test_h, test_ag, test_y)\n",
    "        test_iterator = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            collate_fn=collate_paired_sequences,\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "            # num_workers=4,\n",
    "        )\n",
    "\n",
    "        log(f\"Loaded {len(test_l)} test pairs\", file=output)\n",
    "        log(\"Loading embeddings...\", file=output)\n",
    "        if output != sys.stdout:\n",
    "            output.flush()\n",
    "\n",
    "        all_proteins = set(train_l).union(train_h).union(train_ag).union(test_l).union(test_h).union(test_ag)\n",
    "\n",
    "        fastaPath = args.seq_path\n",
    "        embeddingPath = args.feature_path\n",
    "        embeddings = embed_dict(fastaPath, embeddingPath)\n",
    "        log(\"embeded successfully...\", file=output)\n",
    "        aaindex_feature = seq_aaindex_dict(all_proteins, fastaPath)\n",
    "\n",
    "        model = ModelAffinity(batch_size, use_cuda)\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        base_optimizer = optim.Adam\n",
    "        optimizer = SAM(params, base_optimizer, lr=lr)\n",
    "        log(f'Using save prefix \"{save_prefix}\"', file=output)\n",
    "        log(f\"Training with SAM: lr={lr}\", file=output)\n",
    "        log(f\"\\tnum_epochs: {num_epochs}\", file=output)\n",
    "        log(f\"\\tbatch_size: {batch_size}\", file=output)\n",
    "        log(f\"\\tmodule 1 weight: {weight1}\", file=output)\n",
    "        log(f\"\\tmodule 2 weight: {weight2}\", file=output)\n",
    "        if output != sys.stdout:\n",
    "            output.flush()\n",
    "\n",
    "        batch_report_fmt = (\"[{}/{}] training {:.1%}: Loss={:.6}, MSE={:.6}, MAE={:.6}\")\n",
    "        epoch_report_fmt = (\n",
    "            \"-----------------------------------Finished Epoch {}/{}: Loss={:.6}, RMSE={:.6}, MAE={:.6}, r_2={:.6}, p={:.6}\")\n",
    "\n",
    "        N = len(train_iterator) * batch_size\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            model.train()\n",
    "            n = 0\n",
    "            loss_accum = 0\n",
    "            mse_accum = 0\n",
    "            mae_accum = 0\n",
    "            all_y = []\n",
    "            all_p_hat = []\n",
    "            optimizer.zero_grad()\n",
    "            for (lchain, hchain, antigen, y) in train_iterator:\n",
    "                phat = predict_affinity(\n",
    "                    model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda=use_cuda)\n",
    "                phat = phat.float().view(-1)\n",
    "\n",
    "                if use_cuda:\n",
    "                    y = y.cuda()\n",
    "                # y = Variable(y)\n",
    "                y = y.float()\n",
    "\n",
    "                criterion = nn.MSELoss()\n",
    "                b = len(y)\n",
    "                loss = criterion(phat, y)\n",
    "                loss.requires_grad_(True)\n",
    "                loss.backward()\n",
    "                # scaler.scale(loss).backward()\n",
    "                if use_cuda:\n",
    "                    y = y.cpu()\n",
    "                    phat = phat.cpu()\n",
    "\n",
    "                all_y.append(y)\n",
    "                all_p_hat.append(phat)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    phat = phat.float()\n",
    "                    y = y.float()\n",
    "                    mse = torch.mean((y - phat) ** 2).item()\n",
    "                    mae = torch.mean(torch.abs(y - phat)).item()\n",
    "                n += b\n",
    "                delta = b * (loss.item() - loss_accum)\n",
    "                loss_accum += delta / n\n",
    "                delta = b * (mse - mse_accum)\n",
    "                mse_accum += delta / n\n",
    "                delta = b * (mae - mae_accum)\n",
    "                mae_accum += delta / n\n",
    "                report = (n - b) // 100 < n // 100\n",
    "\n",
    "                optimizer.step()\n",
    "                if report:\n",
    "                    tokens = [epoch + 1, num_epochs, n / N, loss_accum, mse_accum, mae_accum, ]\n",
    "                    log(batch_report_fmt.format(*tokens), file=output)\n",
    "                    if output != sys.stdout:\n",
    "                        output.flush()\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "\n",
    "                (inter_loss, inter_rmse, inter_mae, inter_r_2, inter_p,) = model_eval(\n",
    "                    model, test_iterator, embeddings, aaindex_feature, False, weight1, weight2, use_cuda=use_cuda)\n",
    "                tokens = [epoch + 1, num_epochs, inter_loss, inter_mae, inter_rmse, inter_r_2, inter_p, ]\n",
    "                # scheduler.step(inter_mse)\n",
    "                log(epoch_report_fmt.format(*tokens), file=output)\n",
    "                if output != sys.stdout:\n",
    "                    output.flush()\n",
    "                '''\n",
    "                # 保存模型 (每个epoch)\n",
    "                if save_prefix is not None:\n",
    "                    save_path = (save_prefix + \"_epoch\" + str(epoch + 1).zfill(digits) + \".pth\")\n",
    "                    log(f\"Saving model to {save_path}\", file=output)\n",
    "                    model.cpu()\n",
    "                    torch.save(model.state_dict(), save_path)\n",
    "                    if use_cuda:\n",
    "                        model.cuda()\n",
    "                '''\n",
    "        # 保存最终模型\n",
    "        if save_prefix is not None:\n",
    "            save_path = save_prefix + \"_final.pth\"\n",
    "            log(f\"Saving final model to {save_path}\", file=output)\n",
    "            model.cpu()\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            if use_cuda:\n",
    "                model.cuda()\n",
    "\n",
    "# 初始化输出\n",
    "output = args.outfile\n",
    "if output is None:\n",
    "    output = sys.stdout\n",
    "else:\n",
    "    output = open(output, \"w\")\n",
    "\n",
    "# 打印开始信息\n",
    "log(\"Starting training with parameters:\", file=output, print_also=True)\n",
    "for key, value in vars(args).items():\n",
    "    log(f\"  {key}: {value}\", file=output, print_also=True)\n",
    "\n",
    "# 设置设备\n",
    "device = args.device\n",
    "use_cuda = (device > -1) and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(device)\n",
    "    log(\n",
    "        f\"Using CUDA device {device} - {torch.cuda.get_device_name(device)}\",\n",
    "        file=output,\n",
    "        print_also=True,\n",
    "    )\n",
    "else:\n",
    "    log(\"Using CPU\", file=output, print_also=True)\n",
    "    device = \"cpu\"\n",
    "\n",
    "# 设置随机种子\n",
    "if args.seed is not None:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "# 开始训练\n",
    "train_model(args, output)\n",
    "\n",
    "# 关闭输出文件\n",
    "if output != sys.stdout:\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d66fa5e-325c-4557-aafe-53b6267f99f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功读取TSV文件：/root/private_data/ckx/affinity_contract/MVSF-AB/my_datasets/final_dataset_train_no_du.tsv\n",
      "文件包含 2424 条记录和 5 列\n",
      "列名: ['pdb_id', 'heavy_sequence', 'light_sequence', 'antigen_sequence', 'delta_g']\n",
      "已创建 datasets/pairs_benchmark.csv，包含 2424 条记录\n",
      "已创建 datasets/seq_natural.fasta，包含 3654 个序列\n",
      "\n",
      "===== pairs_benchmark.csv 内容预览 =====\n",
      "        light       heavy         antigen  delta_g\n",
      "0  1AHW_light  1AHW_heavy  1AHW_antigen_1   -10.90\n",
      "1  1AHW_light  1AHW_heavy  1AHW_antigen_2    -7.53\n",
      "2  1AHW_light  1AHW_heavy  1AHW_antigen_3   -10.46\n",
      "3  1AHW_light  1AHW_heavy  1AHW_antigen_4   -11.86\n",
      "4  1AHW_light  1AHW_heavy  1AHW_antigen_5   -10.20\n",
      "\n",
      "===== seq_natural.fasta 内容预览 =====\n",
      ">1AHW_heavy\n",
      "DIKMTQSPSSMYASLGERVTITCKASQDIRKYLNWYQQKPWKSPKTLIYYATSLADGVPSRFSGSGSGQDYSLTISSLESDDTATYYCLQHGESPYTFGGGTKLEINRADAAPTVSIFPPSSEQLTSGGASVVCFLNNFYPKDINVKWKIDGSERQNGVLNSWTDQDSKDSTYSMSSTLTLTKDEYERHNSYTCEATHKTSTSPIVKSFNRNEC\n",
      ">1AHW_light\n",
      "EIQLQQSGAELVRPGALVKLSCKASGFNIKDYYMHWVKQRPEQGLEWIGLIDPENGNTIYDPKFQGKASITADTSSNTAYLQLSSLTSEDTAVYYCARDNSYYFDYWGQGTTLTVSSAKTTPPSVYPLAPGSAAQTNSMVTLGCLVKGYFPEPVTVTWNSGSLSSGVHTFPAVLQSDLYTLSSSVTVPSSTWPSETVTCNVAHPASSTKVDKKI\n",
      ">1AHW_antigen_1\n",
      "TNTVAAYNLTWKSTNFKTILEWEPKPVNQVYTVQISTKSGDWKSKCFYTTDTECDLTDEIVKDVKQTYLARVFSYPAGNEPLYENSPEFTPYLETNLGQPTIQSFEQVGAAVNVTVEDERTLVRRNNTFLSLRDVFGKDLIYTLYYWKSSSSGKKTAKTNTNEFLIDVDKGENYCFSVQAVIPSRTVNRKSTDSPVECMG\n",
      ">1AHW_antigen_2\n",
      "TNTVAAYNLTWKSTNFKTILEWEPKPVNQVYTVQISTKSGDWKSKCFYTTDTECDLTDEIVKDVKQTYLARVFSYPAGNEPLYENSPEFTPYLETNLGQPTIQSFEQVGTKVNVTVEDERTLVRRNNTFLSLRDVFGKDLIYTLYYWKSSSSGKKTAKTNTNEFLIDVDKGENYCFSVQAVIPSRTVNAASTDSPVECMG\n",
      ">1AHW_antigen_3\n",
      "TNTVAAYNLTWKSTNFKTILEWEPKPVNQVYTVQISTKSGDWKSKCFYTTDTECDLTDEIVKDVKQTYLARVFSYPAGNEPLYENSPEFTPYLETNLGQPTIQSFEQVGTKVNVTVEDERTLVRRNNTFLSLRDVFGKDLIYTLYYWKSSSSGKKTAKTNTNEFLIDVDKGENYCFSVQAVIPSRTVARKSTDSPVECMG\n",
      ">1AHW_antigen_4\n",
      "TNTVAAYNLTWKSTNFKTILEWEPKPVNQVYTVQISTKSGDWKSKCFYTTDTECDLTDEIVKDVKQTYLARVFSYPAGNEPLYENSPEFTPYLETNLGQPTIQSFEQVGTKVNVTVEDERTLVRRNNTFLSLRDVFGKDLIYTLYYWKSSSSGKKTAKTNTNEFLIDVDKGENYCFSVQAVIPSRTANRKSTDSPVECMG\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 确保目录存在\n",
    "os.makedirs('datasets', exist_ok=True)\n",
    "\n",
    "# 读取TSV文件\n",
    "file_path = \"/root/private_data/ckx/affinity_contract/MVSF-AB/my_datasets/final_dataset_train_no_du.tsv\"\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "print(f\"成功读取TSV文件：{file_path}\")\n",
    "print(f\"文件包含 {len(df)} 条记录和 {len(df.columns)} 列\")\n",
    "print(\"列名:\", df.columns.tolist())\n",
    "\n",
    "# 创建一个字典来存储所有唯一的序列\n",
    "sequences = {}\n",
    "pair_data = []\n",
    "\n",
    "# 处理每个数据点\n",
    "for i, row in df.iterrows():\n",
    "    pdb_id = row['pdb_id']\n",
    "    variant_id = i+1  # 为每个变体分配一个唯一ID\n",
    "    \n",
    "    # 创建ID\n",
    "    heavy_id = f\"{pdb_id}_heavy\"\n",
    "    light_id = f\"{pdb_id}_light\"\n",
    "    antigen_id = f\"{pdb_id}_antigen_{variant_id}\"\n",
    "    \n",
    "    # 存储序列\n",
    "    sequences[heavy_id] = row['heavy_sequence']\n",
    "    sequences[light_id] = row['light_sequence']\n",
    "    sequences[antigen_id] = row['antigen_sequence']\n",
    "    \n",
    "    # 存储配对信息\n",
    "    pair_data.append({\n",
    "        'light': light_id,\n",
    "        'heavy': heavy_id,\n",
    "        'antigen': antigen_id,\n",
    "        'delta_g': float(row['delta_g'])\n",
    "    })\n",
    "\n",
    "# 创建CSV文件\n",
    "pair_df = pd.DataFrame(pair_data)\n",
    "pair_df.to_csv('my_datasets/pairs_benchmark.csv', index=False)\n",
    "print(f\"已创建 datasets/pairs_benchmark.csv，包含 {len(pair_data)} 条记录\")\n",
    "\n",
    "# 创建FASTA文件\n",
    "with open('my_datasets/seq_natural.fasta', 'w') as f:\n",
    "    for seq_id, sequence in sequences.items():\n",
    "        f.write(f\">{seq_id}\\n{sequence}\\n\")\n",
    "print(f\"已创建 datasets/seq_natural.fasta，包含 {len(sequences)} 个序列\")\n",
    "\n",
    "# 显示文件的前几行内容\n",
    "print(\"\\n===== pairs_benchmark.csv 内容预览 =====\")\n",
    "print(pair_df.head().to_string())\n",
    "\n",
    "print(\"\\n===== seq_natural.fasta 内容预览 =====\")\n",
    "with open('datasets/seq_natural.fasta', 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        print(line.strip())\n",
    "        if i > 10:  # 只打印前几行\n",
    "            print(\"...\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c46c353e-34fe-4324-a2f8-f99fb7725c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 567/567 [00:00<00:00, 1417264.82B/s]\n",
      "100%|██████████| 370264230/370264230 [01:25<00:00, 4311005.28B/s] \n",
      "/opt/conda/lib/python3.10/site-packages/tape/models/modeling_utils.py:523: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(resolved_archive_file, map_location='cpu')\n",
      "  0%|          | 0/3654 [00:00<?, ?it/s]/tmp/ipykernel_15301/2645238943.py:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  token_ids = torch.tensor([tokenizer.encode(seq)])\n",
      "100%|██████████| 3654/3654 [00:23<00:00, 157.70it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "from protein_bert_pytorch import ProteinBERT, PretrainingWrapper\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from tape import TAPETokenizer,ProteinBertModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "def get_feature(_list):\n",
    "    # load model\n",
    "    model = ProteinBertModel.from_pretrained('bert-base')\n",
    "    torch.save(model, 'pretrain_bert.models')\n",
    "    device = torch.device('cuda')\n",
    "    # model = torch.load('../cmap_final/src/models/pretrain_bert.models')\n",
    "    # model = ProteinBertModel.from_pretrained('./bert-base-chinese')\n",
    "    model = model.to(device)\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.eval()\n",
    "    tokenizer = TAPETokenizer(vocab='iupac')  # iupac是TAPE模型的词汇表，UniRep模型使用unirep。\n",
    "    feature = []\n",
    "    for seq in tqdm(_list):      # 进度条\n",
    "        token_ids = torch.tensor([tokenizer.encode(seq)])\n",
    "        output = model(token_ids.to(device))\n",
    "        pooled_output = output[1]\n",
    "        feature.append(pooled_output[0].tolist())\n",
    "    _df = pd.DataFrame(np.array(feature))\n",
    "    return _df\n",
    "\n",
    "def get_feature2():\n",
    "    model = ProteinBERT(\n",
    "        num_tokens=21,\n",
    "        num_annotation=8943,\n",
    "        dim=512,\n",
    "        dim_global=256,\n",
    "        depth=6,\n",
    "        narrow_conv_kernel=9,\n",
    "        wide_conv_kernel=9,\n",
    "        wide_conv_dilation=5,\n",
    "        attn_heads=8,\n",
    "        attn_dim_head=64\n",
    "    )\n",
    "\n",
    "    seq = torch.randint(0, 21, (2, 2048))\n",
    "    mask = torch.ones(2, 2048).bool()\n",
    "    annotation = torch.randint(0, 1, (2, 8943)).float()\n",
    "\n",
    "    seq_logits, annotation_logits = model(seq, annotation, mask=mask)\n",
    "\n",
    "# 修改的parse函数 - 适用于Jupyter Notebook\n",
    "def parse_fasta(fasta_file):\n",
    "    \"\"\"\n",
    "    解析FASTA文件，提取序列名称和序列\n",
    "    \n",
    "    参数:\n",
    "        fasta_file: FASTA文件路径\n",
    "        \n",
    "    返回:\n",
    "        names: 序列名称列表\n",
    "        sequences: 序列列表\n",
    "    \"\"\"\n",
    "    names = []\n",
    "    sequences = []\n",
    "    \n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        names.append(record.name)\n",
    "        sequences.append(str(record.seq))\n",
    "    \n",
    "    return names, sequences\n",
    "\n",
    "# 主程序代码\n",
    "fastaPath = './my_datasets/seq_natural.fasta'\n",
    "outputPath = './my_datasets/seq_natural_embedding.csv'\n",
    "\n",
    "# 使用修改后的函数解析FASTA文件\n",
    "names, sequence = parse_fasta(fastaPath)\n",
    "\n",
    "# 处理序列\n",
    "new_sequence = []\n",
    "for seq in sequence:\n",
    "    seq = seq.replace('_', '')\n",
    "    seq = seq.replace('J', '')\n",
    "    new_sequence.append(seq)\n",
    "\n",
    "# 将序列名和处理后的序列写入CSV\n",
    "rows = zip(names, new_sequence)\n",
    "with open(outputPath, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "# 获取特征向量\n",
    "df = get_feature(new_sequence)\n",
    "\n",
    "# 保存特征向量\n",
    "df.to_csv(outputPath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d54ee9-e36c-4c09-96ea-c22df117b5ee",
   "metadata": {},
   "source": [
    "# mydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "558a7bb5-5139-4ed1-b1c8-a5da67db77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CUDA设备 0 - NVIDIA L20\n",
      "加载了 2424 对测试数据\n",
      "加载序列和特征...\n",
      "成功加载嵌入向量\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15301/1251787191.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型: saved_models/model_fold6_final.pth\n",
      "开始测试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 152/152 [00:01<00:00, 145.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估结果:\n",
      "Loss: 111.702332\n",
      "RMSE: 2.113469\n",
      "MAE: 1.557453\n",
      "R²: 0.097215\n",
      "Pearson相关系数: 0.405689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(111.70233154296875,\n",
       " 2.113469123840332,\n",
       " 1.557452917098999,\n",
       " 0.09721457958221436,\n",
       " 0.40568885516443026)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.models.mvsf import ModelAffinity\n",
    "from src.utils import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(model_path, test_data_path, seq_path, feature_path, batch_size=16, device=0):\n",
    "    \"\"\"\n",
    "    加载训练好的模型并在测试数据集上进行测试\n",
    "    \n",
    "    Args:\n",
    "        model_path: 模型文件路径\n",
    "        test_data_path: 测试数据CSV文件路径\n",
    "        seq_path: 序列FASTA文件\n",
    "        feature_path: 特征embedding文件路径\n",
    "        batch_size: 批量大小\n",
    "        device: 使用的GPU设备，-1表示CPU\n",
    "    \n",
    "    Returns:\n",
    "        评估指标：loss, rmse, mae, r2, p\n",
    "    \"\"\"\n",
    "    # 设置设备\n",
    "    use_cuda = (device > -1) and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        torch.cuda.set_device(device)\n",
    "        print(f\"使用CUDA设备 {device} - {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"使用CPU\")\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # 创建必要的目录\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    if len(test_df.columns) == 4 and 'light' in test_df.columns:\n",
    "        # 已经是正确格式\n",
    "        test_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "    else:\n",
    "        # 需要转换格式\n",
    "        print(\"转换TSV数据为所需的CSV格式...\")\n",
    "        pair_data = []\n",
    "        \n",
    "        # 处理每个数据点\n",
    "        for i, row in test_df.iterrows():\n",
    "            try:\n",
    "                pdb_id = row['pdb_id']\n",
    "                \n",
    "                # 创建ID\n",
    "                heavy_id = f\"{pdb_id}_heavy\"\n",
    "                light_id = f\"{pdb_id}_light\"\n",
    "                antigen_id = f\"{pdb_id}_antigen_{i+1}\"\n",
    "                \n",
    "                # 存储配对信息\n",
    "                pair_data.append({\n",
    "                    'light': light_id,\n",
    "                    'heavy': heavy_id,\n",
    "                    'antigen': antigen_id,\n",
    "                    'delta_g': float(row['delta_g'])\n",
    "                })\n",
    "            except KeyError:\n",
    "                print(f\"警告: 行 {i} 缺少必要字段，已跳过\")\n",
    "                continue\n",
    "        \n",
    "        test_df = pd.DataFrame(pair_data)\n",
    "    \n",
    "    # 提取数据\n",
    "    test_l = test_df[\"light\"]\n",
    "    test_h = test_df[\"heavy\"]\n",
    "    test_ag = test_df[\"antigen\"]\n",
    "    test_y = torch.from_numpy(test_df[\"delta_g\"].values)\n",
    "    test_y = -test_y  # 注意这里取了负值，与训练时保持一致\n",
    "    \n",
    "    # 创建测试数据集和数据加载器\n",
    "    test_dataset = PairedDataset(test_l, test_h, test_ag, test_y)\n",
    "    test_iterator = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_paired_sequences,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    print(f\"加载了 {len(test_l)} 对测试数据\")\n",
    "    \n",
    "    # 加载所有蛋白质序列和特征\n",
    "    print(\"加载序列和特征...\")\n",
    "    all_proteins = set(test_l).union(test_h).union(test_ag)\n",
    "    embeddings = embed_dict(seq_path, feature_path)\n",
    "    print(\"成功加载嵌入向量\")\n",
    "    aaindex_feature = seq_aaindex_dict(all_proteins, seq_path)\n",
    "    \n",
    "    # 创建模型实例并加载权重\n",
    "    model = ModelAffinity(batch_size, use_cuda)\n",
    "    model.use_cuda = use_cuda\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    # 加载训练好的模型参数\n",
    "    print(f\"加载模型: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 测试模型\n",
    "    print(\"开始测试...\")\n",
    "    with torch.no_grad():\n",
    "        p_hat = []\n",
    "        true_y = []\n",
    "        \n",
    "        for lchain, hchain, antigen, y in tqdm(test_iterator, desc=\"处理样本\"):\n",
    "            try:\n",
    "                ph = predict_affinity(model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda)\n",
    "                p_hat.append(ph)\n",
    "                true_y.append(y)\n",
    "            except Exception as e:\n",
    "                print(f\"处理样本时出错: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if len(p_hat) == 0:\n",
    "            print(\"没有成功的预测，请检查数据准备过程\")\n",
    "            return None\n",
    "            \n",
    "        y = torch.cat(true_y, 0)\n",
    "        p_hat = torch.cat(p_hat, 0)\n",
    "        \n",
    "        # 确保形状一致 - 修复维度不匹配问题\n",
    "        y = y.view(-1)          # 确保是一维张量\n",
    "        p_hat = p_hat.view(-1)  # 确保是一维张量\n",
    "        \n",
    "        if use_cuda:\n",
    "            y = y.cuda()\n",
    "            p_hat = p_hat.cuda()\n",
    "        \n",
    "        p_hat = p_hat.float()\n",
    "        y = y.float()\n",
    "        \n",
    "        # 计算原始损失\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(p_hat, y).item()\n",
    "        \n",
    "        # 转换回原始刻度\n",
    "        max_val = 16.9138\n",
    "        min_val = 5.0400\n",
    "        p_hat_original = (p_hat * (max_val - min_val)) + min_val\n",
    "        \n",
    "        # 计算评估指标\n",
    "        rmse = torch.sqrt(torch.mean((y - p_hat_original) ** 2)).item()\n",
    "        mae = torch.mean(torch.abs(y - p_hat_original)).item()\n",
    "        \n",
    "        # 使用numpy计算R2分数和Pearson相关系数，避免形状问题\n",
    "        from sklearn.metrics import r2_score as sklearn_r2\n",
    "        import scipy.stats as stats\n",
    "        \n",
    "        y_np = y.cpu().numpy()\n",
    "        p_hat_np = p_hat_original.cpu().numpy()\n",
    "        \n",
    "        r_2 = sklearn_r2(y_np, p_hat_np)\n",
    "        p, _ = stats.pearsonr(y_np, p_hat_np)\n",
    "        \n",
    "        # 保存结果到CSV\n",
    "        results = pd.DataFrame({\n",
    "            'True_Value': y_np,\n",
    "            'Predicted_Value': p_hat_np\n",
    "        })\n",
    "        results.to_csv('test_results.csv', index=False)\n",
    "        \n",
    "        # 打印评估结果\n",
    "        print(\"\\n评估结果:\")\n",
    "        print(f\"Loss: {loss:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"R²: {r_2:.6f}\")\n",
    "        print(f\"Pearson相关系数: {p:.6f}\")\n",
    "        \n",
    "        return loss, rmse, mae, r_2, p\n",
    "\n",
    "# 运行测试\n",
    "test_model(\"saved_models/model_fold6_final.pth\", \n",
    "           \"my_datasets/pairs_benchmark.csv\", \n",
    "           \"my_datasets/seq_natural.fasta\", \n",
    "           \"my_datasets/seq_natural_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1451ec2-e5cf-4109-9918-2382d560801a",
   "metadata": {},
   "source": [
    "# sabdab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85d716c6-d1e8-43ad-8293-072621f1df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CUDA设备 0 - NVIDIA L20\n",
      "加载了 578 对测试数据\n",
      "加载序列和特征...\n",
      "成功加载嵌入向量\n",
      "加载模型: saved_models/model_fold6_final.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15301/1046750583.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始测试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 37/37 [00:00<00:00, 132.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估结果:\n",
      "Loss: 113.737648\n",
      "RMSE: 1.500991\n",
      "MAE: 1.159484\n",
      "R²: 0.500516\n",
      "Pearson相关系数: 0.732565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(113.7376480102539,\n",
       " 1.5009914636611938,\n",
       " 1.1594836711883545,\n",
       " 0.500516414642334,\n",
       " 0.7325651701419711)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.models.mvsf import ModelAffinity\n",
    "from src.utils import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(model_path, test_data_path, seq_path, feature_path, batch_size=16, device=0):\n",
    "    \"\"\"\n",
    "    加载训练好的模型并在测试数据集上进行测试\n",
    "    \n",
    "    Args:\n",
    "        model_path: 模型文件路径\n",
    "        test_data_path: 测试数据CSV文件路径\n",
    "        seq_path: 序列FASTA文件\n",
    "        feature_path: 特征embedding文件路径\n",
    "        batch_size: 批量大小\n",
    "        device: 使用的GPU设备，-1表示CPU\n",
    "    \n",
    "    Returns:\n",
    "        评估指标：loss, rmse, mae, r2, p\n",
    "    \"\"\"\n",
    "    # 设置设备\n",
    "    use_cuda = (device > -1) and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        torch.cuda.set_device(device)\n",
    "        print(f\"使用CUDA设备 {device} - {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"使用CPU\")\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # 创建必要的目录\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    if len(test_df.columns) == 4 and 'light' in test_df.columns:\n",
    "        # 已经是正确格式\n",
    "        test_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "    else:\n",
    "        # 需要转换格式\n",
    "        print(\"转换TSV数据为所需的CSV格式...\")\n",
    "        pair_data = []\n",
    "        \n",
    "        # 处理每个数据点\n",
    "        for i, row in test_df.iterrows():\n",
    "            try:\n",
    "                pdb_id = row['pdb_id']\n",
    "                \n",
    "                # 创建ID\n",
    "                heavy_id = f\"{pdb_id}_heavy\"\n",
    "                light_id = f\"{pdb_id}_light\"\n",
    "                antigen_id = f\"{pdb_id}_antigen_{i+1}\"\n",
    "                \n",
    "                # 存储配对信息\n",
    "                pair_data.append({\n",
    "                    'light': light_id,\n",
    "                    'heavy': heavy_id,\n",
    "                    'antigen': antigen_id,\n",
    "                    'delta_g': float(row['delta_g'])\n",
    "                })\n",
    "            except KeyError:\n",
    "                print(f\"警告: 行 {i} 缺少必要字段，已跳过\")\n",
    "                continue\n",
    "        \n",
    "        test_df = pd.DataFrame(pair_data)\n",
    "    \n",
    "    # 提取数据\n",
    "    test_l = test_df[\"light\"]\n",
    "    test_h = test_df[\"heavy\"]\n",
    "    test_ag = test_df[\"antigen\"]\n",
    "    test_y = torch.from_numpy(test_df[\"delta_g\"].values)\n",
    "    test_y = -test_y  # 注意这里取了负值，与训练时保持一致\n",
    "    \n",
    "    # 创建测试数据集和数据加载器\n",
    "    test_dataset = PairedDataset(test_l, test_h, test_ag, test_y)\n",
    "    test_iterator = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_paired_sequences,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    print(f\"加载了 {len(test_l)} 对测试数据\")\n",
    "    \n",
    "    # 加载所有蛋白质序列和特征\n",
    "    print(\"加载序列和特征...\")\n",
    "    all_proteins = set(test_l).union(test_h).union(test_ag)\n",
    "    embeddings = embed_dict(seq_path, feature_path)\n",
    "    print(\"成功加载嵌入向量\")\n",
    "    aaindex_feature = seq_aaindex_dict(all_proteins, seq_path)\n",
    "    \n",
    "    # 创建模型实例并加载权重\n",
    "    model = ModelAffinity(batch_size, use_cuda)\n",
    "    model.use_cuda = use_cuda\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    # 加载训练好的模型参数\n",
    "    print(f\"加载模型: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 测试模型\n",
    "    print(\"开始测试...\")\n",
    "    with torch.no_grad():\n",
    "        p_hat = []\n",
    "        true_y = []\n",
    "        \n",
    "        for lchain, hchain, antigen, y in tqdm(test_iterator, desc=\"处理样本\"):\n",
    "            try:\n",
    "                ph = predict_affinity(model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda)\n",
    "                p_hat.append(ph)\n",
    "                true_y.append(y)\n",
    "            except Exception as e:\n",
    "                print(f\"处理样本时出错: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if len(p_hat) == 0:\n",
    "            print(\"没有成功的预测，请检查数据准备过程\")\n",
    "            return None\n",
    "            \n",
    "        y = torch.cat(true_y, 0)\n",
    "        p_hat = torch.cat(p_hat, 0)\n",
    "        \n",
    "        # 确保形状一致 - 修复维度不匹配问题\n",
    "        y = y.view(-1)          # 确保是一维张量\n",
    "        p_hat = p_hat.view(-1)  # 确保是一维张量\n",
    "        \n",
    "        if use_cuda:\n",
    "            y = y.cuda()\n",
    "            p_hat = p_hat.cuda()\n",
    "        \n",
    "        p_hat = p_hat.float()\n",
    "        y = y.float()\n",
    "        \n",
    "        # 计算原始损失\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(p_hat, y).item()\n",
    "        \n",
    "        # 转换回原始刻度\n",
    "        max_val = 16.9138\n",
    "        min_val = 5.0400\n",
    "        p_hat_original = (p_hat * (max_val - min_val)) + min_val\n",
    "        \n",
    "        # 计算评估指标\n",
    "        rmse = torch.sqrt(torch.mean((y - p_hat_original) ** 2)).item()\n",
    "        mae = torch.mean(torch.abs(y - p_hat_original)).item()\n",
    "        \n",
    "        # 使用numpy计算R2分数和Pearson相关系数，避免形状问题\n",
    "        from sklearn.metrics import r2_score as sklearn_r2\n",
    "        import scipy.stats as stats\n",
    "        \n",
    "        y_np = y.cpu().numpy()\n",
    "        p_hat_np = p_hat_original.cpu().numpy()\n",
    "        \n",
    "        r_2 = sklearn_r2(y_np, p_hat_np)\n",
    "        p, _ = stats.pearsonr(y_np, p_hat_np)\n",
    "        \n",
    "        # 保存结果到CSV\n",
    "        results = pd.DataFrame({\n",
    "            'True_Value': y_np,\n",
    "            'Predicted_Value': p_hat_np\n",
    "        })\n",
    "        results.to_csv('test_results.csv', index=False)\n",
    "        \n",
    "        # 打印评估结果\n",
    "        print(\"\\n评估结果:\")\n",
    "        print(f\"Loss: {loss:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"R²: {r_2:.6f}\")\n",
    "        print(f\"Pearson相关系数: {p:.6f}\")\n",
    "        \n",
    "        return loss, rmse, mae, r_2, p\n",
    "\n",
    "# 运行测试\n",
    "test_model(\"saved_models/model_fold6_final.pth\", \n",
    "           \"datasets/pairs_sabdab.csv\", \n",
    "           \"datasets/seq_natural.fasta\", \n",
    "           \"datasets/seq_natural_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81639426-9d76-4b19-ae45-0f9f3ac572fd",
   "metadata": {},
   "source": [
    "# skempi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3787aac5-e193-4070-8cde-89035422dc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CUDA设备 0 - NVIDIA L20\n",
      "加载了 387 对测试数据\n",
      "加载序列和特征...\n",
      "成功加载嵌入向量\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15301/2637437775.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型: saved_models/model_fold6_final.pth\n",
      "开始测试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 25/25 [00:00<00:00, 128.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估结果:\n",
      "Loss: 118.771324\n",
      "RMSE: 2.009979\n",
      "MAE: 1.629269\n",
      "R²: -0.012038\n",
      "Pearson相关系数: 0.219263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(118.77132415771484,\n",
       " 2.009979486465454,\n",
       " 1.6292694807052612,\n",
       " -0.012038469314575195,\n",
       " 0.219263052475394)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.models.mvsf import ModelAffinity\n",
    "from src.utils import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(model_path, test_data_path, seq_path, feature_path, batch_size=16, device=0):\n",
    "    \"\"\"\n",
    "    加载训练好的模型并在测试数据集上进行测试\n",
    "    \n",
    "    Args:\n",
    "        model_path: 模型文件路径\n",
    "        test_data_path: 测试数据CSV文件路径\n",
    "        seq_path: 序列FASTA文件\n",
    "        feature_path: 特征embedding文件路径\n",
    "        batch_size: 批量大小\n",
    "        device: 使用的GPU设备，-1表示CPU\n",
    "    \n",
    "    Returns:\n",
    "        评估指标：loss, rmse, mae, r2, p\n",
    "    \"\"\"\n",
    "    # 设置设备\n",
    "    use_cuda = (device > -1) and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        torch.cuda.set_device(device)\n",
    "        print(f\"使用CUDA设备 {device} - {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"使用CPU\")\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # 创建必要的目录\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    if len(test_df.columns) == 4 and 'light' in test_df.columns:\n",
    "        # 已经是正确格式\n",
    "        test_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "    else:\n",
    "        # 需要转换格式\n",
    "        print(\"转换TSV数据为所需的CSV格式...\")\n",
    "        pair_data = []\n",
    "        \n",
    "        # 处理每个数据点\n",
    "        for i, row in test_df.iterrows():\n",
    "            try:\n",
    "                pdb_id = row['pdb_id']\n",
    "                \n",
    "                # 创建ID\n",
    "                heavy_id = f\"{pdb_id}_heavy\"\n",
    "                light_id = f\"{pdb_id}_light\"\n",
    "                antigen_id = f\"{pdb_id}_antigen_{i+1}\"\n",
    "                \n",
    "                # 存储配对信息\n",
    "                pair_data.append({\n",
    "                    'light': light_id,\n",
    "                    'heavy': heavy_id,\n",
    "                    'antigen': antigen_id,\n",
    "                    'delta_g': float(row['delta_g'])\n",
    "                })\n",
    "            except KeyError:\n",
    "                print(f\"警告: 行 {i} 缺少必要字段，已跳过\")\n",
    "                continue\n",
    "        \n",
    "        test_df = pd.DataFrame(pair_data)\n",
    "    \n",
    "    # 提取数据\n",
    "    test_l = test_df[\"light\"]\n",
    "    test_h = test_df[\"heavy\"]\n",
    "    test_ag = test_df[\"antigen\"]\n",
    "    test_y = torch.from_numpy(test_df[\"delta_g\"].values)\n",
    "    test_y = -test_y  # 注意这里取了负值，与训练时保持一致\n",
    "    \n",
    "    # 创建测试数据集和数据加载器\n",
    "    test_dataset = PairedDataset(test_l, test_h, test_ag, test_y)\n",
    "    test_iterator = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_paired_sequences,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    print(f\"加载了 {len(test_l)} 对测试数据\")\n",
    "    \n",
    "    # 加载所有蛋白质序列和特征\n",
    "    print(\"加载序列和特征...\")\n",
    "    all_proteins = set(test_l).union(test_h).union(test_ag)\n",
    "    embeddings = embed_dict(seq_path, feature_path)\n",
    "    print(\"成功加载嵌入向量\")\n",
    "    aaindex_feature = seq_aaindex_dict(all_proteins, seq_path)\n",
    "    \n",
    "    # 创建模型实例并加载权重\n",
    "    model = ModelAffinity(batch_size, use_cuda)\n",
    "    model.use_cuda = use_cuda\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    # 加载训练好的模型参数\n",
    "    print(f\"加载模型: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 测试模型\n",
    "    print(\"开始测试...\")\n",
    "    with torch.no_grad():\n",
    "        p_hat = []\n",
    "        true_y = []\n",
    "        \n",
    "        for lchain, hchain, antigen, y in tqdm(test_iterator, desc=\"处理样本\"):\n",
    "            try:\n",
    "                ph = predict_affinity(model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda)\n",
    "                p_hat.append(ph)\n",
    "                true_y.append(y)\n",
    "            except Exception as e:\n",
    "                print(f\"处理样本时出错: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if len(p_hat) == 0:\n",
    "            print(\"没有成功的预测，请检查数据准备过程\")\n",
    "            return None\n",
    "            \n",
    "        y = torch.cat(true_y, 0)\n",
    "        p_hat = torch.cat(p_hat, 0)\n",
    "        \n",
    "        # 确保形状一致 - 修复维度不匹配问题\n",
    "        y = y.view(-1)          # 确保是一维张量\n",
    "        p_hat = p_hat.view(-1)  # 确保是一维张量\n",
    "        \n",
    "        if use_cuda:\n",
    "            y = y.cuda()\n",
    "            p_hat = p_hat.cuda()\n",
    "        \n",
    "        p_hat = p_hat.float()\n",
    "        y = y.float()\n",
    "        \n",
    "        # 计算原始损失\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(p_hat, y).item()\n",
    "        \n",
    "        # 转换回原始刻度\n",
    "        max_val = 16.9138\n",
    "        min_val = 5.0400\n",
    "        p_hat_original = (p_hat * (max_val - min_val)) + min_val\n",
    "        \n",
    "        # 计算评估指标\n",
    "        rmse = torch.sqrt(torch.mean((y - p_hat_original) ** 2)).item()\n",
    "        mae = torch.mean(torch.abs(y - p_hat_original)).item()\n",
    "        \n",
    "        # 使用numpy计算R2分数和Pearson相关系数，避免形状问题\n",
    "        from sklearn.metrics import r2_score as sklearn_r2\n",
    "        import scipy.stats as stats\n",
    "        \n",
    "        y_np = y.cpu().numpy()\n",
    "        p_hat_np = p_hat_original.cpu().numpy()\n",
    "        \n",
    "        r_2 = sklearn_r2(y_np, p_hat_np)\n",
    "        p, _ = stats.pearsonr(y_np, p_hat_np)\n",
    "        \n",
    "        # 保存结果到CSV\n",
    "        results = pd.DataFrame({\n",
    "            'True_Value': y_np,\n",
    "            'Predicted_Value': p_hat_np\n",
    "        })\n",
    "        results.to_csv('test_results.csv', index=False)\n",
    "        \n",
    "        # 打印评估结果\n",
    "        print(\"\\n评估结果:\")\n",
    "        print(f\"Loss: {loss:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"R²: {r_2:.6f}\")\n",
    "        print(f\"Pearson相关系数: {p:.6f}\")\n",
    "        \n",
    "        return loss, rmse, mae, r_2, p\n",
    "\n",
    "# 运行测试\n",
    "test_model(\"saved_models/model_fold6_final.pth\", \n",
    "           \"datasets/pairs_skempi.csv\", \n",
    "           \"datasets/seq.fasta\", \n",
    "           \"datasets/embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ce32b-8101-4331-8b43-e908a40a9a92",
   "metadata": {},
   "source": [
    "# abbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a110763-731f-463f-8581-16b241ae18e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CUDA设备 0 - NVIDIA L20\n",
      "加载了 1089 对测试数据\n",
      "加载序列和特征...\n",
      "成功加载嵌入向量\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15301/1678327426.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载模型: saved_models/model_fold6_final.pth\n",
      "开始测试...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理样本: 100%|██████████| 69/69 [00:00<00:00, 136.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "评估结果:\n",
      "Loss: 88.558701\n",
      "RMSE: 3.785968\n",
      "MAE: 3.020082\n",
      "R²: -0.876432\n",
      "Pearson相关系数: 0.084193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88.55870056152344,\n",
       " 3.785968065261841,\n",
       " 3.0200822353363037,\n",
       " -0.876431941986084,\n",
       " 0.08419250456683267)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.models.mvsf import ModelAffinity\n",
    "from src.utils import *\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_model(model_path, test_data_path, seq_path, feature_path, batch_size=16, device=0):\n",
    "    \"\"\"\n",
    "    加载训练好的模型并在测试数据集上进行测试\n",
    "    \n",
    "    Args:\n",
    "        model_path: 模型文件路径\n",
    "        test_data_path: 测试数据CSV文件路径\n",
    "        seq_path: 序列FASTA文件\n",
    "        feature_path: 特征embedding文件路径\n",
    "        batch_size: 批量大小\n",
    "        device: 使用的GPU设备，-1表示CPU\n",
    "    \n",
    "    Returns:\n",
    "        评估指标：loss, rmse, mae, r2, p\n",
    "    \"\"\"\n",
    "    # 设置设备\n",
    "    use_cuda = (device > -1) and torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        torch.cuda.set_device(device)\n",
    "        print(f\"使用CUDA设备 {device} - {torch.cuda.get_device_name(device)}\")\n",
    "    else:\n",
    "        print(\"使用CPU\")\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    # 创建必要的目录\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "    \n",
    "    # 加载测试数据\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    if len(test_df.columns) == 4 and 'light' in test_df.columns:\n",
    "        # 已经是正确格式\n",
    "        test_df.columns = [\"light\", \"heavy\", \"antigen\", \"delta_g\"]\n",
    "    else:\n",
    "        # 需要转换格式\n",
    "        print(\"转换TSV数据为所需的CSV格式...\")\n",
    "        pair_data = []\n",
    "        \n",
    "        # 处理每个数据点\n",
    "        for i, row in test_df.iterrows():\n",
    "            try:\n",
    "                pdb_id = row['pdb_id']\n",
    "                \n",
    "                # 创建ID\n",
    "                heavy_id = f\"{pdb_id}_heavy\"\n",
    "                light_id = f\"{pdb_id}_light\"\n",
    "                antigen_id = f\"{pdb_id}_antigen_{i+1}\"\n",
    "                \n",
    "                # 存储配对信息\n",
    "                pair_data.append({\n",
    "                    'light': light_id,\n",
    "                    'heavy': heavy_id,\n",
    "                    'antigen': antigen_id,\n",
    "                    'delta_g': float(row['delta_g'])\n",
    "                })\n",
    "            except KeyError:\n",
    "                print(f\"警告: 行 {i} 缺少必要字段，已跳过\")\n",
    "                continue\n",
    "        \n",
    "        test_df = pd.DataFrame(pair_data)\n",
    "    \n",
    "    # 提取数据\n",
    "    test_l = test_df[\"light\"]\n",
    "    test_h = test_df[\"heavy\"]\n",
    "    test_ag = test_df[\"antigen\"]\n",
    "    test_y = torch.from_numpy(test_df[\"delta_g\"].values)\n",
    "    test_y = -test_y  # 注意这里取了负值，与训练时保持一致\n",
    "    \n",
    "    # 创建测试数据集和数据加载器\n",
    "    test_dataset = PairedDataset(test_l, test_h, test_ag, test_y)\n",
    "    test_iterator = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_paired_sequences,\n",
    "        shuffle=False,\n",
    "        pin_memory=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    print(f\"加载了 {len(test_l)} 对测试数据\")\n",
    "    \n",
    "    # 加载所有蛋白质序列和特征\n",
    "    print(\"加载序列和特征...\")\n",
    "    all_proteins = set(test_l).union(test_h).union(test_ag)\n",
    "    embeddings = embed_dict(seq_path, feature_path)\n",
    "    print(\"成功加载嵌入向量\")\n",
    "    aaindex_feature = seq_aaindex_dict(all_proteins, seq_path)\n",
    "    \n",
    "    # 创建模型实例并加载权重\n",
    "    model = ModelAffinity(batch_size, use_cuda)\n",
    "    model.use_cuda = use_cuda\n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    \n",
    "    # 加载训练好的模型参数\n",
    "    print(f\"加载模型: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cuda' if use_cuda else 'cpu'))\n",
    "    \n",
    "    # 设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 测试模型\n",
    "    print(\"开始测试...\")\n",
    "    with torch.no_grad():\n",
    "        p_hat = []\n",
    "        true_y = []\n",
    "        \n",
    "        for lchain, hchain, antigen, y in tqdm(test_iterator, desc=\"处理样本\"):\n",
    "            try:\n",
    "                ph = predict_affinity(model, lchain, hchain, antigen, embeddings, aaindex_feature, use_cuda)\n",
    "                p_hat.append(ph)\n",
    "                true_y.append(y)\n",
    "            except Exception as e:\n",
    "                print(f\"处理样本时出错: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        if len(p_hat) == 0:\n",
    "            print(\"没有成功的预测，请检查数据准备过程\")\n",
    "            return None\n",
    "            \n",
    "        y = torch.cat(true_y, 0)\n",
    "        p_hat = torch.cat(p_hat, 0)\n",
    "        \n",
    "        # 确保形状一致 - 修复维度不匹配问题\n",
    "        y = y.view(-1)          # 确保是一维张量\n",
    "        p_hat = p_hat.view(-1)  # 确保是一维张量\n",
    "        \n",
    "        if use_cuda:\n",
    "            y = y.cuda()\n",
    "            p_hat = p_hat.cuda()\n",
    "        \n",
    "        p_hat = p_hat.float()\n",
    "        y = y.float()\n",
    "        \n",
    "        # 计算原始损失\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = criterion(p_hat, y).item()\n",
    "        \n",
    "        # 转换回原始刻度\n",
    "        max_val = 16.9138\n",
    "        min_val = 5.0400\n",
    "        p_hat_original = (p_hat * (max_val - min_val)) + min_val\n",
    "        \n",
    "        # 计算评估指标\n",
    "        rmse = torch.sqrt(torch.mean((y - p_hat_original) ** 2)).item()\n",
    "        mae = torch.mean(torch.abs(y - p_hat_original)).item()\n",
    "        \n",
    "        # 使用numpy计算R2分数和Pearson相关系数，避免形状问题\n",
    "        from sklearn.metrics import r2_score as sklearn_r2\n",
    "        import scipy.stats as stats\n",
    "        \n",
    "        y_np = y.cpu().numpy()\n",
    "        p_hat_np = p_hat_original.cpu().numpy()\n",
    "        \n",
    "        r_2 = sklearn_r2(y_np, p_hat_np)\n",
    "        p, _ = stats.pearsonr(y_np, p_hat_np)\n",
    "        \n",
    "        # 保存结果到CSV\n",
    "        results = pd.DataFrame({\n",
    "            'True_Value': y_np,\n",
    "            'Predicted_Value': p_hat_np\n",
    "        })\n",
    "        results.to_csv('test_results.csv', index=False)\n",
    "        \n",
    "        # 打印评估结果\n",
    "        print(\"\\n评估结果:\")\n",
    "        print(f\"Loss: {loss:.6f}\")\n",
    "        print(f\"RMSE: {rmse:.6f}\")\n",
    "        print(f\"MAE: {mae:.6f}\")\n",
    "        print(f\"R²: {r_2:.6f}\")\n",
    "        print(f\"Pearson相关系数: {p:.6f}\")\n",
    "        \n",
    "        return loss, rmse, mae, r_2, p\n",
    "\n",
    "# 运行测试\n",
    "test_model(\"saved_models/model_fold6_final.pth\", \n",
    "           \"datasets/pairs_abbind.csv\", \n",
    "           \"datasets/seq.fasta\", \n",
    "           \"datasets/embedding.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1369580-fd73-4d1b-9711-9e59052196d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
